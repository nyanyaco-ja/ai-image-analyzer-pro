import cv2
import numpy as np
from skimage.metrics import structural_similarity as ssim
from skimage.metrics import peak_signal_noise_ratio as psnr
try:
    from skimage.metrics import structural_similarity as compare_ssim
    from pytorch_msssim import ms_ssim as pytorch_ms_ssim
    MS_SSIM_AVAILABLE = True
except ImportError:
    MS_SSIM_AVAILABLE = False
import matplotlib
matplotlib.use('Agg')  # GUIéè¡¨ç¤ºã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’ä½¿ç”¨
import matplotlib.pyplot as plt
from PIL import Image
import os
from scipy import stats
from skimage import feature
import json
from datetime import datetime

# LPIPSç”¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import torch
    import torch.nn.functional as F
    import lpips
    import kornia
    import kornia.filters as KF
    import kornia.color as KC
    LPIPS_AVAILABLE = True
    KORNIA_AVAILABLE = True

    # GPUåˆ©ç”¨å¯å¦ã‚’ãƒã‚§ãƒƒã‚¯
    if torch.cuda.is_available():
        DEVICE = torch.device('cuda')
        GPU_AVAILABLE = True
        GPU_NAME = torch.cuda.get_device_name(0)
    else:
        DEVICE = torch.device('cpu')
        GPU_AVAILABLE = False
        GPU_NAME = None
except ImportError:
    LPIPS_AVAILABLE = False
    KORNIA_AVAILABLE = False
    GPU_AVAILABLE = False
    DEVICE = None
    GPU_NAME = None
    torch = None

# CLIPç”¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from transformers import CLIPProcessor, CLIPModel
    CLIP_AVAILABLE = True
    CLIP_MODEL = None
    CLIP_PROCESSOR = None
except ImportError:
    CLIP_AVAILABLE = False
    CLIP_MODEL = None
    CLIP_PROCESSOR = None

# CPU/GPUãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ç”¨
try:
    import psutil
    import GPUtil
    MONITORING_AVAILABLE = True
except ImportError:
    MONITORING_AVAILABLE = False

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['MS Gothic', 'Yu Gothic', 'Meiryo', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False  # ãƒã‚¤ãƒŠã‚¹è¨˜å·ã®æ–‡å­—åŒ–ã‘å¯¾ç­–

def get_system_usage():
    """CPU/GPUä½¿ç”¨ç‡ã‚’å–å¾—"""
    usage_info = {}

    if MONITORING_AVAILABLE:
        # CPUä½¿ç”¨ç‡
        usage_info['cpu_percent'] = psutil.cpu_percent(interval=0.1)
        usage_info['cpu_count'] = psutil.cpu_count()

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡
        memory = psutil.virtual_memory()
        usage_info['ram_percent'] = memory.percent
        usage_info['ram_used_gb'] = memory.used / (1024**3)
        usage_info['ram_total_gb'] = memory.total / (1024**3)

        # GPUä½¿ç”¨ç‡
        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu = gpus[0]
                usage_info['gpu_percent'] = gpu.load * 100
                usage_info['gpu_memory_percent'] = gpu.memoryUtil * 100
                usage_info['gpu_memory_used_mb'] = gpu.memoryUsed
                usage_info['gpu_memory_total_mb'] = gpu.memoryTotal
                usage_info['gpu_temp'] = gpu.temperature
            else:
                usage_info['gpu_percent'] = None
        except:
            usage_info['gpu_percent'] = None

    return usage_info

def print_usage_status(stage_name):
    """å‡¦ç†æ®µéšã”ã¨ã®ä½¿ç”¨ç‡ã‚’è¡¨ç¤º"""
    if not MONITORING_AVAILABLE:
        return

    usage = get_system_usage()

    print(f"\n[{stage_name}] ã‚·ã‚¹ãƒ†ãƒ ä½¿ç”¨çŠ¶æ³:")
    print(f"  CPU: {usage.get('cpu_percent', 0):.1f}% ({usage.get('cpu_count', 0)}ã‚³ã‚¢)")
    print(f"  RAM: {usage.get('ram_percent', 0):.1f}% ({usage.get('ram_used_gb', 0):.1f}/{usage.get('ram_total_gb', 0):.1f} GB)")

    if usage.get('gpu_percent') is not None:
        print(f"  GPU: {usage.get('gpu_percent', 0):.1f}% ä½¿ç”¨ä¸­")
        print(f"  VRAM: {usage.get('gpu_memory_percent', 0):.1f}% ({usage.get('gpu_memory_used_mb', 0):.0f}/{usage.get('gpu_memory_total_mb', 0):.0f} MB)")
        if usage.get('gpu_temp'):
            print(f"  GPUæ¸©åº¦: {usage.get('gpu_temp')}Â°C")
    else:
        print(f"  GPU: æœªä½¿ç”¨ï¼ˆCPUå‡¦ç†ä¸­ï¼‰")

def calculate_sharpness(image_gray):
    """ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹ï¼ˆé®®é‹­åº¦ï¼‰è¨ˆç®— - ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³åˆ†æ•£æ³•"""
    laplacian = cv2.Laplacian(image_gray, cv2.CV_64F)
    return laplacian.var()

def calculate_contrast(image_gray):
    """ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆè¨ˆç®— - RMSå¯¾æ¯”"""
    return image_gray.std()

def calculate_entropy(image_gray):
    """ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®— - æƒ…å ±é‡ã®æŒ‡æ¨™"""
    hist = cv2.calcHist([image_gray], [0], None, [256], [0, 256])
    hist = hist.ravel() / hist.sum()
    hist = hist[hist > 0]
    return -np.sum(hist * np.log2(hist))

def calculate_lpips(img1_rgb, img2_rgb):
    """
    LPIPSï¼ˆçŸ¥è¦šçš„é¡ä¼¼åº¦ï¼‰è¨ˆç®—

    Returns:
        float: LPIPSè·é›¢ï¼ˆ0ã«è¿‘ã„ã»ã©çŸ¥è¦šçš„ã«é¡ä¼¼ï¼‰
    """
    if not LPIPS_AVAILABLE:
        return None, None

    try:
        # è­¦å‘Šã‚’æŠ‘åˆ¶
        import warnings
        warnings.filterwarnings('ignore', category=UserWarning, module='torchvision')

        # LPIPSãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆAlexNetãƒ™ãƒ¼ã‚¹ï¼‰
        loss_fn = lpips.LPIPS(net='alex').to(DEVICE)

        # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®šï¼ˆæ¨è«–ç”¨ï¼‰
        loss_fn.eval()

        # ç”»åƒã‚’PyTorchãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ› [0-255] -> [-1, 1]
        def to_tensor(img):
            # RGB -> PyTorchã®é †åº (C, H, W)
            img_tensor = torch.from_numpy(img).permute(2, 0, 1).float()
            # [0, 255] -> [-1, 1]
            img_tensor = (img_tensor / 127.5) - 1.0
            # ãƒãƒƒãƒæ¬¡å…ƒã‚’è¿½åŠ ã—ã¦GPU/CPUã«è»¢é€
            return img_tensor.unsqueeze(0).to(DEVICE)

        img1_tensor = to_tensor(img1_rgb)
        img2_tensor = to_tensor(img2_rgb)

        # GPUä½¿ç”¨ç‡å–å¾—ï¼ˆGPUã®å ´åˆï¼‰
        gpu_usage = None
        if GPU_AVAILABLE:
            torch.cuda.synchronize()
            gpu_usage = torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated() * 100

        # LPIPSè·é›¢ã‚’è¨ˆç®—
        with torch.no_grad():
            distance = loss_fn(img1_tensor, img2_tensor)

        return float(distance.item()), gpu_usage

    except Exception as e:
        print(f"LPIPSè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        return None, None

def is_document_image(img_rgb):
    """
    ç”»åƒãŒæ–‡æ›¸/ãƒ†ã‚­ã‚¹ãƒˆä¸»ä½“ã®ç”»åƒã‹ã©ã†ã‹ã‚’åˆ¤å®š

    åŒ»ç™‚ã‚«ãƒ«ãƒ†ã€ãƒ¬ã‚·ãƒ¼ãƒˆã€ã‚¹ã‚­ãƒ£ãƒ³æ–‡æ›¸ãªã©ã¯CLIPãŒè‹¦æ‰‹ã¨ã™ã‚‹ãŸã‚æ¤œå‡ºã™ã‚‹

    Args:
        img_rgb: RGBç”»åƒ (numpy array)

    Returns:
        bool: æ–‡æ›¸ç”»åƒã¨åˆ¤å®šã•ã‚ŒãŸå ´åˆTrue
    """
    try:
        # 1. æ˜ã‚‹ã„èƒŒæ™¯ç‡ã®è¨ˆç®—ï¼ˆæ–‡æ›¸ã¯æ˜ã‚‹ã„èƒŒæ™¯ãŒå¤šã„ï¼‰
        # RGBå¹³å‡ãŒ200ä»¥ä¸Šã®ãƒ”ã‚¯ã‚»ãƒ«ã‚’ã€Œæ˜ã‚‹ã„èƒŒæ™¯ã€ã¨ã¿ãªã™ï¼ˆåŒ»ç™‚ã‚«ãƒ«ãƒ†å¯¾å¿œï¼‰
        bright_pixels = np.sum(np.mean(img_rgb, axis=2) >= 200)
        total_pixels = img_rgb.shape[0] * img_rgb.shape[1]
        bright_ratio = bright_pixels / total_pixels

        # 2. éå¸¸ã«æ˜ã‚‹ã„ãƒ”ã‚¯ã‚»ãƒ«ï¼ˆç™½ã«è¿‘ã„ï¼‰
        white_pixels = np.sum(np.all(img_rgb >= 230, axis=2))
        white_ratio = white_pixels / total_pixels

        # 3. è‰²åˆ†æ•£ã®è¨ˆç®—ï¼ˆæ–‡æ›¸ã¯è‰²ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãŒå°‘ãªã„ï¼‰
        color_std = np.std(img_rgb)

        # 4. ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ç‡ï¼ˆæ–‡æ›¸ã¯ç™½é»’ãŒå¤šã„ï¼‰
        # RGBã®å·®ãŒå°ã•ã„ãƒ”ã‚¯ã‚»ãƒ«ã‚’ã€Œã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ã€ã¨ã¿ãªã™
        rgb_diff = np.max(img_rgb, axis=2) - np.min(img_rgb, axis=2)
        gray_pixels = np.sum(rgb_diff < 40)
        gray_ratio = gray_pixels / total_pixels

        # 5. LABè‰²ç©ºé–“ã§ã®Lå€¤ï¼ˆæ˜åº¦ï¼‰ãŒé«˜ã„
        lab_l_mean = np.mean(img_rgb)  # ç°¡æ˜“çš„ãªæ˜åº¦æŒ‡æ¨™

        # åˆ¤å®šåŸºæº–ï¼ˆåŒ»ç™‚ã‚«ãƒ«ãƒ†ã«æœ€é©åŒ–ï¼‰:
        # - æ˜ã‚‹ã„èƒŒæ™¯ç‡ > 50% AND (è‰²åˆ†æ•£ < 60 OR ã‚°ãƒ¬ãƒ¼ç‡ > 70%) â†’ æ–‡æ›¸
        # - éå¸¸ã«æ˜ã‚‹ã„èƒŒæ™¯ç‡ > 30% AND ã‚°ãƒ¬ãƒ¼ç‡ > 60% â†’ æ–‡æ›¸
        # - å¹³å‡è¼åº¦ > 200 AND è‰²åˆ†æ•£ < 70 â†’ æ–‡æ›¸ï¼ˆåŒ»ç™‚ã‚«ãƒ«ãƒ†ç‰¹åŒ–ï¼‰
        is_document = (bright_ratio > 0.50 and (color_std < 60 or gray_ratio > 0.70)) or \
                     (white_ratio > 0.30 and gray_ratio > 0.60) or \
                     (lab_l_mean > 200 and color_std < 70)

        # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šåˆ¤å®šæƒ…å ±ã‚’å¸¸ã«è¡¨ç¤º
        print(f"    æ–‡æ›¸åˆ¤å®š - æ˜èƒŒæ™¯: {bright_ratio*100:.1f}%, ç™½èƒŒæ™¯: {white_ratio*100:.1f}%, è‰²åˆ†æ•£: {color_std:.1f}, ã‚°ãƒ¬ãƒ¼ç‡: {gray_ratio*100:.1f}%, å¹³å‡è¼åº¦: {lab_l_mean:.1f} â†’ {'ğŸ“„æ–‡æ›¸' if is_document else 'ğŸ–¼ï¸è‡ªç„¶ç”»åƒ'}")

        return is_document

    except Exception as e:
        print(f"æ–‡æ›¸åˆ¤å®šã‚¨ãƒ©ãƒ¼: {e}")
        return False

def calculate_clip_similarity(img1_rgb, img2_rgb):
    """
    CLIP Embeddings ã‚’ä½¿ç”¨ã—ãŸæ„å‘³çš„é¡ä¼¼åº¦è¨ˆç®—

    Returns:
        float: ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼ˆ1.0ã«è¿‘ã„ã»ã©æ„å‘³çš„ã«é¡ä¼¼ã€-1.0ã€œ1.0ã®ç¯„å›²ï¼‰
    """
    global CLIP_MODEL, CLIP_PROCESSOR

    if not CLIP_AVAILABLE:
        return None

    try:
        # åˆå›ã®ã¿ãƒ¢ãƒ‡ãƒ«ã¨ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’èª­ã¿è¾¼ã¿
        if CLIP_MODEL is None or CLIP_PROCESSOR is None:
            print("CLIP ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            # safetensorså½¢å¼ã§ãƒ­ãƒ¼ãƒ‰ï¼ˆPyTorch 2.6æœªæº€ã§ã‚‚å‹•ä½œï¼‰
            CLIP_MODEL = CLIPModel.from_pretrained("openai/clip-vit-base-patch32", use_safetensors=True)
            CLIP_PROCESSOR = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

            # GPUãŒåˆ©ç”¨å¯èƒ½ãªã‚‰GPUã«è»¢é€
            if GPU_AVAILABLE and DEVICE:
                CLIP_MODEL = CLIP_MODEL.to(DEVICE)

            # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
            CLIP_MODEL.eval()
            print(f"CLIP ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº† (ãƒ‡ãƒã‚¤ã‚¹: {DEVICE if DEVICE else 'CPU'})")

        # RGBç”»åƒã‚’PIL Imageã«å¤‰æ›ï¼ˆCLIPProcessorãŒæœŸå¾…ã™ã‚‹å½¢å¼ï¼‰
        from PIL import Image as PILImage
        img1_pil = PILImage.fromarray(img1_rgb.astype('uint8'))
        img2_pil = PILImage.fromarray(img2_rgb.astype('uint8'))

        # ç”»åƒã‚’å‰å‡¦ç†
        inputs1 = CLIP_PROCESSOR(images=img1_pil, return_tensors="pt")
        inputs2 = CLIP_PROCESSOR(images=img2_pil, return_tensors="pt")

        # GPUãŒåˆ©ç”¨å¯èƒ½ãªã‚‰GPUã«è»¢é€
        if GPU_AVAILABLE and DEVICE:
            inputs1 = {k: v.to(DEVICE) for k, v in inputs1.items()}
            inputs2 = {k: v.to(DEVICE) for k, v in inputs2.items()}

        # EmbeddingæŠ½å‡º
        with torch.no_grad():
            image_features1 = CLIP_MODEL.get_image_features(**inputs1)
            image_features2 = CLIP_MODEL.get_image_features(**inputs2)

        # L2æ­£è¦åŒ–
        image_features1 = image_features1 / image_features1.norm(p=2, dim=-1, keepdim=True)
        image_features2 = image_features2 / image_features2.norm(p=2, dim=-1, keepdim=True)

        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—ï¼ˆå†…ç©ï¼‰
        cosine_similarity = (image_features1 @ image_features2.T).item()

        return cosine_similarity

    except Exception as e:
        print(f"CLIPé¡ä¼¼åº¦è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return None

def calculate_ssim_gpu(img1_rgb, img2_rgb):
    """
    GPUå¯¾å¿œSSIMè¨ˆç®—ï¼ˆPyTorchä½¿ç”¨ï¼‰

    Returns:
        float: SSIMå€¤ï¼ˆ1.0ã«è¿‘ã„ã»ã©é¡ä¼¼ï¼‰
    """
    if not LPIPS_AVAILABLE or torch is None:
        # PyTorchãŒãªã„å ´åˆã¯CPUç‰ˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return ssim(img1_rgb, img2_rgb, channel_axis=2)

    try:
        # pytorch-msssimãŒåˆ©ç”¨å¯èƒ½ãªã‚‰ãã‚Œã‚’ä½¿ç”¨
        if MS_SSIM_AVAILABLE:
            from pytorch_msssim import ssim as pytorch_ssim

            def to_tensor(img):
                img_tensor = torch.from_numpy(img).permute(2, 0, 1).float()
                img_tensor = img_tensor / 255.0
                return img_tensor.unsqueeze(0).to(DEVICE)

            img1_tensor = to_tensor(img1_rgb)
            img2_tensor = to_tensor(img2_rgb)

            with torch.no_grad():
                ssim_val = pytorch_ssim(img1_tensor, img2_tensor, data_range=1.0)
            return float(ssim_val.item())

        # pytorch-msssimãŒãªã„å ´åˆã¯ç‹¬è‡ªGPUå®Ÿè£…
        else:
            def to_tensor(img):
                img_tensor = torch.from_numpy(img).permute(2, 0, 1).float()
                img_tensor = img_tensor / 255.0
                return img_tensor.unsqueeze(0).to(DEVICE)

            img1_tensor = to_tensor(img1_rgb)
            img2_tensor = to_tensor(img2_rgb)

            # ç°¡æ˜“SSIMè¨ˆç®—ï¼ˆGPUä¸Šã§å¹³å‡ã¨åˆ†æ•£ã‚’è¨ˆç®—ï¼‰
            with torch.no_grad():
                mu1 = F.avg_pool2d(img1_tensor, 11, 1, 5)
                mu2 = F.avg_pool2d(img2_tensor, 11, 1, 5)

                mu1_sq = mu1 ** 2
                mu2_sq = mu2 ** 2
                mu1_mu2 = mu1 * mu2

                sigma1_sq = F.avg_pool2d(img1_tensor ** 2, 11, 1, 5) - mu1_sq
                sigma2_sq = F.avg_pool2d(img2_tensor ** 2, 11, 1, 5) - mu2_sq
                sigma12 = F.avg_pool2d(img1_tensor * img2_tensor, 11, 1, 5) - mu1_mu2

                C1 = 0.01 ** 2
                C2 = 0.03 ** 2

                ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / \
                          ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))

                ssim_val = torch.mean(ssim_map)

            return float(ssim_val.item())

    except Exception as e:
        print(f"GPU SSIMè¨ˆç®—ã‚¨ãƒ©ãƒ¼ï¼ˆCPUç‰ˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰: {e}")
        return ssim(img1_rgb, img2_rgb, channel_axis=2)

def calculate_psnr_gpu(img1_rgb, img2_rgb):
    """
    GPUå¯¾å¿œPSNRè¨ˆç®—ï¼ˆPyTorchä½¿ç”¨ï¼‰

    Returns:
        float: PSNRå€¤ï¼ˆé«˜ã„ã»ã©é¡ä¼¼ï¼‰
    """
    if not LPIPS_AVAILABLE or torch is None:
        # PyTorchãŒãªã„å ´åˆã¯CPUç‰ˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return psnr(img1_rgb, img2_rgb)

    try:
        # ç”»åƒã‚’PyTorchãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
        def to_tensor(img):
            img_tensor = torch.from_numpy(img).permute(2, 0, 1).float()
            img_tensor = img_tensor / 255.0
            return img_tensor.unsqueeze(0).to(DEVICE)

        img1_tensor = to_tensor(img1_rgb)
        img2_tensor = to_tensor(img2_rgb)

        # MSEè¨ˆç®—
        with torch.no_grad():
            mse = F.mse_loss(img1_tensor, img2_tensor)

            if mse == 0:
                return float('inf')

            # PSNRè¨ˆç®—
            psnr_val = 10 * torch.log10(1.0 / mse)

        return float(psnr_val.item())

    except Exception as e:
        print(f"GPU PSNRè¨ˆç®—ã‚¨ãƒ©ãƒ¼ï¼ˆCPUç‰ˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰: {e}")
        return psnr(img1_rgb, img2_rgb)

def calculate_sharpness_gpu(img_gray):
    """
    GPUå¯¾å¿œã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹è¨ˆç®—ï¼ˆKornia Laplacianï¼‰

    Returns:
        float: ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹å€¤ï¼ˆé«˜ã„ã»ã©é®®æ˜ï¼‰
    """
    if not KORNIA_AVAILABLE or torch is None:
        # KorniaãŒãªã„å ´åˆã¯CPUç‰ˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        laplacian = cv2.Laplacian(img_gray, cv2.CV_64F)
        return laplacian.var()

    try:
        # ç”»åƒã‚’PyTorchãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
        img_tensor = torch.from_numpy(img_gray).float().unsqueeze(0).unsqueeze(0).to(DEVICE) / 255.0

        # Korniaã®ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³
        with torch.no_grad():
            laplacian = KF.laplacian(img_tensor, kernel_size=3)
            variance = torch.var(laplacian)

        return float(variance.item()) * 255 * 255  # ã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´

    except Exception as e:
        print(f"GPU ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹è¨ˆç®—ã‚¨ãƒ©ãƒ¼ï¼ˆCPUç‰ˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰: {e}")
        laplacian = cv2.Laplacian(img_gray, cv2.CV_64F)
        return laplacian.var()

def estimate_noise_gpu(img_gray):
    """
    GPUå¯¾å¿œãƒã‚¤ã‚ºæ¨å®šï¼ˆé«˜å‘¨æ³¢æˆåˆ†ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼‰

    Returns:
        float: ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«æ¨å®šå€¤
    """
    if not LPIPS_AVAILABLE or torch is None:
        # CPUç‰ˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        H = cv2.dct(np.float32(img_gray) / 255.0)
        noise_level = np.sum(np.abs(H[int(H.shape[0]*0.9):, int(H.shape[1]*0.9):]))
        return noise_level

    try:
        # ç”»åƒã‚’PyTorchãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
        img_tensor = torch.from_numpy(img_gray).float().unsqueeze(0).unsqueeze(0).to(DEVICE) / 255.0

        # é«˜å‘¨æ³¢ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        high_pass_kernel = torch.tensor([[-1, -1, -1],
                                         [-1,  8, -1],
                                         [-1, -1, -1]], dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(DEVICE)

        with torch.no_grad():
            high_freq = F.conv2d(img_tensor, high_pass_kernel, padding=1)
            noise_level = torch.mean(torch.abs(high_freq))

        return float(noise_level.item()) * 100  # ã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´

    except Exception as e:
        print(f"GPU ãƒã‚¤ã‚ºæ¨å®šã‚¨ãƒ©ãƒ¼ï¼ˆCPUç‰ˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰: {e}")
        H = cv2.dct(np.float32(img_gray) / 255.0)
        noise_level = np.sum(np.abs(H[int(H.shape[0]*0.9):, int(H.shape[1]*0.9):]))
        return noise_level

def detect_edges_gpu(img_gray):
    """
    GPUå¯¾å¿œã‚¨ãƒƒã‚¸æ¤œå‡ºï¼ˆKornia Sobelï¼‰

    Returns:
        float: ã‚¨ãƒƒã‚¸å¯†åº¦
    """
    if not KORNIA_AVAILABLE or torch is None:
        # CPUç‰ˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        edges = cv2.Canny(img_gray, 100, 200)
        return np.sum(edges) / (edges.shape[0] * edges.shape[1] * 255) * 100

    try:
        # ç”»åƒã‚’PyTorchãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
        img_tensor = torch.from_numpy(img_gray).float().unsqueeze(0).unsqueeze(0).to(DEVICE) / 255.0

        # Korniaã® Sobel
        with torch.no_grad():
            edges = KF.sobel(img_tensor)
            edge_density = torch.sum(edges > 0.1) / edges.numel() * 100

        return float(edge_density.item())

    except Exception as e:
        print(f"GPU ã‚¨ãƒƒã‚¸æ¤œå‡ºã‚¨ãƒ©ãƒ¼ï¼ˆCPUç‰ˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰: {e}")
        edges = cv2.Canny(img_gray, 100, 200)
        return np.sum(edges) / (edges.shape[0] * edges.shape[1] * 255) * 100

def calculate_color_difference_gpu(img1_rgb, img2_rgb):
    """
    GPUå¯¾å¿œLABè‰²ç©ºé–“ã§ã®è‰²å·®è¨ˆç®—ï¼ˆKorniaï¼‰

    Returns:
        float: å¹³å‡è‰²å·®ï¼ˆÎ”Eï¼‰
    """
    if not KORNIA_AVAILABLE or torch is None:
        # CPUç‰ˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        img1_lab = cv2.cvtColor(img1_rgb, cv2.COLOR_RGB2LAB)
        img2_lab = cv2.cvtColor(img2_rgb, cv2.COLOR_RGB2LAB)
        delta_e = np.sqrt(np.sum((img1_lab.astype(float) - img2_lab.astype(float)) ** 2, axis=2))
        return np.mean(delta_e)

    try:
        # ç”»åƒã‚’PyTorchãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
        def to_tensor(img):
            img_tensor = torch.from_numpy(img).permute(2, 0, 1).float()
            return img_tensor.unsqueeze(0).to(DEVICE) / 255.0

        img1_tensor = to_tensor(img1_rgb)
        img2_tensor = to_tensor(img2_rgb)

        # Korniaã®LABå¤‰æ›
        with torch.no_grad():
            img1_lab = KC.rgb_to_lab(img1_tensor)
            img2_lab = KC.rgb_to_lab(img2_tensor)
            delta_e = torch.sqrt(torch.sum((img1_lab - img2_lab) ** 2, dim=1))
            mean_delta_e = torch.mean(delta_e)

        return float(mean_delta_e.item())

    except Exception as e:
        print(f"GPU è‰²å·®è¨ˆç®—ã‚¨ãƒ©ãƒ¼ï¼ˆCPUç‰ˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰: {e}")
        img1_lab = cv2.cvtColor(img1_rgb, cv2.COLOR_RGB2LAB)
        img2_lab = cv2.cvtColor(img2_rgb, cv2.COLOR_RGB2LAB)
        delta_e = np.sqrt(np.sum((img1_lab.astype(float) - img2_lab.astype(float)) ** 2, axis=2))
        return np.mean(delta_e)

def calculate_ms_ssim(img1_rgb, img2_rgb):
    """
    MS-SSIMï¼ˆMulti-Scale SSIMï¼‰è¨ˆç®—

    Returns:
        float: MS-SSIMå€¤ï¼ˆ1.0ã«è¿‘ã„ã»ã©é¡ä¼¼ï¼‰
    """
    if not MS_SSIM_AVAILABLE:
        return None

    try:
        # ç”»åƒã‚’PyTorchãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ› [0-255] -> [0, 1]
        def to_tensor(img):
            # RGB -> PyTorchã®é †åº (C, H, W)
            img_tensor = torch.from_numpy(img).permute(2, 0, 1).float()
            # [0, 255] -> [0, 1]
            img_tensor = img_tensor / 255.0
            # ãƒãƒƒãƒæ¬¡å…ƒã‚’è¿½åŠ ã—ã¦GPU/CPUã«è»¢é€
            return img_tensor.unsqueeze(0).to(DEVICE)

        img1_tensor = to_tensor(img1_rgb)
        img2_tensor = to_tensor(img2_rgb)

        # MS-SSIMè¨ˆç®—ï¼ˆdata_range=1.0ã§æ­£è¦åŒ–æ¸ˆã¿ç”»åƒç”¨ï¼‰
        with torch.no_grad():
            ms_ssim_val = pytorch_ms_ssim(img1_tensor, img2_tensor, data_range=1.0)

        return float(ms_ssim_val.item())

    except Exception as e:
        print(f"MS-SSIMè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def analyze_local_quality(img1, img2, patch_size=64):
    """å±€æ‰€çš„ãªå“è³ªåˆ†æ - ãƒ‘ãƒƒãƒå˜ä½ã§SSIMã‚’è¨ˆç®—"""
    h, w = img1.shape[:2]
    ssim_map = []

    for y in range(0, h - patch_size, patch_size):
        for x in range(0, w - patch_size, patch_size):
            patch1 = img1[y:y+patch_size, x:x+patch_size]
            patch2 = img2[y:y+patch_size, x:x+patch_size]

            if patch1.shape[:2] == (patch_size, patch_size):
                local_ssim = ssim(patch1, patch2, channel_axis=2)
                ssim_map.append(local_ssim)

    return np.array(ssim_map)

def detect_artifacts(image_gray):
    """ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆæ¤œå‡º - ãƒ–ãƒ­ãƒƒã‚¯ãƒã‚¤ã‚ºã‚„ãƒªãƒ³ã‚®ãƒ³ã‚°"""
    # ãƒ–ãƒ­ãƒƒã‚¯ãƒã‚¤ã‚ºæ¤œå‡ºï¼ˆ8x8ãƒ–ãƒ­ãƒƒã‚¯å¢ƒç•Œã®ä¸é€£ç¶šæ€§ï¼‰
    block_noise = 0
    h, w = image_gray.shape

    for y in range(8, h, 8):
        diff = np.abs(image_gray[y-1, :].astype(float) - image_gray[y, :].astype(float))
        block_noise += np.mean(diff)

    for x in range(8, w, 8):
        diff = np.abs(image_gray[:, x-1].astype(float) - image_gray[:, x].astype(float))
        block_noise += np.mean(diff)

    # ãƒªãƒ³ã‚®ãƒ³ã‚°æ¤œå‡ºï¼ˆã‚¨ãƒƒã‚¸å‘¨è¾ºã®æŒ¯å‹•ï¼‰
    edges = cv2.Canny(image_gray, 100, 200)
    kernel = np.ones((5, 5), np.uint8)
    edge_region = cv2.dilate(edges, kernel, iterations=2)

    edge_pixels = image_gray[edge_region > 0]
    ringing = np.std(edge_pixels) if len(edge_pixels) > 0 else 0

    return block_noise, ringing

def analyze_color_distribution(img_rgb):
    """è‰²åˆ†å¸ƒã®è©³ç´°åˆ†æï¼ˆRGB, HSV, LABï¼‰"""
    # å„ãƒãƒ£ãƒ³ãƒãƒ«ã®çµ±è¨ˆ
    stats_data = {}

    # RGBåˆ†æ
    for i, channel in enumerate(['Red', 'Green', 'Blue']):
        channel_data = img_rgb[:, :, i]
        stats_data[channel] = {
            'mean': float(np.mean(channel_data)),
            'std': float(np.std(channel_data)),
            'min': int(np.min(channel_data)),
            'max': int(np.max(channel_data)),
            'median': float(np.median(channel_data))
        }

    # HSVåˆ†æ
    hsv = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HSV)
    saturation = hsv[:, :, 1]
    stats_data['Saturation'] = {
        'mean': float(np.mean(saturation)),
        'std': float(np.std(saturation))
    }

    # LABè‰²ç©ºé–“åˆ†æï¼ˆçŸ¥è¦šçš„ãªè‰²å·®è©•ä¾¡ç”¨ï¼‰
    lab = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2LAB)
    stats_data['LAB'] = {
        'L_mean': float(np.mean(lab[:, :, 0])),  # æ˜åº¦
        'L_std': float(np.std(lab[:, :, 0])),
        'a_mean': float(np.mean(lab[:, :, 1])),  # èµ¤-ç·‘
        'a_std': float(np.std(lab[:, :, 1])),
        'b_mean': float(np.mean(lab[:, :, 2])),  # é»„-é’
        'b_std': float(np.std(lab[:, :, 2]))
    }

    return stats_data

def analyze_frequency_domain(img_gray):
    """å‘¨æ³¢æ•°é ˜åŸŸåˆ†æ - FFT"""
    f_transform = np.fft.fft2(img_gray)
    f_shift = np.fft.fftshift(f_transform)
    magnitude = np.abs(f_shift)

    # é«˜å‘¨æ³¢æˆåˆ†ã®å‰²åˆ
    h, w = magnitude.shape
    center_h, center_w = h // 2, w // 2
    radius = min(center_h, center_w) // 3

    y, x = np.ogrid[:h, :w]
    mask = (x - center_w)**2 + (y - center_h)**2 <= radius**2

    low_freq_energy = np.sum(magnitude[mask])
    high_freq_energy = np.sum(magnitude[~mask])
    total_energy = low_freq_energy + high_freq_energy

    return {
        'low_freq_ratio': low_freq_energy / total_energy,
        'high_freq_ratio': high_freq_energy / total_energy,
        'total_energy': float(total_energy)
    }

def analyze_texture(img_gray):
    """ãƒ†ã‚¯ã‚¹ãƒãƒ£åˆ†æï¼ˆGPUå¯¾å¿œ - Kornia Sobelï¼‰"""
    if not KORNIA_AVAILABLE or torch is None:
        # CPUç‰ˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        small = cv2.resize(img_gray, (img_gray.shape[1]//4, img_gray.shape[0]//4))
        sobel_x = cv2.Sobel(small, cv2.CV_64F, 1, 0, ksize=3)
        sobel_y = cv2.Sobel(small, cv2.CV_64F, 0, 1, ksize=3)
        texture_complexity = np.sqrt(sobel_x**2 + sobel_y**2).mean()
        return {'texture_complexity': float(texture_complexity)}

    try:
        # GPUç‰ˆ
        img_tensor = torch.from_numpy(img_gray).float().unsqueeze(0).unsqueeze(0).to(DEVICE) / 255.0
        # ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        img_small = F.interpolate(img_tensor, scale_factor=0.25, mode='bilinear', align_corners=False)

        with torch.no_grad():
            sobel_magnitude = KF.sobel(img_small)
            texture_complexity = torch.mean(sobel_magnitude)

        return {'texture_complexity': float(texture_complexity.item() * 255)}
    except Exception as e:
        print(f"GPU ãƒ†ã‚¯ã‚¹ãƒãƒ£åˆ†æã‚¨ãƒ©ãƒ¼ï¼ˆCPUç‰ˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰: {e}")
        small = cv2.resize(img_gray, (img_gray.shape[1]//4, img_gray.shape[0]//4))
        sobel_x = cv2.Sobel(small, cv2.CV_64F, 1, 0, ksize=3)
        sobel_y = cv2.Sobel(small, cv2.CV_64F, 0, 1, ksize=3)
        texture_complexity = np.sqrt(sobel_x**2 + sobel_y**2).mean()
        return {'texture_complexity': float(texture_complexity)}

def create_detailed_visualizations(img1_rgb, img2_rgb, img1_gray, img2_gray, output_dir):
    """è©³ç´°ãªå¯è¦–åŒ–ç”»åƒã‚’ç”Ÿæˆ"""
    fig = plt.figure(figsize=(20, 12))

    # 1. å…ƒç”»åƒ
    plt.subplot(3, 4, 1)
    plt.imshow(img1_rgb)
    plt.title('å…ƒç”»åƒ (Ground Truth)', fontsize=12, fontweight='bold')
    plt.axis('off')

    plt.subplot(3, 4, 2)
    plt.imshow(img2_rgb)
    plt.title('AIå‡¦ç†çµæœ', fontsize=12, fontweight='bold')
    plt.axis('off')

    # 2. ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
    plt.subplot(3, 4, 3)
    for i, color in enumerate(['r', 'g', 'b']):
        hist = cv2.calcHist([img1_rgb], [i], None, [256], [0, 256])
        plt.plot(hist, color=color, alpha=0.7, linewidth=1.5)
    plt.title('ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ  - å…ƒç”»åƒ', fontsize=11)
    plt.xlim([0, 256])
    plt.xlabel('è¼åº¦å€¤', fontsize=9)
    plt.ylabel('ãƒ”ã‚¯ã‚»ãƒ«æ•°', fontsize=9)

    plt.subplot(3, 4, 4)
    for i, color in enumerate(['r', 'g', 'b']):
        hist = cv2.calcHist([img2_rgb], [i], None, [256], [0, 256])
        plt.plot(hist, color=color, alpha=0.7, linewidth=1.5)
    plt.title('ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ  - AIå‡¦ç†çµæœ', fontsize=11)
    plt.xlim([0, 256])
    plt.xlabel('è¼åº¦å€¤', fontsize=9)
    plt.ylabel('ãƒ”ã‚¯ã‚»ãƒ«æ•°', fontsize=9)

    # 3. ã‚¨ãƒƒã‚¸æ¤œå‡º
    edges1 = cv2.Canny(img1_gray, 100, 200)
    edges2 = cv2.Canny(img2_gray, 100, 200)

    plt.subplot(3, 4, 5)
    plt.imshow(edges1, cmap='gray')
    plt.title('ã‚¨ãƒƒã‚¸æ¤œå‡º - å…ƒç”»åƒ', fontsize=11)
    plt.axis('off')

    plt.subplot(3, 4, 6)
    plt.imshow(edges2, cmap='gray')
    plt.title('ã‚¨ãƒƒã‚¸æ¤œå‡º - AIå‡¦ç†çµæœ', fontsize=11)
    plt.axis('off')

    # 4. å·®åˆ†
    diff = cv2.absdiff(img1_rgb, img2_rgb)
    diff_gray = cv2.cvtColor(diff, cv2.COLOR_RGB2GRAY)

    plt.subplot(3, 4, 7)
    plt.imshow(diff)
    plt.title('çµ¶å¯¾å·®åˆ†', fontsize=11)
    plt.axis('off')

    plt.subplot(3, 4, 8)
    plt.imshow(diff_gray, cmap='hot')
    plt.title('å·®åˆ†ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—', fontsize=11)
    plt.axis('off')
    cb = plt.colorbar(fraction=0.046, pad=0.04)
    cb.ax.tick_params(labelsize=8)

    # 5. FFTï¼ˆå‘¨æ³¢æ•°é ˜åŸŸï¼‰
    f1 = np.fft.fft2(img1_gray)
    f2 = np.fft.fft2(img2_gray)

    magnitude1 = np.log(np.abs(np.fft.fftshift(f1)) + 1)
    magnitude2 = np.log(np.abs(np.fft.fftshift(f2)) + 1)

    plt.subplot(3, 4, 9)
    plt.imshow(magnitude1, cmap='gray')
    plt.title('å‘¨æ³¢æ•°ã‚¹ãƒšã‚¯ãƒˆãƒ« - å…ƒç”»åƒ', fontsize=11)
    plt.axis('off')

    plt.subplot(3, 4, 10)
    plt.imshow(magnitude2, cmap='gray')
    plt.title('å‘¨æ³¢æ•°ã‚¹ãƒšã‚¯ãƒˆãƒ« - AIå‡¦ç†çµæœ', fontsize=11)
    plt.axis('off')

    # 6. ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹å¯è¦–åŒ–ï¼ˆãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³ï¼‰
    lap1 = cv2.Laplacian(img1_gray, cv2.CV_64F)
    lap2 = cv2.Laplacian(img2_gray, cv2.CV_64F)

    plt.subplot(3, 4, 11)
    im1 = plt.imshow(np.abs(lap1), cmap='viridis')
    plt.title('ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹ãƒãƒƒãƒ— - å…ƒç”»åƒ', fontsize=11)
    plt.axis('off')
    cb1 = plt.colorbar(im1, fraction=0.046, pad=0.04)
    cb1.ax.tick_params(labelsize=8)

    plt.subplot(3, 4, 12)
    im2 = plt.imshow(np.abs(lap2), cmap='viridis')
    plt.title('ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹ãƒãƒƒãƒ— - AIå‡¦ç†çµæœ', fontsize=11)
    plt.axis('off')
    cb2 = plt.colorbar(im2, fraction=0.046, pad=0.04)
    cb2.ax.tick_params(labelsize=8)

    plt.tight_layout()
    output_path = os.path.join(output_dir, 'detailed_analysis.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()

def create_comparison_report(results, img1_name, img2_name, output_dir):
    """æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆç”»åƒã‚’ç”Ÿæˆ"""
    fig = plt.figure(figsize=(16, 10))
    fig.suptitle('ç”»åƒæ¯”è¼ƒåˆ†æãƒ¬ãƒãƒ¼ãƒˆ', fontsize=20, fontweight='bold', y=0.98)

    # ã‚¹ã‚³ã‚¢è¡¨ç¤ºï¼ˆä¸¡ç”»åƒæ¯”è¼ƒï¼‰
    ax1 = plt.subplot(2, 3, 1)
    breakdown = results['total_score']['breakdown']

    categories = ['ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹', 'ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ', 'ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼', 'ãƒã‚¤ã‚ºå¯¾ç­–', 'ã‚¨ãƒƒã‚¸ä¿æŒ', 'æ­ªã¿æŠ‘åˆ¶', 'ãƒ†ã‚¯ã‚¹ãƒãƒ£']
    img1_values = [breakdown['img1']['sharpness'], breakdown['img1']['contrast'],
                   breakdown['img1']['entropy'], breakdown['img1']['noise'],
                   breakdown['img1']['edge'], breakdown['img1']['artifact'],
                   breakdown['img1']['texture']]
    img2_values = [breakdown['img2']['sharpness'], breakdown['img2']['contrast'],
                   breakdown['img2']['entropy'], breakdown['img2']['noise'],
                   breakdown['img2']['edge'], breakdown['img2']['artifact'],
                   breakdown['img2']['texture']]

    x = np.arange(len(categories))
    width = 0.35

    bars1 = ax1.barh(x - width/2, img1_values, width, label='å…ƒç”»åƒ', color='#3498db')
    bars2 = ax1.barh(x + width/2, img2_values, width, label='AIå‡¦ç†çµæœ', color='#e74c3c')

    ax1.set_yticks(x)
    ax1.set_yticklabels(categories)
    ax1.set_xlim(0, 100)
    ax1.set_xlabel('ã‚¹ã‚³ã‚¢', fontsize=11, fontweight='bold')
    ax1.set_title('é …ç›®åˆ¥ã‚¹ã‚³ã‚¢æ¯”è¼ƒ', fontsize=13, fontweight='bold', pad=10)
    ax1.legend(fontsize=10)
    ax1.grid(axis='x', alpha=0.3)

    # ç·åˆã‚¹ã‚³ã‚¢
    ax2 = plt.subplot(2, 3, 2)
    total_score = results['total_score']['img2']
    img1_score = results['total_score']['img1']

    ax2.barh(['å…ƒç”»åƒ (åŸºæº–)', 'AIå‡¦ç†çµæœ'], [img1_score, total_score],
             color=['#3498db', '#e74c3c' if total_score < 70 else '#f39c12' if total_score < 90 else '#2ecc71'])
    ax2.set_xlim(0, 100)
    ax2.set_xlabel('ç·åˆã‚¹ã‚³ã‚¢', fontsize=11, fontweight='bold')
    ax2.set_title('ç·åˆè©•ä¾¡', fontsize=13, fontweight='bold', pad=10)
    ax2.grid(axis='x', alpha=0.3)

    for i, (score, name) in enumerate(zip([img1_score, total_score], ['å…ƒç”»åƒ', 'AIå‡¦ç†çµæœ'])):
        ax2.text(score + 2, i, f'{score:.1f}', va='center', fontsize=12, fontweight='bold')

    # ä¸»è¦æŒ‡æ¨™
    ax3 = plt.subplot(2, 3, 3)
    ax3.axis('off')

    delta_e_value = results['color_distribution'].get('delta_e', 0)

    # SSIM/PSNR/delta_eã¯å…ƒç”»åƒã®æœ‰ç„¡ã§å½¢å¼ãŒç•°ãªã‚‹
    ssim_data = results['ssim']
    if isinstance(ssim_data, dict):
        ssim_display = f"å…ƒç”»åƒ: {ssim_data['img1_vs_original']:.4f}\n  AIå‡¦ç†çµæœ: {ssim_data['img2_vs_original']:.4f}"
    else:
        ssim_display = f"{ssim_data:.4f}"

    psnr_data = results['psnr']
    if isinstance(psnr_data, dict):
        psnr_display = f"å…ƒç”»åƒ: {psnr_data['img1_vs_original']:.2f} dB\n  AIå‡¦ç†çµæœ: {psnr_data['img2_vs_original']:.2f} dB"
    else:
        psnr_display = f"{psnr_data:.2f} dB"

    if isinstance(delta_e_value, dict):
        delta_e_display = f"å…ƒç”»åƒ: {delta_e_value['img1_vs_original']:.2f}\n  AIå‡¦ç†çµæœ: {delta_e_value['img2_vs_original']:.2f}"
    else:
        delta_e_display = f"{delta_e_value:.2f}"

    info_text = f"""
ã€ä¸»è¦æŒ‡æ¨™ã€‘

SSIM: {ssim_display}
  (1.0 = å®Œå…¨ä¸€è‡´)

PSNR: {psnr_display}
  (30dBä»¥ä¸Šã§è¦–è¦šçš„ã«åŒç­‰)

ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹:
  å…ƒç”»åƒ: {results['sharpness']['img1']:.2f}
  AIå‡¦ç†çµæœ: {results['sharpness']['img2']:.2f}
  å·®: {results['sharpness']['difference_pct']:+.1f}%

è‰²å·® (Î”E): {delta_e_display}
  (< 5: è¨±å®¹, > 10: æ˜ç¢ºãªé•ã„)
    """

    ax3.text(0.1, 0.5, info_text, fontsize=11, family='monospace',
             verticalalignment='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))
    ax3.set_title('è©³ç´°ãƒ‡ãƒ¼ã‚¿', fontsize=13, fontweight='bold', pad=10)

    # ã‚¨ãƒƒã‚¸æ¯”è¼ƒ
    ax4 = plt.subplot(2, 3, 4)
    edge_data = [results['edges']['img1_density'], results['edges']['img2_density']]
    ax4.bar(['å…ƒç”»åƒ', 'AIå‡¦ç†çµæœ'], edge_data, color=['#3498db', '#9b59b6'])
    ax4.set_ylabel('ã‚¨ãƒƒã‚¸å¯†åº¦ (%)', fontsize=11, fontweight='bold')
    ax4.set_title('ã‚¨ãƒƒã‚¸ä¿æŒç‡', fontsize=13, fontweight='bold', pad=10)
    ax4.grid(axis='y', alpha=0.3)

    for i, val in enumerate(edge_data):
        ax4.text(i, val, f'{val:,}', ha='center', va='bottom', fontsize=10, fontweight='bold')

    # ãƒã‚¤ã‚ºãƒ»ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ
    ax5 = plt.subplot(2, 3, 5)
    noise_data = [results['noise']['img1'], results['noise']['img2']]
    artifact1 = results['artifacts']['img1_block_noise'] + results['artifacts']['img1_ringing']
    artifact2 = results['artifacts']['img2_block_noise'] + results['artifacts']['img2_ringing']

    x = np.arange(2)
    width = 0.35

    ax5.bar(x - width/2, noise_data, width, label='ãƒã‚¤ã‚º', color='#e67e22')
    ax5.bar(x + width/2, [artifact1, artifact2], width, label='ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ', color='#c0392b')

    ax5.set_ylabel('å€¤ (ä½ã„æ–¹ãŒè‰¯ã„)', fontsize=11, fontweight='bold')
    ax5.set_title('ãƒã‚¤ã‚ºã¨ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ', fontsize=13, fontweight='bold', pad=10)
    ax5.set_xticks(x)
    ax5.set_xticklabels(['å…ƒç”»åƒ', 'AIå‡¦ç†çµæœ'])
    ax5.legend(fontsize=10)
    ax5.grid(axis='y', alpha=0.3)

    # å‘¨æ³¢æ•°åˆ†æ
    ax6 = plt.subplot(2, 3, 6)
    freq1 = [results['frequency_analysis']['img1']['low_freq_ratio'] * 100,
             results['frequency_analysis']['img1']['high_freq_ratio'] * 100]
    freq2 = [results['frequency_analysis']['img2']['low_freq_ratio'] * 100,
             results['frequency_analysis']['img2']['high_freq_ratio'] * 100]

    x = np.arange(2)
    width = 0.35

    ax6.bar(x - width/2, freq1, width, label='å…ƒç”»åƒ', color='#3498db')
    ax6.bar(x + width/2, freq2, width, label='AIå‡¦ç†çµæœ', color='#9b59b6')

    ax6.set_ylabel('æ¯”ç‡ (%)', fontsize=11, fontweight='bold')
    ax6.set_title('å‘¨æ³¢æ•°æˆåˆ†åˆ†å¸ƒ', fontsize=13, fontweight='bold', pad=10)
    ax6.set_xticks(x)
    ax6.set_xticklabels(['ä½å‘¨æ³¢', 'é«˜å‘¨æ³¢'])
    ax6.legend(fontsize=10)
    ax6.set_ylim(0, 100)
    ax6.grid(axis='y', alpha=0.3)

    plt.tight_layout()
    report_path = os.path.join(output_dir, 'comparison_report.png')
    plt.savefig(report_path, dpi=150, bbox_inches='tight')
    plt.close()

    return report_path

def imread_unicode(filename):
    """æ—¥æœ¬èªãƒ‘ã‚¹ã«å¯¾å¿œã—ãŸç”»åƒèª­ã¿è¾¼ã¿ï¼ˆé€æ˜èƒŒæ™¯å¯¾å¿œï¼‰"""
    try:
        from PIL import Image
        pil_image = Image.open(filename)

        # é€æ˜èƒŒæ™¯ï¼ˆRGBAï¼‰ã®å ´åˆã€ç™½èƒŒæ™¯ã§åˆæˆ
        if pil_image.mode == 'RGBA':
            print(f"  é€æ˜èƒŒæ™¯ã‚’æ¤œå‡º: {filename}")
            print(f"  ç™½èƒŒæ™¯ã§åˆæˆã—ã¾ã™")
            # ç™½èƒŒæ™¯ã‚’ä½œæˆ
            background = Image.new('RGB', pil_image.size, (255, 255, 255))
            # ã‚¢ãƒ«ãƒ•ã‚¡ãƒãƒ£ãƒ³ãƒãƒ«ã‚’ä½¿ã£ã¦åˆæˆ
            background.paste(pil_image, mask=pil_image.split()[3])  # 3ç•ªç›®ã¯ã‚¢ãƒ«ãƒ•ã‚¡ãƒãƒ£ãƒ³ãƒãƒ«
            pil_image = background
        elif pil_image.mode != 'RGB':
            pil_image = pil_image.convert('RGB')

        # numpyé…åˆ—ã«å¤‰æ›
        img_array = np.array(pil_image)
        # RGB -> BGR (OpenCVå½¢å¼)
        img_bgr = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)
        return img_bgr
    except Exception as e:
        print(f"ç”»åƒèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def analyze_images(img1_path, img2_path, output_dir='analysis_results', original_path=None, evaluation_mode='image'):
    """
    å…ƒç”»åƒã¨AIå‡¦ç†çµæœã‚’è©³ç´°ã«æ¯”è¼ƒåˆ†æã™ã‚‹ï¼ˆç²¾åº¦è©•ä¾¡ï¼‰

    Parameters:
    img1_path: å…ƒç”»åƒã®ãƒ‘ã‚¹ï¼ˆGround Truth / åŸºæº–ç”»åƒï¼‰
    img2_path: AIå‡¦ç†çµæœã®ãƒ‘ã‚¹ï¼ˆè¶…è§£åƒç”»åƒãªã©ï¼‰
    output_dir: çµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    original_path: ä½¿ç”¨ã—ãªã„ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ã™ï¼‰
    evaluation_mode: è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ ('image', 'document', 'developer', 'academic')

    Returns:
    results: åˆ†æçµæœã®è¾æ›¸
    """

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    os.makedirs(output_dir, exist_ok=True)

    # ç”»åƒèª­ã¿è¾¼ã¿ï¼ˆæ—¥æœ¬èªãƒ‘ã‚¹å¯¾å¿œï¼‰
    img1 = imread_unicode(img1_path)  # å…ƒç”»åƒï¼ˆGround Truthï¼‰
    img2 = imread_unicode(img2_path)  # AIå‡¦ç†çµæœ

    # ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼ˆã‚³ãƒ¼ãƒ‰å…¨ä½“ã§çµ±ä¸€çš„ã«ä½¿ç”¨ï¼‰
    img_original = img1  # å…ƒç”»åƒ
    img_ai_result = img2  # AIå‡¦ç†çµæœ

    if img1 is None or img2 is None:
        print("ã‚¨ãƒ©ãƒ¼: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“")
        print(f"å…ƒç”»åƒãƒ‘ã‚¹: {img1_path}")
        print(f"AIå‡¦ç†çµæœãƒ‘ã‚¹: {img2_path}")
        return

    # ç”»åƒã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ã¨èª¿æ•´
    if img1.shape != img2.shape:
        print(f"\nç”»åƒã‚µã‚¤ã‚ºãŒç•°ãªã‚Šã¾ã™:")
        print(f"  å…ƒç”»åƒ: {img1.shape[1]} x {img1.shape[0]} px")
        print(f"  AIå‡¦ç†çµæœ: {img2.shape[1]} x {img2.shape[0]} px")
        print(f"AIå‡¦ç†çµæœã‚’å…ƒç”»åƒã®ã‚µã‚¤ã‚ºã«ãƒªã‚µã‚¤ã‚ºã—ã¾ã™...\n")

        # AIå‡¦ç†çµæœã‚’å…ƒç”»åƒã®ã‚µã‚¤ã‚ºã«åˆã‚ã›ã‚‹
        img2 = cv2.resize(img2, (img1.shape[1], img1.shape[0]), interpolation=cv2.INTER_LANCZOS4)
        img_ai_result = img2  # ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚‚æ›´æ–°


    # åˆ†æãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¡¨ç¤º
    if evaluation_mode == "academic":
        print("\n" + "=" * 80)
        print("ã€åˆ†æãƒ‘ã‚¿ãƒ¼ãƒ³ã€‘å­¦è¡“è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ï¼ˆAcademic Evaluationï¼‰")
        print("=" * 80)
        print("ğŸ“š æ¨™æº–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ–¹å¼: Ã—2 Scale Super-Resolution")
        print(f"   Ground Truthï¼ˆå…ƒç”»åƒï¼‰: {img1.shape[1]}x{img1.shape[0]}px")
        print(f"   AIå‡¦ç†çµæœ: {img2.shape[1]}x{img2.shape[0]}px")
        print("   æ¯”è¼ƒå¯¾è±¡: DIV2K, Set5, Set14ç­‰ã¨ã®å®šé‡æ¯”è¼ƒ")
        print("=" * 80)
    else:
        print("\n" + "=" * 80)
        print("ã€åˆ†æãƒ‘ã‚¿ãƒ¼ãƒ³ã€‘ç²¾åº¦è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ï¼ˆå…ƒç”»åƒåŸºæº–ï¼‰")
        print("=" * 80)
        print("ğŸ“Œ ç”¨é€”: AIè¶…è§£åƒã€ç”»è³ªæ”¹å–„ã€ãƒã‚¤ã‚ºé™¤å»ç­‰ã®ç²¾åº¦è©•ä¾¡")
        print(f"   å…ƒç”»åƒ: {img1.shape[1]}x{img1.shape[0]}px")
        print(f"   AIå‡¦ç†çµæœ: {img2.shape[1]}x{img2.shape[0]}px")
        print("=" * 80)

    # RGB/ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›ï¼ˆå…ƒç”»åƒï¼‰
    img_original_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)
    img_original_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)

    # RGBå¤‰æ›ï¼ˆOpenCVã¯BGRãªã®ã§ï¼‰
    img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)
    img2_rgb = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)

    # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›
    img1_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
    img2_gray = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)

    # çµæœã‚’ä¿å­˜ã™ã‚‹è¾æ›¸
    results = {
        'timestamp': datetime.now().isoformat(),
        'image1_path': img1_path,
        'image2_path': img2_path,
        'original_path': original_path,
        'has_original': img_original_rgb is not None
    }

    print("=" * 80)
    print("è©³ç´°ç”»åƒæ¯”è¼ƒåˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
    print("=" * 80)
    print("ğŸ“Œ æ¯”è¼ƒå¯¾è±¡: å…ƒç”»åƒï¼ˆå‡¦ç†å‰/Beforeï¼‰ vs AIè¶…è§£åƒçµæœï¼ˆå‡¦ç†å¾Œ/Afterï¼‰")
    print("=" * 80)

    # 1. åŸºæœ¬æƒ…å ±
    print("\nã€1. åŸºæœ¬æƒ…å ±ã€‘")
    print(f"è¶…è§£åƒçµæœ1ã‚µã‚¤ã‚º: {img1.shape[1]} x {img1.shape[0]} px")
    print(f"è¶…è§£åƒçµæœ2ã‚µã‚¤ã‚º: {img2.shape[1]} x {img2.shape[0]} px")

    size1 = os.path.getsize(img1_path) / (1024 * 1024)
    size2 = os.path.getsize(img2_path) / (1024 * 1024)
    print(f"è¶…è§£åƒçµæœ1ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {size1:.2f} MB")
    print(f"è¶…è§£åƒçµæœ2ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {size2:.2f} MB")
    print(f"ã‚µã‚¤ã‚ºå·®: {abs(size1 - size2):.2f} MB ({((size2/size1 - 1) * 100):+.1f}%)")

    if img_original is not None:
        size_original = os.path.getsize(original_path) / (1024 * 1024)
        print(f"å…ƒç”»åƒï¼ˆå‡¦ç†å‰ï¼‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {size_original:.2f} MB")

    # GPU/CPUæƒ…å ±
    print(f"\nè¨ˆç®—ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±:")
    if LPIPS_AVAILABLE:
        if GPU_AVAILABLE:
            print(f"  GPU: {GPU_NAME}")
            print(f"  CUDAåˆ©ç”¨å¯èƒ½: ã¯ã„")
            print(f"  VRAMã‚µã‚¤ã‚º: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB")
        else:
            print(f"  GPU: ãªã—ï¼ˆCPUä½¿ç”¨ï¼‰")
            print(f"  CUDAåˆ©ç”¨å¯èƒ½: ã„ã„ãˆ")
    else:
        print(f"  PyTorchæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆGPUæ©Ÿèƒ½ç„¡åŠ¹ï¼‰")

    results['basic_info'] = {
        'img1_size': [int(img1.shape[1]), int(img1.shape[0])],
        'img2_size': [int(img2.shape[1]), int(img2.shape[0])],
        'img1_filesize_mb': round(size1, 2),
        'img2_filesize_mb': round(size2, 2),
        'gpu_available': GPU_AVAILABLE,
        'gpu_name': GPU_NAME,
        'device': str(DEVICE) if DEVICE else 'N/A'
    }

    # 2. æ§‹é€ é¡ä¼¼æ€§ï¼ˆSSIMï¼‰
    print("\nã€2. æ§‹é€ é¡ä¼¼æ€§ï¼ˆSSIMï¼‰ã€‘")
    print("1.0 = å®Œå…¨ä¸€è‡´ã€0.0 = å…¨ãé•ã†")
    if GPU_AVAILABLE:
        print(f"[GPUå‡¦ç†] ãƒ‡ãƒã‚¤ã‚¹: {DEVICE}")
    print_usage_status("SSIMè¨ˆç®—é–‹å§‹ï¼ˆGPUä½¿ç”¨ï¼‰" if GPU_AVAILABLE else "SSIMè¨ˆç®—é–‹å§‹ï¼ˆCPUä½¿ç”¨ï¼‰")

    if img_original_rgb is not None:
        # å…ƒç”»åƒãŒã‚ã‚‹å ´åˆï¼šãã‚Œãã‚Œå…ƒç”»åƒã¨ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
        ssim_img1_vs_orig = calculate_ssim_gpu(img1_rgb, img_original_rgb)
        ssim_img2_vs_orig = calculate_ssim_gpu(img2_rgb, img_original_rgb)
        print(f"ç”»åƒ1 vs å…ƒç”»åƒ SSIM: {ssim_img1_vs_orig:.4f}")
        print(f"ç”»åƒ2 vs å…ƒç”»åƒ SSIM: {ssim_img2_vs_orig:.4f}")
        if ssim_img1_vs_orig > ssim_img2_vs_orig:
            print(f"â†’ å…ƒç”»åƒã®æ–¹ãŒå…ƒç”»åƒã«è¿‘ã„ (+{(ssim_img1_vs_orig - ssim_img2_vs_orig):.4f})")
        else:
            print(f"â†’ AIå‡¦ç†çµæœã®æ–¹ãŒå…ƒç”»åƒã«è¿‘ã„ (+{(ssim_img2_vs_orig - ssim_img1_vs_orig):.4f})")
        results['ssim'] = {
            'img1_vs_original': round(ssim_img1_vs_orig, 4),
            'img2_vs_original': round(ssim_img2_vs_orig, 4)
        }
    else:
        # å…ƒç”»åƒãŒãªã„å ´åˆï¼šå…ƒç”»åƒ vs AIå‡¦ç†çµæœ
        ssim_score = calculate_ssim_gpu(img1_rgb, img2_rgb)
        print(f"SSIM (å…ƒç”»åƒ vs AIå‡¦ç†çµæœ): {ssim_score:.4f}")
        results['ssim'] = round(ssim_score, 4)

    # 2.5. MS-SSIMï¼ˆMulti-Scale SSIMï¼‰
    print("\nã€2.5. MS-SSIMï¼ˆãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«SSIMï¼‰ã€‘")
    print("è¤‡æ•°ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®æ§‹é€ é¡ä¼¼æ€§ï¼ˆ1.0ã«è¿‘ã„ã»ã©é¡ä¼¼ï¼‰")
    print_usage_status("MS-SSIMè¨ˆç®—é–‹å§‹")
    ms_ssim_score = calculate_ms_ssim(img1_rgb, img2_rgb)

    if ms_ssim_score is not None:
        print(f"MS-SSIM: {ms_ssim_score:.4f}")
        if ms_ssim_score >= 0.99:
            print("  è©•ä¾¡: ã»ã¼å®Œå…¨ã«ä¸€è‡´")
        elif ms_ssim_score >= 0.95:
            print("  è©•ä¾¡: éå¸¸ã«é¡ä¼¼")
        elif ms_ssim_score >= 0.90:
            print("  è©•ä¾¡: é¡ä¼¼")
        elif ms_ssim_score >= 0.80:
            print("  è©•ä¾¡: ã‚„ã‚„é¡ä¼¼")
        else:
            print("  è©•ä¾¡: ç•°ãªã‚‹")
        results['ms_ssim'] = round(ms_ssim_score, 4)
    else:
        print("  â€»MS-SSIMè¨ˆç®—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸï¼ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰")
        results['ms_ssim'] = None

    # 3. PSNR
    print("\nã€3. PSNRï¼ˆãƒ”ãƒ¼ã‚¯ä¿¡å·å¯¾é›‘éŸ³æ¯”ï¼‰ã€‘")
    print("æ•°å€¤ãŒé«˜ã„ã»ã©é¡ä¼¼ï¼ˆ30dBä»¥ä¸Šã§è¦–è¦šçš„ã«ã»ã¼åŒä¸€ï¼‰")
    print_usage_status("PSNRè¨ˆç®—é–‹å§‹ï¼ˆGPUä½¿ç”¨ï¼‰" if GPU_AVAILABLE else "PSNRè¨ˆç®—é–‹å§‹ï¼ˆCPUä½¿ç”¨ï¼‰")

    if img_original_rgb is not None:
        # å…ƒç”»åƒãŒã‚ã‚‹å ´åˆï¼šãã‚Œãã‚Œå…ƒç”»åƒã¨ã®PSNRã‚’è¨ˆç®—
        psnr_img1_vs_orig = calculate_psnr_gpu(img1_rgb, img_original_rgb)
        psnr_img2_vs_orig = calculate_psnr_gpu(img2_rgb, img_original_rgb)
        print(f"ç”»åƒ1 vs å…ƒç”»åƒ PSNR: {psnr_img1_vs_orig:.2f} dB")
        print(f"ç”»åƒ2 vs å…ƒç”»åƒ PSNR: {psnr_img2_vs_orig:.2f} dB")
        if psnr_img1_vs_orig > psnr_img2_vs_orig:
            print(f"â†’ å…ƒç”»åƒã®æ–¹ãŒå…ƒç”»åƒã«è¿‘ã„ (+{(psnr_img1_vs_orig - psnr_img2_vs_orig):.2f} dB)")
        else:
            print(f"â†’ AIå‡¦ç†çµæœã®æ–¹ãŒå…ƒç”»åƒã«è¿‘ã„ (+{(psnr_img2_vs_orig - psnr_img1_vs_orig):.2f} dB)")
        results['psnr'] = {
            'img1_vs_original': round(psnr_img1_vs_orig, 2),
            'img2_vs_original': round(psnr_img2_vs_orig, 2)
        }
    else:
        # å…ƒç”»åƒãŒãªã„å ´åˆï¼šå…ƒç”»åƒ vs AIå‡¦ç†çµæœ
        psnr_score = calculate_psnr_gpu(img1_rgb, img2_rgb)
        print(f"PSNR (å…ƒç”»åƒ vs AIå‡¦ç†çµæœ): {psnr_score:.2f} dB")
        results['psnr'] = round(psnr_score, 2)

    # 3.4. ãƒ”ã‚¯ã‚»ãƒ«å·®åˆ†ï¼ˆMAE - å¹³å‡çµ¶å¯¾èª¤å·®ï¼‰
    print("\nã€3.4. ãƒ”ã‚¯ã‚»ãƒ«å·®åˆ†ï¼ˆMAEï¼‰ã€‘")
    print("å…ƒç”»åƒã¨ã®ãƒ”ã‚¯ã‚»ãƒ«å˜ä½ã§ã®çµ¶å¯¾å·®åˆ†ï¼ˆä½ã„ã»ã©è¿‘ã„ã€0=å®Œå…¨ä¸€è‡´ï¼‰")

    if img_original_rgb is not None:
        # å…ƒç”»åƒãŒã‚ã‚‹å ´åˆï¼šãã‚Œãã‚Œå…ƒç”»åƒã¨ã®å·®åˆ†ã‚’è¨ˆç®—
        diff_img1 = np.abs(img1_rgb.astype(float) - img_original_rgb.astype(float))
        diff_img2 = np.abs(img2_rgb.astype(float) - img_original_rgb.astype(float))

        # å…¨ä½“ã®MAE
        mae_img1 = np.mean(diff_img1)
        mae_img2 = np.mean(diff_img2)

        print(f"ğŸ“Š å…¨ä½“MAE:")
        print(f"  è¶…è§£åƒçµæœ1ï¼ˆAfterï¼‰ vs å…ƒç”»åƒï¼ˆBeforeï¼‰: {mae_img1:.2f} (å·®åˆ†ç‡: {(mae_img1/255)*100:.1f}%)")
        print(f"  è¶…è§£åƒçµæœ2ï¼ˆAfterï¼‰ vs å…ƒç”»åƒï¼ˆBeforeï¼‰: {mae_img2:.2f} (å·®åˆ†ç‡: {(mae_img2/255)*100:.1f}%)")

        # ãƒ†ã‚­ã‚¹ãƒˆé ˜åŸŸã®ã¿ã®MAEï¼ˆç™½èƒŒæ™¯ã‚’é™¤å¤–ï¼‰
        # RGBå¹³å‡ãŒ200æœªæº€ã®ãƒ”ã‚¯ã‚»ãƒ«ã‚’ã€Œãƒ†ã‚­ã‚¹ãƒˆé ˜åŸŸã€ã¨ã¿ãªã™
        text_mask_img1 = np.mean(img1_rgb, axis=2) < 200
        text_mask_img2 = np.mean(img2_rgb, axis=2) < 200
        text_mask_original = np.mean(img_original_rgb, axis=2) < 200
        # 3ã¤ã®ç”»åƒã®ã„ãšã‚Œã‹ã«ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹é ˜åŸŸã‚’çµ±åˆ
        text_mask_combined = text_mask_img1 | text_mask_img2 | text_mask_original

        # ãƒ†ã‚­ã‚¹ãƒˆé ˜åŸŸã®ãƒ”ã‚¯ã‚»ãƒ«æ•°ã‚’ç¢ºèª
        text_pixel_count = np.sum(text_mask_combined)
        total_pixel_count = text_mask_combined.size
        text_ratio = text_pixel_count / total_pixel_count

        if text_pixel_count > 0:
            # ãƒ†ã‚­ã‚¹ãƒˆé ˜åŸŸãŒå­˜åœ¨ã™ã‚‹å ´åˆ
            mae_text_img1 = np.mean(diff_img1[text_mask_combined])
            mae_text_img2 = np.mean(diff_img2[text_mask_combined])

            print(f"\nğŸ“ ãƒ†ã‚­ã‚¹ãƒˆé ˜åŸŸMAEï¼ˆç™½èƒŒæ™¯é™¤å¤–ã€{text_ratio*100:.1f}%ã®é ˜åŸŸï¼‰:")
            print(f"  è¶…è§£åƒçµæœ1ï¼ˆAfterï¼‰ vs å…ƒç”»åƒï¼ˆBeforeï¼‰: {mae_text_img1:.2f} (å·®åˆ†ç‡: {(mae_text_img1/255)*100:.1f}%)")
            print(f"  è¶…è§£åƒçµæœ2ï¼ˆAfterï¼‰ vs å…ƒç”»åƒï¼ˆBeforeï¼‰: {mae_text_img2:.2f} (å·®åˆ†ç‡: {(mae_text_img2/255)*100:.1f}%)")
        else:
            # ãƒ†ã‚­ã‚¹ãƒˆé ˜åŸŸãŒãªã„å ´åˆï¼ˆç´”ç²‹ãªç™½ç”»åƒãªã©ï¼‰
            mae_text_img1 = None
            mae_text_img2 = None
            print(f"\n  âš ï¸  ãƒ†ã‚­ã‚¹ãƒˆé ˜åŸŸãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸï¼ˆç™½èƒŒæ™¯ã®ã¿ã®ç”»åƒï¼‰")

        # å…¨ä½“MAEã§ã®æ¯”è¼ƒ
        print(f"\nğŸ’¡ å…¨ä½“MAEæ¯”è¼ƒ:")
        if mae_img1 < mae_img2:
            print(f"  â†’ è¶…è§£åƒçµæœ1ã®æ–¹ãŒå…ƒç”»åƒï¼ˆBeforeï¼‰ã«è¿‘ã„ (å·®åˆ†å·®: {mae_img2 - mae_img1:.2f})")
        else:
            print(f"  â†’ è¶…è§£åƒçµæœ2ã®æ–¹ãŒå…ƒç”»åƒï¼ˆBeforeï¼‰ã«è¿‘ã„ (å·®åˆ†å·®: {mae_img1 - mae_img2:.2f})")

        # ãƒ†ã‚­ã‚¹ãƒˆMAEã§ã®æ¯”è¼ƒã¨è©•ä¾¡ï¼ˆã‚ˆã‚Šé‡è¦ï¼‰
        if mae_text_img1 is not None and mae_text_img2 is not None:
            print(f"\nğŸ¯ ãƒ†ã‚­ã‚¹ãƒˆé ˜åŸŸMAEæ¯”è¼ƒï¼ˆé‡è¦åº¦ï¼šé«˜ï¼‰:")
            if mae_text_img1 < mae_text_img2:
                print(f"  â†’ è¶…è§£åƒçµæœ1ã®æ–¹ãŒå…ƒç”»åƒï¼ˆBeforeï¼‰ã«è¿‘ã„ (å·®åˆ†å·®: {mae_text_img2 - mae_text_img1:.2f})")
            else:
                print(f"  â†’ è¶…è§£åƒçµæœ2ã®æ–¹ãŒå…ƒç”»åƒï¼ˆBeforeï¼‰ã«è¿‘ã„ (å·®åˆ†å·®: {mae_text_img1 - mae_text_img2:.2f})")

            # ãƒ†ã‚­ã‚¹ãƒˆé ˜åŸŸã§ã®è©•ä¾¡åŸºæº–ï¼ˆã‚ˆã‚Šå³æ ¼ï¼‰
            print(f"\n  ã€ãƒ†ã‚­ã‚¹ãƒˆé ˜åŸŸã®è©•ä¾¡ã€‘")
            for idx, mae_text_val in enumerate([mae_text_img1, mae_text_img2], 1):
                if mae_text_val < 10:
                    eval_str = "ã»ã¼å®Œå…¨ä¸€è‡´ï¼ˆåŒã˜å†…å®¹ï¼‰âœ…"
                elif mae_text_val < 30:
                    eval_str = "é¡ä¼¼ï¼ˆä¸€éƒ¨ç•°ãªã‚‹å¯èƒ½æ€§ï¼‰"
                elif mae_text_val < 60:
                    eval_str = "âš ï¸ æ˜ã‚‰ã‹ã«ç•°ãªã‚‹å†…å®¹"
                else:
                    eval_str = "ğŸš¨ å…¨ãç•°ãªã‚‹ç”»åƒï¼ˆåˆ¥ã®æ–‡æ›¸/åˆ¥ã®æ‚£è€…ï¼‰"
                print(f"  è¶…è§£åƒçµæœ{idx}: {eval_str}")

        results['mae'] = {
            'img1_vs_original': round(mae_img1, 2),
            'img2_vs_original': round(mae_img2, 2),
            'img1_diff_ratio': round((mae_img1/255)*100, 2),
            'img2_diff_ratio': round((mae_img2/255)*100, 2),
            'img1_text_mae': round(mae_text_img1, 2) if mae_text_img1 is not None else None,
            'img2_text_mae': round(mae_text_img2, 2) if mae_text_img2 is not None else None,
            'text_region_ratio': round(text_ratio * 100, 2) if text_pixel_count > 0 else 0
        }
    else:
        # å…ƒç”»åƒãŒãªã„å ´åˆï¼šå…ƒç”»åƒ vs AIå‡¦ç†çµæœ
        mae_score = np.mean(np.abs(img1_rgb.astype(float) - img2_rgb.astype(float)))
        print(f"MAE (å…ƒç”»åƒ vs AIå‡¦ç†çµæœ): {mae_score:.2f} (å·®åˆ†ç‡: {(mae_score/255)*100:.1f}%)")

        if mae_score < 5:
            print("  è©•ä¾¡: ã»ã¼å®Œå…¨ä¸€è‡´")
        elif mae_score < 10:
            print("  è©•ä¾¡: éå¸¸ã«é¡ä¼¼")
        elif mae_score < 20:
            print("  è©•ä¾¡: é¡ä¼¼")
        elif mae_score < 40:
            print("  è©•ä¾¡: ã‚„ã‚„ç•°ãªã‚‹")
        else:
            print("  è©•ä¾¡: å¤§ããç•°ãªã‚‹")

        results['mae'] = {
            'value': round(mae_score, 2),
            'diff_ratio': round((mae_score/255)*100, 2)
        }

    # 3.5. LPIPSï¼ˆçŸ¥è¦šçš„é¡ä¼¼åº¦ï¼‰
    print("\nã€3.5. LPIPSï¼ˆçŸ¥è¦šçš„é¡ä¼¼åº¦ï¼‰ã€‘")
    print("æ·±å±¤å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®çŸ¥è¦šçš„é¡ä¼¼åº¦ï¼ˆ0ã«è¿‘ã„ã»ã©é¡ä¼¼ï¼‰")
    print_usage_status("LPIPSè¨ˆç®—é–‹å§‹")
    lpips_score, gpu_usage = calculate_lpips(img1_rgb, img2_rgb)
    print_usage_status("LPIPSè¨ˆç®—å®Œäº†")

    if lpips_score is not None:
        print(f"LPIPS: {lpips_score:.4f}")
        if GPU_AVAILABLE and gpu_usage is not None:
            print(f"  GPUä½¿ç”¨: ã¯ã„ï¼ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {gpu_usage:.1f}%ï¼‰")
        elif GPU_AVAILABLE:
            print(f"  GPUä½¿ç”¨: ã¯ã„")
        else:
            print(f"  GPUä½¿ç”¨: ã„ã„ãˆï¼ˆCPUå‡¦ç†ï¼‰")

        if lpips_score < 0.1:
            print("  è©•ä¾¡: çŸ¥è¦šçš„ã«ã»ã¼åŒä¸€")
        elif lpips_score < 0.3:
            print("  è©•ä¾¡: çŸ¥è¦šçš„ã«é¡ä¼¼")
        elif lpips_score < 0.5:
            print("  è©•ä¾¡: ã‚„ã‚„ç•°ãªã‚‹")
        else:
            print("  è©•ä¾¡: å¤§ããç•°ãªã‚‹")
        results['lpips'] = round(lpips_score, 4)
    else:
        print("  â€»LPIPSè¨ˆç®—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸï¼ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰")
        results['lpips'] = None

    # 3.6. CLIP Embeddingsï¼ˆæ„å‘³çš„é¡ä¼¼åº¦ï¼‰
    print("\nã€3.6. CLIP Embeddingsï¼ˆæ„å‘³çš„é¡ä¼¼åº¦ï¼‰ã€‘")
    print("OpenAI CLIP ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹æ„å‘³çš„é¡ä¼¼åº¦ï¼ˆ1.0ã«è¿‘ã„ã»ã©æ„å‘³çš„ã«é¡ä¼¼ï¼‰")
    print_usage_status("CLIPè¨ˆç®—é–‹å§‹")

    # æ–‡æ›¸ç”»åƒæ¤œå‡ºï¼ˆCLIPãŒè‹¦æ‰‹ã¨ã™ã‚‹ç”»åƒã‚¿ã‚¤ãƒ—ï¼‰
    is_doc_img1 = is_document_image(img1_rgb)
    is_doc_img2 = is_document_image(img2_rgb)
    is_doc_original = is_document_image(img_original_rgb) if img_original_rgb is not None else False
    is_any_document_detected = is_doc_img1 or is_doc_img2 or is_doc_original

    # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã‚’è€ƒæ…®
    if evaluation_mode == 'document':
        # æ–‡æ›¸ãƒ¢ãƒ¼ãƒ‰ï¼šå¼·åˆ¶çš„ã«æ–‡æ›¸ã¨ã—ã¦æ‰±ã†
        is_any_document = True
        print("ğŸ“„ è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰: æ–‡æ›¸ãƒ¢ãƒ¼ãƒ‰ï¼ˆå³æ ¼ãªåŸºæº–ã§è©•ä¾¡ï¼‰")
    elif evaluation_mode == 'developer':
        # é–‹ç™ºè€…ãƒ¢ãƒ¼ãƒ‰ï¼šè‡ªå‹•æ¤œå‡ºçµæœã‚’ä½¿ç”¨
        is_any_document = is_any_document_detected
        print("ğŸ”§ è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰: é–‹ç™ºè€…ãƒ¢ãƒ¼ãƒ‰ï¼ˆå‚è€ƒæƒ…å ±ã¨ã—ã¦è¡¨ç¤ºï¼‰")
    else:
        # ç”»åƒãƒ¢ãƒ¼ãƒ‰ï¼šè‡ªå‹•æ¤œå‡ºçµæœã‚’ä½¿ç”¨
        is_any_document = is_any_document_detected
        if is_any_document:
            print("ğŸ“„ æ–‡æ›¸ç”»åƒã‚’è‡ªå‹•æ¤œå‡ºï¼ˆæ–‡æ›¸ãƒ¢ãƒ¼ãƒ‰ã®ä½¿ç”¨ã‚’æ¨å¥¨ï¼‰")

    if img_original_rgb is not None:
        # å…ƒç”»åƒãŒã‚ã‚‹å ´åˆï¼šãã‚Œãã‚Œå…ƒç”»åƒã¨ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
        clip_img1_vs_orig = calculate_clip_similarity(img1_rgb, img_original_rgb)
        clip_img2_vs_orig = calculate_clip_similarity(img2_rgb, img_original_rgb)
        print_usage_status("CLIPè¨ˆç®—å®Œäº†")

        if clip_img1_vs_orig is not None and clip_img2_vs_orig is not None:
            print(f"ç”»åƒ1 vs å…ƒç”»åƒ CLIP: {clip_img1_vs_orig:.4f}")
            print(f"ç”»åƒ2 vs å…ƒç”»åƒ CLIP: {clip_img2_vs_orig:.4f}")
            if GPU_AVAILABLE:
                print(f"  GPUä½¿ç”¨: ã¯ã„")
            else:
                print(f"  GPUä½¿ç”¨: ã„ã„ãˆï¼ˆCPUå‡¦ç†ï¼‰")

            if clip_img1_vs_orig > clip_img2_vs_orig:
                print(f"â†’ å…ƒç”»åƒã®æ–¹ãŒå…ƒç”»åƒã«æ„å‘³çš„ã«è¿‘ã„ (+{(clip_img1_vs_orig - clip_img2_vs_orig):.4f})")
            else:
                print(f"â†’ AIå‡¦ç†çµæœã®æ–¹ãŒå…ƒç”»åƒã«æ„å‘³çš„ã«è¿‘ã„ (+{(clip_img2_vs_orig - clip_img1_vs_orig):.4f})")

            # å„ç”»åƒã®è©•ä¾¡ï¼ˆæ–‡æ›¸ç”»åƒã®å ´åˆã¯å³æ ¼ãªåŸºæº–ã‚’é©ç”¨ï¼‰
            if is_any_document:
                print("  âš ï¸  æ–‡æ›¸/ã‚«ãƒ«ãƒ†ç”»åƒã‚’æ¤œå‡º: CLIPã¯å³æ ¼ãªåŸºæº–ã§è©•ä¾¡ã—ã¾ã™")
                # æ–‡æ›¸ç”»åƒç”¨ã®å³æ ¼ãªé–¾å€¤
                for idx, clip_val in enumerate([clip_img1_vs_orig, clip_img2_vs_orig], 1):
                    if clip_val > 0.98:
                        eval_str = "æ„å‘³çš„ã«ã»ã¼åŒä¸€"
                    elif clip_val > 0.95:
                        eval_str = "æ„å‘³çš„ã«é¡ä¼¼ï¼ˆè¦æ³¨æ„ï¼šæ–‡æ›¸ã¯æ§‹é€ é¡ä¼¼ã§é«˜ã‚¹ã‚³ã‚¢ã«ãªã‚Šã‚„ã™ã„ï¼‰"
                    elif clip_val > 0.90:
                        eval_str = "âš ï¸ æ§‹é€ ã¯é¡ä¼¼ã ãŒå†…å®¹ã¯ç•°ãªã‚‹å¯èƒ½æ€§ ğŸ”"
                    else:
                        eval_str = "å…¨ãç•°ãªã‚‹ç”»åƒï¼ˆå†…å®¹ãŒé•ã†ï¼‰ğŸš¨"
                    print(f"  ç”»åƒ{idx}: {eval_str}")
            else:
                # è‡ªç„¶ç”»åƒç”¨ã®é€šå¸¸é–¾å€¤
                for idx, clip_val in enumerate([clip_img1_vs_orig, clip_img2_vs_orig], 1):
                    if clip_val > 0.95:
                        eval_str = "æ„å‘³çš„ã«ã»ã¼åŒä¸€"
                    elif clip_val > 0.85:
                        eval_str = "æ„å‘³çš„ã«éå¸¸ã«é¡ä¼¼"
                    elif clip_val > 0.70:
                        eval_str = "æ„å‘³çš„ã«é¡ä¼¼"
                    elif clip_val > 0.50:
                        eval_str = "ã‚„ã‚„é¡ä¼¼"
                    else:
                        eval_str = "å…¨ãç•°ãªã‚‹ç”»åƒï¼ˆå†…å®¹ãŒé•ã†ï¼‰ğŸš¨"
                    print(f"  ç”»åƒ{idx}: {eval_str}")

            results['clip_similarity'] = {
                'img1_vs_original': round(clip_img1_vs_orig, 4),
                'img2_vs_original': round(clip_img2_vs_orig, 4),
                'is_document': is_any_document  # æ–‡æ›¸ç”»åƒãƒ•ãƒ©ã‚°ã‚’è¿½åŠ 
            }
        else:
            print("  â€»CLIPè¨ˆç®—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸï¼ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰")
            results['clip_similarity'] = None
    else:
        # å…ƒç”»åƒãŒãªã„å ´åˆï¼šå…ƒç”»åƒ vs AIå‡¦ç†çµæœ
        clip_similarity = calculate_clip_similarity(img1_rgb, img2_rgb)
        print_usage_status("CLIPè¨ˆç®—å®Œäº†")

        if clip_similarity is not None:
            print(f"CLIP ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦: {clip_similarity:.4f}")
            if GPU_AVAILABLE:
                print(f"  GPUä½¿ç”¨: ã¯ã„")
            else:
                print(f"  GPUä½¿ç”¨: ã„ã„ãˆï¼ˆCPUå‡¦ç†ï¼‰")

            # æ–‡æ›¸ç”»åƒã®å ´åˆã¯å³æ ¼ãªåŸºæº–ã‚’é©ç”¨
            if is_any_document:
                print("  âš ï¸  æ–‡æ›¸/ã‚«ãƒ«ãƒ†ç”»åƒã‚’æ¤œå‡º: CLIPã¯å³æ ¼ãªåŸºæº–ã§è©•ä¾¡ã—ã¾ã™")
                if clip_similarity > 0.98:
                    print("  è©•ä¾¡: æ„å‘³çš„ã«ã»ã¼åŒä¸€ã®ç”»åƒ")
                elif clip_similarity > 0.95:
                    print("  è©•ä¾¡: æ„å‘³çš„ã«é¡ä¼¼ï¼ˆè¦æ³¨æ„ï¼šæ–‡æ›¸ã¯æ§‹é€ é¡ä¼¼ã§é«˜ã‚¹ã‚³ã‚¢ã«ãªã‚Šã‚„ã™ã„ï¼‰")
                elif clip_similarity > 0.90:
                    print("  è©•ä¾¡: âš ï¸ æ§‹é€ ã¯é¡ä¼¼ã ãŒå†…å®¹ã¯ç•°ãªã‚‹å¯èƒ½æ€§ ğŸ”")
                else:
                    print("  è©•ä¾¡: å…¨ãç•°ãªã‚‹ç”»åƒï¼ˆå†…å®¹ãŒé•ã†ï¼‰")
            else:
                # è‡ªç„¶ç”»åƒç”¨ã®é€šå¸¸é–¾å€¤
                if clip_similarity > 0.95:
                    print("  è©•ä¾¡: æ„å‘³çš„ã«ã»ã¼åŒä¸€ã®ç”»åƒ")
                elif clip_similarity > 0.85:
                    print("  è©•ä¾¡: æ„å‘³çš„ã«éå¸¸ã«é¡ä¼¼")
                elif clip_similarity > 0.70:
                    print("  è©•ä¾¡: æ„å‘³çš„ã«é¡ä¼¼")
                elif clip_similarity > 0.50:
                    print("  è©•ä¾¡: ã‚„ã‚„é¡ä¼¼")
                else:
                    print("  è©•ä¾¡: å…¨ãç•°ãªã‚‹ç”»åƒï¼ˆå†…å®¹ãŒé•ã†ï¼‰")

            results['clip_similarity'] = {
                'value': round(clip_similarity, 4),
                'is_document': is_any_document
            }
        else:
            print("  â€»CLIPè¨ˆç®—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸï¼ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰")
            results['clip_similarity'] = None

    # 4. ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹ï¼ˆé®®é‹­åº¦ï¼‰
    print("\nã€4. ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹ï¼ˆé®®é‹­åº¦ï¼‰ã€‘")
    print_usage_status("ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹è¨ˆç®—é–‹å§‹ï¼ˆGPUä½¿ç”¨ï¼‰" if GPU_AVAILABLE else "ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹è¨ˆç®—é–‹å§‹ï¼ˆCPUä½¿ç”¨ï¼‰")
    sharpness1 = calculate_sharpness_gpu(img1_gray)
    sharpness2 = calculate_sharpness_gpu(img2_gray)
    print(f"ç”»åƒ1ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹: {sharpness1:.2f}")
    print(f"ç”»åƒ2ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹: {sharpness2:.2f}")
    print(f"å·®: {abs(sharpness1 - sharpness2):.2f} ({((sharpness2/sharpness1 - 1) * 100):+.1f}%)")

    results['sharpness'] = {
        'img1': round(sharpness1, 2),
        'img2': round(sharpness2, 2),
        'difference_pct': round((sharpness2/sharpness1 - 1) * 100, 1)
    }

    # 5. ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ
    print("\nã€5. ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆã€‘")
    contrast1 = calculate_contrast(img1_gray)
    contrast2 = calculate_contrast(img2_gray)
    print(f"ç”»åƒ1ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ: {contrast1:.2f}")
    print(f"ç”»åƒ2ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ: {contrast2:.2f}")
    print(f"å·®: {abs(contrast1 - contrast2):.2f} ({((contrast2/contrast1 - 1) * 100):+.1f}%)")

    results['contrast'] = {
        'img1': round(contrast1, 2),
        'img2': round(contrast2, 2),
        'difference_pct': round((contrast2/contrast1 - 1) * 100, 1)
    }

    # 6. ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆæƒ…å ±é‡ï¼‰
    print("\nã€6. ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆæƒ…å ±é‡ï¼‰ã€‘")
    print("æ•°å€¤ãŒé«˜ã„ã»ã©æƒ…å ±é‡ãŒå¤šã„ï¼ˆè¤‡é›‘ï¼‰")
    entropy1 = calculate_entropy(img1_gray)
    entropy2 = calculate_entropy(img2_gray)
    print(f"ç”»åƒ1ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: {entropy1:.3f}")
    print(f"ç”»åƒ2ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: {entropy2:.3f}")
    print(f"å·®: {abs(entropy1 - entropy2):.3f}")

    results['entropy'] = {
        'img1': round(entropy1, 3),
        'img2': round(entropy2, 3)
    }

    # 7. ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«
    print("\nã€7. ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«åˆ†æã€‘")
    print_usage_status("ãƒã‚¤ã‚ºæ¨å®šé–‹å§‹ï¼ˆGPUä½¿ç”¨ï¼‰" if GPU_AVAILABLE else "ãƒã‚¤ã‚ºæ¨å®šé–‹å§‹ï¼ˆCPUä½¿ç”¨ï¼‰")
    noise1 = estimate_noise_gpu(img1_gray)
    noise2 = estimate_noise_gpu(img2_gray)
    print(f"ç”»åƒ1ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«: {noise1:.2f}")
    print(f"ç”»åƒ2ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«: {noise2:.2f}")
    print(f"å·®: {abs(noise1 - noise2):.2f} ({((noise2/noise1 - 1) * 100 if noise1 != 0 else 0):+.1f}%)")

    results['noise'] = {
        'img1': round(noise1, 2),
        'img2': round(noise2, 2)
    }

    # 8. ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆæ¤œå‡º
    print("\nã€8. ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆæ¤œå‡ºã€‘")
    block_noise1, ringing1 = detect_artifacts(img1_gray)
    block_noise2, ringing2 = detect_artifacts(img2_gray)

    print(f"ç”»åƒ1ãƒ–ãƒ­ãƒƒã‚¯ãƒã‚¤ã‚º: {block_noise1:.2f}")
    print(f"ç”»åƒ2ãƒ–ãƒ­ãƒƒã‚¯ãƒã‚¤ã‚º: {block_noise2:.2f}")
    print(f"ç”»åƒ1ãƒªãƒ³ã‚®ãƒ³ã‚°: {ringing1:.2f}")
    print(f"ç”»åƒ2ãƒªãƒ³ã‚®ãƒ³ã‚°: {ringing2:.2f}")

    results['artifacts'] = {
        'img1_block_noise': round(block_noise1, 2),
        'img2_block_noise': round(block_noise2, 2),
        'img1_ringing': round(ringing1, 2),
        'img2_ringing': round(ringing2, 2)
    }

    # 9. ã‚¨ãƒƒã‚¸ä¿æŒç‡
    print("\nã€9. ã‚¨ãƒƒã‚¸ä¿æŒç‡ã€‘")
    print_usage_status("ã‚¨ãƒƒã‚¸æ¤œå‡ºé–‹å§‹ï¼ˆGPUä½¿ç”¨ï¼‰" if GPU_AVAILABLE else "ã‚¨ãƒƒã‚¸æ¤œå‡ºé–‹å§‹ï¼ˆCPUä½¿ç”¨ï¼‰")
    edge_density1 = detect_edges_gpu(img1_gray)
    edge_density2 = detect_edges_gpu(img2_gray)

    print(f"ç”»åƒ1ã‚¨ãƒƒã‚¸å¯†åº¦: {edge_density1:.2f}%")
    print(f"ç”»åƒ2ã‚¨ãƒƒã‚¸å¯†åº¦: {edge_density2:.2f}%")
    print(f"å·®: {abs(edge_density1 - edge_density2):.2f}% ({((edge_density2/edge_density1 - 1) * 100 if edge_density1 != 0 else 0):+.1f}%)")

    results['edges'] = {
        'img1_density': round(edge_density1, 2),
        'img2_density': round(edge_density2, 2),
        'difference_pct': round((edge_density2/edge_density1 - 1) * 100 if edge_density1 != 0 else 0, 1)
    }

    # 10. è‰²åˆ†å¸ƒåˆ†æ
    print("\nã€10. è‰²åˆ†å¸ƒåˆ†æï¼ˆRGB/HSV/LABï¼‰ã€‘")
    color_stats1 = analyze_color_distribution(img1_rgb)
    color_stats2 = analyze_color_distribution(img2_rgb)

    # RGB
    print("RGBè‰²ç©ºé–“:")
    for channel in ['Red', 'Green', 'Blue']:
        print(f"  {channel}ãƒãƒ£ãƒ³ãƒãƒ«:")
        print(f"    å…ƒç”»åƒ: å¹³å‡={color_stats1[channel]['mean']:.1f}, æ¨™æº–åå·®={color_stats1[channel]['std']:.1f}")
        print(f"    AIå‡¦ç†çµæœ: å¹³å‡={color_stats2[channel]['mean']:.1f}, æ¨™æº–åå·®={color_stats2[channel]['std']:.1f}")

    # HSV
    print(f"\nHSVè‰²ç©ºé–“ - å½©åº¦:")
    print(f"  å…ƒç”»åƒ: å¹³å‡={color_stats1['Saturation']['mean']:.1f}")
    print(f"  AIå‡¦ç†çµæœ: å¹³å‡={color_stats2['Saturation']['mean']:.1f}")

    # LABï¼ˆçŸ¥è¦šçš„è‰²å·®ï¼‰
    print(f"\nLABè‰²ç©ºé–“ï¼ˆçŸ¥è¦šçš„è‰²åˆ†æï¼‰:")
    print(f"  æ˜åº¦(L):")
    print(f"    å…ƒç”»åƒ: {color_stats1['LAB']['L_mean']:.1f} Â± {color_stats1['LAB']['L_std']:.1f}")
    print(f"    AIå‡¦ç†çµæœ: {color_stats2['LAB']['L_mean']:.1f} Â± {color_stats2['LAB']['L_std']:.1f}")
    print(f"  a(èµ¤-ç·‘):")
    print(f"    å…ƒç”»åƒ: {color_stats1['LAB']['a_mean']:.1f} Â± {color_stats1['LAB']['a_std']:.1f}")
    print(f"    AIå‡¦ç†çµæœ: {color_stats2['LAB']['a_mean']:.1f} Â± {color_stats2['LAB']['a_std']:.1f}")
    print(f"  b(é»„-é’):")
    print(f"    å…ƒç”»åƒ: {color_stats1['LAB']['b_mean']:.1f} Â± {color_stats1['LAB']['b_std']:.1f}")
    print(f"    AIå‡¦ç†çµæœ: {color_stats2['LAB']['b_mean']:.1f} Â± {color_stats2['LAB']['b_std']:.1f}")

    # Delta E (CIE2000) - çŸ¥è¦šçš„è‰²å·®
    print_usage_status("è‰²å·®è¨ˆç®—é–‹å§‹ï¼ˆGPUä½¿ç”¨ï¼‰" if GPU_AVAILABLE else "è‰²å·®è¨ˆç®—é–‹å§‹ï¼ˆCPUä½¿ç”¨ï¼‰")

    if img_original_rgb is not None:
        # å…ƒç”»åƒãŒã‚ã‚‹å ´åˆï¼šãã‚Œãã‚Œå…ƒç”»åƒã¨ã®è‰²å·®ã‚’è¨ˆç®—
        delta_e_img1_vs_orig = calculate_color_difference_gpu(img1_rgb, img_original_rgb)
        delta_e_img2_vs_orig = calculate_color_difference_gpu(img2_rgb, img_original_rgb)
        print(f"\n  ç”»åƒ1 vs å…ƒç”»åƒ Î”E: {delta_e_img1_vs_orig:.2f}")
        print(f"  ç”»åƒ2 vs å…ƒç”»åƒ Î”E: {delta_e_img2_vs_orig:.2f}")
        if delta_e_img1_vs_orig < delta_e_img2_vs_orig:
            print(f"  â†’ å…ƒç”»åƒã®æ–¹ãŒå…ƒç”»åƒã®è‰²ã«è¿‘ã„ (å·®: {delta_e_img2_vs_orig - delta_e_img1_vs_orig:.2f})")
        else:
            print(f"  â†’ AIå‡¦ç†çµæœã®æ–¹ãŒå…ƒç”»åƒã®è‰²ã«è¿‘ã„ (å·®: {delta_e_img1_vs_orig - delta_e_img2_vs_orig:.2f})")
        print(f"    (Î”E < 1: äººé–“ã®ç›®ã§ã¯åŒºåˆ¥ä¸å¯, Î”E < 5: è¨±å®¹ç¯„å›², Î”E > 10: æ˜ç¢ºãªé•ã„)")
        delta_e_result = {
            'img1_vs_original': round(delta_e_img1_vs_orig, 2),
            'img2_vs_original': round(delta_e_img2_vs_orig, 2)
        }
    else:
        # å…ƒç”»åƒãŒãªã„å ´åˆï¼šå…ƒç”»åƒ vs AIå‡¦ç†çµæœ
        delta_e_val = calculate_color_difference_gpu(img1_rgb, img2_rgb)
        print(f"\n  Î”E (è‰²å·®): {delta_e_val:.2f}")
        print(f"    (Î”E < 1: äººé–“ã®ç›®ã§ã¯åŒºåˆ¥ä¸å¯, Î”E < 5: è¨±å®¹ç¯„å›², Î”E > 10: æ˜ç¢ºãªé•ã„)")
        delta_e_result = round(delta_e_val, 2)

    results['color_distribution'] = {
        'img1': color_stats1,
        'img2': color_stats2,
        'delta_e': delta_e_result
    }

    # 11. å‘¨æ³¢æ•°é ˜åŸŸåˆ†æ
    print("\nã€11. å‘¨æ³¢æ•°é ˜åŸŸåˆ†æï¼ˆFFTï¼‰ã€‘")
    freq_analysis1 = analyze_frequency_domain(img1_gray)
    freq_analysis2 = analyze_frequency_domain(img2_gray)

    print(f"ç”»åƒ1ä½å‘¨æ³¢æˆåˆ†æ¯”ç‡: {freq_analysis1['low_freq_ratio']:.3f}")
    print(f"ç”»åƒ2ä½å‘¨æ³¢æˆåˆ†æ¯”ç‡: {freq_analysis2['low_freq_ratio']:.3f}")
    print(f"ç”»åƒ1é«˜å‘¨æ³¢æˆåˆ†æ¯”ç‡: {freq_analysis1['high_freq_ratio']:.3f}")
    print(f"ç”»åƒ2é«˜å‘¨æ³¢æˆåˆ†æ¯”ç‡: {freq_analysis2['high_freq_ratio']:.3f}")

    results['frequency_analysis'] = {
        'img1': freq_analysis1,
        'img2': freq_analysis2
    }

    # 12. ãƒ†ã‚¯ã‚¹ãƒãƒ£åˆ†æ
    print("\nã€12. ãƒ†ã‚¯ã‚¹ãƒãƒ£åˆ†æã€‘")
    texture1 = analyze_texture(img1_gray)
    texture2 = analyze_texture(img2_gray)

    print(f"ç”»åƒ1ãƒ†ã‚¯ã‚¹ãƒãƒ£è¤‡é›‘åº¦: {texture1['texture_complexity']:.2f}")
    print(f"ç”»åƒ2ãƒ†ã‚¯ã‚¹ãƒãƒ£è¤‡é›‘åº¦: {texture2['texture_complexity']:.2f}")

    results['texture'] = {
        'img1': texture1,
        'img2': texture2
    }

    # 13. å±€æ‰€çš„å“è³ªåˆ†æ
    print("\nã€13. å±€æ‰€çš„å“è³ªåˆ†æï¼ˆãƒ‘ãƒƒãƒãƒ™ãƒ¼ã‚¹SSIMï¼‰ã€‘")
    local_ssim = analyze_local_quality(img1_rgb, img2_rgb)

    print(f"å±€æ‰€SSIM å¹³å‡: {np.mean(local_ssim):.4f}")
    print(f"å±€æ‰€SSIM æœ€å°: {np.min(local_ssim):.4f}")
    print(f"å±€æ‰€SSIM æœ€å¤§: {np.max(local_ssim):.4f}")
    print(f"å±€æ‰€SSIM æ¨™æº–åå·®: {np.std(local_ssim):.4f}")

    results['local_quality'] = {
        'mean_ssim': round(np.mean(local_ssim), 4),
        'min_ssim': round(np.min(local_ssim), 4),
        'max_ssim': round(np.max(local_ssim), 4),
        'std_ssim': round(np.std(local_ssim), 4)
    }

    # 14. ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ é¡ä¼¼åº¦
    print("\nã€14. ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ é¡ä¼¼åº¦ã€‘")
    hist1 = cv2.calcHist([img1_gray], [0], None, [256], [0, 256])
    hist2 = cv2.calcHist([img2_gray], [0], None, [256], [0, 256])

    hist_corr = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)
    print(f"ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ç›¸é–¢: {hist_corr:.4f} (1.0 = å®Œå…¨ä¸€è‡´)")

    results['histogram_correlation'] = round(hist_corr, 4)

    # 15. ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆçµ¶å¯¾è©•ä¾¡ï¼‰
    print("\nã€15. ç·åˆè©•ä¾¡ã‚¹ã‚³ã‚¢ã€‘")
    print("=" * 80)

    # å„æŒ‡æ¨™ã‚’çµ¶å¯¾å€¤ã§è©•ä¾¡ï¼ˆä¸¡ç”»åƒã‚’ç‹¬ç«‹ã—ã¦æ¡ç‚¹ï¼‰

    # SSIM/PSNRã®å€¤ã‚’å–å¾—ï¼ˆå…ƒç”»åƒã®æœ‰ç„¡ã§å½¢å¼ãŒç•°ãªã‚‹ï¼‰
    if img_original_rgb is not None:
        # å…ƒç”»åƒãŒã‚ã‚‹å ´åˆï¼šdictã‹ã‚‰å–å¾—
        ssim_data = results.get('ssim', {})
        if isinstance(ssim_data, dict):
            ssim_score_val = (ssim_data.get('img1_vs_original', 0) + ssim_data.get('img2_vs_original', 0)) / 2 * 100
        else:
            ssim_score_val = 0

        psnr_data = results.get('psnr', {})
        if isinstance(psnr_data, dict):
            psnr_score_val = min((psnr_data.get('img1_vs_original', 0) + psnr_data.get('img2_vs_original', 0)) / 2 * 2, 100)
        else:
            psnr_score_val = 0
    else:
        # å…ƒç”»åƒãŒãªã„å ´åˆï¼šfloatã‹ã‚‰å–å¾—
        ssim_score_val = results.get('ssim', 0) * 100
        psnr_score_val = min(results.get('psnr', 0) * 2, 100)

    # ç”»åƒ1ã®ã‚¹ã‚³ã‚¢ï¼ˆ17é …ç›®ï¼‰
    # 2. MS-SSIM
    ms_ssim_score_val = (results.get('ms_ssim', 0) or 0) * 100

    # 4. LPIPSï¼ˆä½ã„ã»ã©è‰¯ã„ã€åè»¢ï¼‰
    lpips_score_val = max(0, 100 - (results.get('lpips', 0) or 0) * 1000) if results.get('lpips') else 50

    # 5. ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹
    sharp1_score = min(sharpness1 / 5, 100)
    sharp2_score = min(sharpness2 / 5, 100)

    # 6. ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ
    contrast1_score = min(contrast1, 100)
    contrast2_score = min(contrast2, 100)

    # 7. ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
    entropy1_score = min(entropy1 / 8 * 100, 100)
    entropy2_score = min(entropy2 / 8 * 100, 100)

    # 8. ãƒã‚¤ã‚ºï¼ˆä½ã„ã»ã©è‰¯ã„ï¼‰
    noise1_score = max(0, 100 - noise1 / 2)
    noise2_score = max(0, 100 - noise2 / 2)

    # 9. ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆï¼ˆä½ã„ã»ã©è‰¯ã„ï¼‰
    artifact1_total = block_noise1 + ringing1
    artifact2_total = block_noise2 + ringing2
    artifact1_score = max(0, 100 - artifact1_total / 50)
    artifact2_score = max(0, 100 - artifact2_total / 50)

    # 10. ã‚¨ãƒƒã‚¸ä¿æŒ
    edge1_score = min(edge_density1 * 2, 100)
    edge2_score = min(edge_density2 * 2, 100)

    # 11. è‰²å·®ï¼ˆä½ã„ã»ã©è‰¯ã„ï¼‰
    delta_e_data = results['color_distribution'].get('delta_e', 0)
    if isinstance(delta_e_data, dict):
        # å…ƒç”»åƒãŒã‚ã‚‹å ´åˆï¼šå¹³å‡å€¤ã‚’ä½¿ç”¨
        avg_delta_e = (delta_e_data.get('img1_vs_original', 0) + delta_e_data.get('img2_vs_original', 0)) / 2
        color_diff_score = max(0, 100 - avg_delta_e * 2)
    else:
        # å…ƒç”»åƒãŒãªã„å ´åˆï¼šå˜ä¸€å€¤ã‚’ä½¿ç”¨
        color_diff_score = max(0, 100 - delta_e_data * 2)

    # 12. ãƒ†ã‚¯ã‚¹ãƒãƒ£
    texture1_score = min(texture1['texture_complexity'] * 10, 100)
    texture2_score = min(texture2['texture_complexity'] * 10, 100)

    # 13. å±€æ‰€å“è³ª
    local_quality_score = results['local_quality']['mean_ssim'] * 100

    # 14. ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
    histogram_score = hist_corr * 100

    # ç”»åƒ1ã®ç·åˆã‚¹ã‚³ã‚¢ï¼ˆç”»åƒå“è³ªé …ç›®ã®ã¿ï¼‰
    total1 = (sharp1_score + contrast1_score + entropy1_score + noise1_score +
              artifact1_score + edge1_score + texture1_score) / 7

    # ç”»åƒ2ã®ç·åˆã‚¹ã‚³ã‚¢ï¼ˆç”»åƒå“è³ªé …ç›®ã®ã¿ï¼‰
    total2 = (sharp2_score + contrast2_score + entropy2_score + noise2_score +
              artifact2_score + edge2_score + texture2_score) / 7

    print(f"ç”»åƒ1ç·åˆã‚¹ã‚³ã‚¢: {total1:.1f} / 100")
    print(f"ç”»åƒ2ç·åˆã‚¹ã‚³ã‚¢: {total2:.1f} / 100")

    if total2 > total1:
        print(f"â†’ ç”»åƒ2ãŒ {total2 - total1:.1f}ç‚¹ å„ªä½")
    elif total1 > total2:
        print(f"â†’ ç”»åƒ1ãŒ {total1 - total2:.1f}ç‚¹ å„ªä½")
    else:
        print(f"â†’ åŒç­‰ã®å“è³ª")

    print("\nã€ã‚¹ã‚³ã‚¢å†…è¨³ï¼ˆ7é …ç›®ã§è©•ä¾¡ï¼‰ã€‘")
    print(f"             ç”»åƒ1   ç”»åƒ2")
    print(f"ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹:   {sharp1_score:5.1f}   {sharp2_score:5.1f}")
    print(f"ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ:   {contrast1_score:5.1f}   {contrast2_score:5.1f}")
    print(f"ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼:   {entropy1_score:5.1f}   {entropy2_score:5.1f}")
    print(f"ãƒã‚¤ã‚ºå¯¾ç­–:     {noise1_score:5.1f}   {noise2_score:5.1f}")
    print(f"ã‚¨ãƒƒã‚¸ä¿æŒ:     {edge1_score:5.1f}   {edge2_score:5.1f}")
    print(f"æ­ªã¿æŠ‘åˆ¶:       {artifact1_score:5.1f}   {artifact2_score:5.1f}")
    print(f"ãƒ†ã‚¯ã‚¹ãƒãƒ£:     {texture1_score:5.1f}   {texture2_score:5.1f}")

    print(f"\nã€é¡ä¼¼åº¦æŒ‡æ¨™ï¼ˆå‚è€ƒå€¤ï¼‰ã€‘")
    print(f"  SSIM:        {ssim_score_val:.1f}/100")
    print(f"  MS-SSIM:     {ms_ssim_score_val:.1f}/100")
    print(f"  PSNR:        {psnr_score_val:.1f}/100")
    print(f"  LPIPS:       {lpips_score_val:.1f}/100")
    print(f"  è‰²å·®:        {color_diff_score:.1f}/100")
    print(f"  å±€æ‰€å“è³ª:    {local_quality_score:.1f}/100")
    print(f"  ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ : {histogram_score:.1f}/100")

    results['total_score'] = {
        'img1': round(total1, 1),
        'img2': round(total2, 1),
        'breakdown': {
            'img1': {
                'sharpness': round(sharp1_score, 1),
                'contrast': round(contrast1_score, 1),
                'entropy': round(entropy1_score, 1),
                'noise': round(noise1_score, 1),
                'edge': round(edge1_score, 1),
                'artifact': round(artifact1_score, 1),
                'texture': round(texture1_score, 1)
            },
            'img2': {
                'sharpness': round(sharp2_score, 1),
                'contrast': round(contrast2_score, 1),
                'entropy': round(entropy2_score, 1),
                'noise': round(noise2_score, 1),
                'edge': round(edge2_score, 1),
                'artifact': round(artifact2_score, 1),
                'texture': round(texture2_score, 1)
            },
            'ssim': round(ssim_score_val, 1),
            'psnr': round(psnr_score_val, 1)
        }
    }

    # 16. çµæœå¯è¦–åŒ–
    print("\nã€16. çµæœå¯è¦–åŒ–ã‚’ç”Ÿæˆä¸­...ã€‘")
    print_usage_status("ç”»åƒç”Ÿæˆé–‹å§‹")

    # è©³ç´°å¯è¦–åŒ–
    create_detailed_visualizations(img1_rgb, img2_rgb, img1_gray, img2_gray, output_dir)

    # æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    img1_name = os.path.basename(img1_path)
    img2_name = os.path.basename(img2_path)
    report_path = create_comparison_report(results, img1_name, img2_name, output_dir)
    print(f"æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ: {report_path}")

    # å·®åˆ†ç”»åƒ
    diff = cv2.absdiff(img1, img2)
    diff_gray = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)
    heatmap = cv2.applyColorMap(diff_gray, cv2.COLORMAP_JET)

    cv2.imwrite(os.path.join(output_dir, 'difference.png'), diff)
    cv2.imwrite(os.path.join(output_dir, 'heatmap.png'), heatmap)

    # ã‚¨ãƒƒã‚¸ç”»åƒã‚’ç”Ÿæˆã—ã¦ä¿å­˜
    edges1_save = cv2.Canny(img1_gray, 100, 200)
    edges2_save = cv2.Canny(img2_gray, 100, 200)
    cv2.imwrite(os.path.join(output_dir, 'edges_img1.png'), edges1_save)
    cv2.imwrite(os.path.join(output_dir, 'edges_img2.png'), edges2_save)

    # æ¯”è¼ƒç”»åƒ
    comparison = np.hstack([img1, img2, diff])
    cv2.imwrite(os.path.join(output_dir, 'comparison.png'), comparison)

    # JSONå½¢å¼ã§çµæœã‚’ä¿å­˜
    # numpyå‹ã‚’Pythonæ¨™æº–å‹ã«å¤‰æ›ã™ã‚‹ãŸã‚ã®ã‚«ã‚¹ã‚¿ãƒ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€
    class NumpyEncoder(json.JSONEncoder):
        def default(self, obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, (np.bool_, bool)):
                return bool(obj)
            return super(NumpyEncoder, self).default(obj)

    json_path = os.path.join(output_dir, 'analysis_results.json')
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False, cls=NumpyEncoder)

    print(f"çµæœã‚’ '{output_dir}/' ã«ä¿å­˜ã—ã¾ã—ãŸ")
    print("  - comparison_report.png: â˜…æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆï¼ˆã‚°ãƒ©ãƒ•ã¨ã‚¹ã‚³ã‚¢è¡¨ç¤ºï¼‰â˜…")
    print("  - detailed_analysis.png: è©³ç´°åˆ†æå¯è¦–åŒ–ï¼ˆ12æšã®åˆ†æç”»åƒï¼‰")
    print("  - difference.png: å·®åˆ†ç”»åƒ")
    print("  - heatmap.png: å·®åˆ†ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—")
    print("  - comparison.png: 3æšä¸¦ã¹ã¦æ¯”è¼ƒ")
    print("  - edges_*.png: ã‚¨ãƒƒã‚¸æ¤œå‡ºçµæœ")
    print("  - analysis_results.json: åˆ†æçµæœãƒ‡ãƒ¼ã‚¿ï¼ˆJSONå½¢å¼ï¼‰")

    print("\n" + "=" * 80)
    print("åˆ†æå®Œäº†")
    print("=" * 80)

    # çµæœã®è§£é‡ˆã‚’è¿½åŠ 
    # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã‚’çµæœã«ä¿å­˜
    results['evaluation_mode'] = evaluation_mode

    try:
        from result_interpreter import interpret_results, format_interpretation_text
        interpretation = interpret_results(results)
        interpretation_text = format_interpretation_text(interpretation)
        print("\n" + interpretation_text)

        # è§£é‡ˆçµæœã‚‚ä¿å­˜
        results['interpretation'] = interpretation
    except Exception as e:
        print(f"è§£é‡ˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

    return results

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    import sys

    if len(sys.argv) >= 3:
        # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‹ã‚‰ç”»åƒãƒ‘ã‚¹ã‚’å–å¾—
        img1_path = sys.argv[1]
        img2_path = sys.argv[2]
        output_dir = sys.argv[3] if len(sys.argv) > 3 else 'analysis_results'
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        img1_path = 'chainner_oiran.png'
        img2_path = 'upscayl_oiran.png'
        output_dir = 'analysis_results'

    print(f"å…ƒç”»åƒ: {img1_path}")
    print(f"AIå‡¦ç†çµæœ: {img2_path}")
    print(f"å‡ºåŠ›å…ˆ: {output_dir}")
    print()

    analyze_images(img1_path, img2_path, output_dir)
