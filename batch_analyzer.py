"""
ãƒãƒƒãƒå‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼šå¤§é‡ã®ç”»åƒãƒšã‚¢ã‚’è‡ªå‹•åˆ†æã—ã¦CSVå‡ºåŠ›

ä½¿ã„æ–¹:
1. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ:
   python batch_analyzer.py --create-config

2. batch_config.json ã‚’ç·¨é›†ã—ã¦ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã‚’è¨­å®š

3. ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ:
   python batch_analyzer.py batch_config.json
"""

import os
import csv
import json
import gc
import time
from pathlib import Path
from tqdm import tqdm
import pandas as pd
from multiprocessing import Pool, cpu_count
from functools import partial
from advanced_image_analyzer import analyze_images

def process_single_pair(args):
    """
    å˜ä¸€ã®ç”»åƒãƒšã‚¢ã‚’å‡¦ç†ï¼ˆä¸¦åˆ—å‡¦ç†ç”¨ï¼‰

    Args:
        args: (orig_img_path, model_name, upscaled_dir, output_detail_dir, evaluation_mode)

    Returns:
        (success, result_or_error_message)
    """
    orig_img_path, model_name, upscaled_dir, output_detail_dir, evaluation_mode = args

    image_id = orig_img_path.stem

    try:
        # è¶…è§£åƒç”»åƒã®ãƒ‘ã‚¹ã‚’æ¢ã™ï¼ˆPNG/JPGä¸¡å¯¾å¿œã€ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹å¯¾å¿œï¼‰
        upscaled_path = None

        # ã¾ãšå®Œå…¨ä¸€è‡´ã‚’è©¦ã™
        for ext in ['.png', '.jpg', '.jpeg']:
            candidate = upscaled_dir / f"{image_id}{ext}"
            if candidate.exists():
                upscaled_path = candidate
                break

        # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ä»˜ããƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        if upscaled_path is None:
            for ext in ['.png', '.jpg', '.jpeg']:
                # image_id ã§å§‹ã¾ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
                pattern = f"{image_id}*{ext}"
                matches = list(upscaled_dir.glob(pattern))
                if matches:
                    upscaled_path = matches[0]  # æœ€åˆã«ãƒãƒƒãƒã—ãŸã‚‚ã®ã‚’ä½¿ç”¨
                    break

        if upscaled_path is None:
            error_msg = f"âš ï¸  è¶…è§£åƒç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_name}/{image_id}"
            return (False, error_msg)

        # åˆ†æå®Ÿè¡Œ
        output_subdir = output_detail_dir / model_name / image_id

        results = analyze_images(
            str(orig_img_path),
            str(upscaled_path),
            str(output_subdir),
            str(orig_img_path),
            evaluation_mode
        )

        # 17é …ç›®ã®ã‚¹ã‚³ã‚¢ã‚’æŠ½å‡º
        row = extract_metrics_for_csv(
            image_id,
            model_name,
            results,
            str(orig_img_path),
            str(upscaled_path)
        )

        # ãƒ¡ãƒ¢ãƒªè§£æ”¾
        del results
        gc.collect()

        return (True, row)

    except Exception as e:
        error_msg = f"âŒ ã‚¨ãƒ©ãƒ¼: {image_id} - {model_name}: {str(e)}"
        return (False, error_msg)

def batch_analyze(config_file, progress_callback=None):
    """
    ãƒãƒƒãƒå‡¦ç†ã®å®Ÿè¡Œ

    Args:
        config_file: è¨­å®šJSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        progress_callback: é€²æ—é€šçŸ¥ç”¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•° (current, total, message)
    """

    # è¨­å®šèª­ã¿è¾¼ã¿
    with open(config_file, 'r', encoding='utf-8') as f:
        config = json.load(f)

    original_dir = Path(config['original_dir'])
    upscaled_dirs = {k: Path(v) for k, v in config['upscaled_dirs'].items()}
    output_csv = config['output_csv']
    output_detail_dir = Path(config.get('output_detail_dir', 'results/detailed/'))
    limit = config.get('limit', 0)  # 0 = å…¨ã¦å‡¦ç†
    append_mode = config.get('append_mode', False)  # False = ä¸Šæ›¸ã, True = è¿½åŠ 
    evaluation_mode = config.get('evaluation_mode', 'image')  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ç”»åƒãƒ¢ãƒ¼ãƒ‰
    num_workers = config.get('num_workers', max(1, cpu_count() - 1))  # ä¸¦åˆ—å‡¦ç†æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: CPUæ•°-1ï¼‰
    checkpoint_interval = config.get('checkpoint_interval', 1000)  # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜é–“éš”
    checkpoint_file = Path(output_csv).parent / f"checkpoint_{Path(output_csv).stem}.csv"

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_detail_dir.mkdir(parents=True, exist_ok=True)
    Path(output_csv).parent.mkdir(parents=True, exist_ok=True)

    # å…ƒç”»åƒãƒªã‚¹ãƒˆå–å¾—ï¼ˆPNGæ¨å¥¨ã€JPGã¯è­¦å‘Šï¼‰
    png_images = sorted(list(original_dir.glob('*.png')))
    jpg_images = sorted(list(original_dir.glob('*.jpg')) + list(original_dir.glob('*.jpeg')))

    # JPGç”»åƒãŒã‚ã‚‹å ´åˆã¯è­¦å‘Š
    if len(jpg_images) > 0:
        print(f"\n{'='*60}")
        print(f"âš ï¸  è­¦å‘Š: JPGãƒ•ã‚¡ã‚¤ãƒ«ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ ({len(jpg_images)}æš)")
        print(f"{'='*60}")
        print(f"JPGã¯éå¯é€†åœ§ç¸®å½¢å¼ã®ãŸã‚ã€ã™ã§ã«ç”»è³ªãŒåŠ£åŒ–ã—ã¦ã„ã¾ã™ã€‚")
        print(f"æ­£ç¢ºãªè©•ä¾¡ã®ãŸã‚ã«ã¯ã€å…ƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰PNGå½¢å¼ã§å†å‡ºåŠ›ã™ã‚‹ã“ã¨ã‚’å¼·ãæ¨å¥¨ã—ã¾ã™ã€‚")
        print(f"")
        print(f"ã€JPGã®å•é¡Œç‚¹ã€‘")
        print(f"  - ãƒ–ãƒ­ãƒƒã‚¯ãƒã‚¤ã‚ºï¼ˆ8Ã—8ãƒ”ã‚¯ã‚»ãƒ«å˜ä½ã®åœ§ç¸®ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆï¼‰")
        print(f"  - é«˜å‘¨æ³¢æˆåˆ†ã®æå¤±ï¼ˆç´°ã‹ã„ãƒ‡ã‚£ãƒ†ãƒ¼ãƒ«ãŒæ¶ˆå¤±ï¼‰")
        print(f"  - è‰²æƒ…å ±ã®åŠ£åŒ–ï¼ˆè‰²æ»²ã¿ã€ãƒãƒ³ãƒ‡ã‚£ãƒ³ã‚°ï¼‰")
        print(f"  - AIè¶…è§£åƒã®åŠ£åŒ–ã¨åŒºåˆ¥ä¸å¯èƒ½")
        print(f"")
        print(f"ã€æ¨å¥¨å¯¾å¿œã€‘")
        print(f"  1. å…ƒã®ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢/ã‚«ãƒ¡ãƒ©ã‹ã‚‰PNGå½¢å¼ã§å†å‡ºåŠ›")
        print(f"  2. TIFFç­‰ã®å¯é€†å½¢å¼ã‹ã‚‰å¤‰æ›")
        print(f"  3. åŒ»ç™‚ç”¨é€”ãƒ»è«–æ–‡ç™ºè¡¨ã«ã¯å¿…ãšPNGå½¢å¼ã‚’ä½¿ç”¨")
        print(f"{'='*60}\n")

    original_images = sorted(png_images + jpg_images)

    # å‡¦ç†æšæ•°åˆ¶é™
    if limit > 0 and limit < len(original_images):
        original_images = original_images[:limit]
        print(f"âš ï¸  åˆ†å‰²å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: æœ€åˆã®{limit}æšã®ã¿å‡¦ç†ã—ã¾ã™")

    if len(original_images) == 0:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: å…ƒç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {original_dir}")
        return

    # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰è¡¨ç¤ºç”¨è¾æ›¸
    mode_names = {
        'image': 'ğŸ“¸ ç”»åƒãƒ¢ãƒ¼ãƒ‰ï¼ˆåŒ»ç™‚ç”»åƒãƒ»å†™çœŸãªã©ï¼‰',
        'document': 'ğŸ“„ æ–‡æ›¸ãƒ¢ãƒ¼ãƒ‰ï¼ˆã‚«ãƒ«ãƒ†ãƒ»å¥‘ç´„æ›¸ãªã©ï¼‰',
        'academic': 'ğŸ“š å­¦è¡“è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ï¼ˆè«–æ–‡ç”¨ãƒ»æ¨™æº–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯äº’æ›ï¼‰',
        'developer': 'ğŸ”§ é–‹ç™ºè€…ãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰'
    }

    print(f"\n{'='*60}")
    print(f"ğŸš€ ãƒãƒƒãƒå‡¦ç†é–‹å§‹")
    print(f"{'='*60}")
    print(f"ğŸ“ å…ƒç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {original_dir}")
    print(f"ğŸ–¼ï¸  å…ƒç”»åƒæ•°: {len(original_images)}æš")
    print(f"ğŸ¤– è¶…è§£åƒãƒ¢ãƒ‡ãƒ«æ•°: {len(upscaled_dirs)}ç¨®é¡")
    for model_name in upscaled_dirs.keys():
        print(f"   - {model_name}")
    print(f"ğŸ’¾ å‡ºåŠ›CSV: {output_csv}")
    print(f"âš™ï¸  è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰: {mode_names.get(evaluation_mode, evaluation_mode)}")
    print(f"âš¡ ä¸¦åˆ—å‡¦ç†æ•°: {num_workers}ãƒ—ãƒ­ã‚»ã‚¹")
    print(f"ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé–“éš”: {checkpoint_interval}ã‚µãƒ³ãƒ—ãƒ«ã”ã¨")
    print(f"{'='*60}\n")

    # è¶…è§£åƒãƒ¢ãƒ‡ãƒ«ãƒ•ã‚©ãƒ«ãƒ€ã®JPGæ¤œå‡º
    jpg_detected_models = []
    for model_name, upscaled_dir in upscaled_dirs.items():
        jpg_count = len(list(upscaled_dir.glob('*.jpg'))) + len(list(upscaled_dir.glob('*.jpeg')))
        if jpg_count > 0:
            jpg_detected_models.append((model_name, jpg_count))

    if len(jpg_detected_models) > 0:
        print(f"\n{'='*60}")
        print(f"âš ï¸  è­¦å‘Š: è¶…è§£åƒçµæœã«JPGãƒ•ã‚¡ã‚¤ãƒ«ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
        print(f"{'='*60}")
        for model_name, jpg_count in jpg_detected_models:
            print(f"  - {model_name}: {jpg_count}æšã®JPGãƒ•ã‚¡ã‚¤ãƒ«")
        print(f"")
        print(f"JPGã¯éå¯é€†åœ§ç¸®ã®ãŸã‚ã€AIå‡¦ç†ã®å“è³ªã‚’æ­£ç¢ºã«è©•ä¾¡ã§ãã¾ã›ã‚“ã€‚")
        print(f"å…ƒã®AIè¶…è§£åƒãƒ„ãƒ¼ãƒ«ã§PNGå½¢å¼ã§å‡ºåŠ›ã—ç›´ã™ã“ã¨ã‚’å¼·ãæ¨å¥¨ã—ã¾ã™ã€‚")
        print(f"{'='*60}\n")

    # çµæœã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
    all_results = []
    total_pairs = len(original_images) * len(upscaled_dirs)
    processed = 0
    errors = 0

    # å‡¦ç†ã‚¿ã‚¹ã‚¯ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆï¼ˆå…¨ã¦ã®ç”»åƒÃ—ãƒ¢ãƒ‡ãƒ«ã®çµ„ã¿åˆã‚ã›ï¼‰
    tasks = []
    for orig_img_path in original_images:
        for model_name, upscaled_dir in upscaled_dirs.items():
            tasks.append((orig_img_path, model_name, upscaled_dir, output_detail_dir, evaluation_mode))

    print(f"ğŸ“‹ å‡¦ç†ã‚¿ã‚¹ã‚¯æ•°: {len(tasks)}")
    print(f"â±ï¸  æ¨å®šå‡¦ç†æ™‚é–“: {len(tasks) * 15 / num_workers / 60:.1f}åˆ† (1ã‚µãƒ³ãƒ—ãƒ«15ç§’æƒ³å®š)")
    print(f"{'='*60}\n")

    # é–‹å§‹æ™‚åˆ»è¨˜éŒ²
    start_time = time.time()

    # ä¸¦åˆ—å‡¦ç†ã§å®Ÿè¡Œ
    print(f"âš¡ {num_workers}ãƒ—ãƒ­ã‚»ã‚¹ã§ä¸¦åˆ—å‡¦ç†é–‹å§‹...\n")

    with Pool(processes=num_workers) as pool:
        # imapã‚’ä½¿ã£ã¦é€æ¬¡çš„ã«çµæœã‚’å–å¾—ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼‰
        results_iter = pool.imap(process_single_pair, tasks)

        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ä»˜ãã§å‡¦ç†
        for idx, (success, result) in enumerate(tqdm(results_iter, total=len(tasks), desc="ãƒãƒƒãƒå‡¦ç†ä¸­"), 1):
            if success:
                all_results.append(result)
                processed += 1

                # é€²æ—é€šçŸ¥
                if progress_callback:
                    progress_callback(processed, total_pairs, f"å®Œäº†: {result['image_id']} - {result['model']}")
            else:
                # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
                print(f"\n{result}")
                errors += 1

                # é€²æ—é€šçŸ¥
                if progress_callback:
                    progress_callback(processed + errors, total_pairs, result)

            # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
            if idx % checkpoint_interval == 0 and len(all_results) > 0:
                elapsed_time = time.time() - start_time
                avg_time_per_sample = elapsed_time / idx
                eta_seconds = avg_time_per_sample * (len(tasks) - idx)

                print(f"\n{'='*60}")
                print(f"ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ä¸­... ({idx}/{len(tasks)})")
                print(f"â±ï¸  çµŒéæ™‚é–“: {elapsed_time/60:.1f}åˆ†")
                print(f"â±ï¸  æ®‹ã‚Šæ™‚é–“: {eta_seconds/60:.1f}åˆ†")
                print(f"âœ”ï¸  æˆåŠŸ: {processed}, âŒ ã‚¨ãƒ©ãƒ¼: {errors}")
                print(f"{'='*60}\n")

                save_results_to_csv(all_results, str(checkpoint_file), append_mode=False)
                print(f"ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜å®Œäº†: {checkpoint_file}\n")

    # å‡¦ç†æ™‚é–“è¨ˆç®—
    total_time = time.time() - start_time
    avg_time_per_sample = total_time / len(tasks) if len(tasks) > 0 else 0

    # çµæœã‚’CSVä¿å­˜
    if len(all_results) > 0:
        save_results_to_csv(all_results, output_csv, append_mode)

        # ãƒ¢ãƒ‡ãƒ«åˆ¥ã®å‡¦ç†ä»¶æ•°ã‚’é›†è¨ˆ
        model_counts = {}
        for row in all_results:
            model = row['model']
            model_counts[model] = model_counts.get(model, 0) + 1

        print(f"\n{'='*60}")
        print(f"âœ… ãƒãƒƒãƒå‡¦ç†å®Œäº†ï¼")
        print(f"{'='*60}")
        print(f"âœ”ï¸  æˆåŠŸ: {processed} / {total_pairs}")
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {errors} / {total_pairs}")
        print(f"â±ï¸  ç·å‡¦ç†æ™‚é–“: {total_time/60:.1f}åˆ† ({total_time/3600:.2f}æ™‚é–“)")
        print(f"âš¡ å¹³å‡å‡¦ç†é€Ÿåº¦: {avg_time_per_sample:.2f}ç§’/ã‚µãƒ³ãƒ—ãƒ«")
        print(f"ğŸš€ ä¸¦åˆ—åŒ–åŠ¹ç‡: {num_workers}ãƒ—ãƒ­ã‚»ã‚¹ä½¿ç”¨")
        print(f"\nğŸ“Š ãƒ¢ãƒ‡ãƒ«åˆ¥å‡¦ç†ä»¶æ•°:")
        for model, count in model_counts.items():
            print(f"   {model}: {count}ä»¶")
        print(f"\nğŸ“„ çµæœCSV: {output_csv}")
        print(f"ğŸ“Š è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {output_detail_dir}")
        if checkpoint_file.exists():
            print(f"ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ: {checkpoint_file}")
        print(f"{'='*60}\n")

        # ç°¡æ˜“çµ±è¨ˆã‚’è¡¨ç¤º
        display_summary_statistics(all_results)

        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ï¼ˆæ­£å¸¸çµ‚äº†æ™‚ï¼‰
        if checkpoint_file.exists():
            checkpoint_file.unlink()
            print(f"\nğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤æ¸ˆã¿ï¼ˆæ­£å¸¸çµ‚äº†ï¼‰")
    else:
        print(f"\nâŒ å‡¦ç†å¯èƒ½ãªç”»åƒãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")


def extract_metrics_for_csv(image_id, model_name, results, original_path, upscaled_path):
    """
    åˆ†æçµæœã‹ã‚‰17é …ç›®+ãƒ¡ã‚¿æƒ…å ±ã‚’æŠ½å‡ºã—ã¦CSVç”¨ã®è¡Œãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
    """

    # SSIM/PSNR/è‰²å·®ã¯å…ƒç”»åƒã‚ã‚Šã®å ´åˆdictå½¢å¼
    def safe_extract(data, key1, key2=None):
        """dictå‹ã¨floatå‹ã®ä¸¡æ–¹ã«å¯¾å¿œ"""
        value = data.get(key1, 0)
        if isinstance(value, dict):
            if key2:
                return value.get(key2, 0)
            # dictã®å ´åˆã€img2ï¼ˆè¶…è§£åƒï¼‰ã®ã‚¹ã‚³ã‚¢ã‚’è¿”ã™
            return value.get('img2_vs_original', 0)
        return value if isinstance(value, (int, float)) else 0

    # è‰²å·®ã®å–å¾—
    delta_e_data = results.get('color_distribution', {}).get('delta_e', 0)
    if isinstance(delta_e_data, dict):
        delta_e_score = delta_e_data.get('img2_vs_original', 0)
    else:
        delta_e_score = delta_e_data if isinstance(delta_e_data, (int, float)) else 0

    row = {
        # ãƒ¡ã‚¿æƒ…å ±
        'image_id': image_id,
        'model': model_name,
        'original_path': original_path,
        'upscaled_path': upscaled_path,

        # 1. SSIMï¼ˆæ§‹é€ é¡ä¼¼æ€§ï¼‰- è¶…è§£åƒç”»åƒã®ã‚¹ã‚³ã‚¢
        'ssim': safe_extract(results, 'ssim', 'img2_vs_original'),

        # 2. MS-SSIM
        'ms_ssim': results.get('ms_ssim', 0),

        # 3. PSNRï¼ˆä¿¡å·å¯¾é›‘éŸ³æ¯”ï¼‰- è¶…è§£åƒç”»åƒã®ã‚¹ã‚³ã‚¢
        'psnr': safe_extract(results, 'psnr', 'img2_vs_original'),

        # 4. LPIPSï¼ˆçŸ¥è¦šçš„é¡ä¼¼åº¦ï¼‰
        'lpips': results.get('lpips', 0),

        # 5. ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹ - è¶…è§£åƒç”»åƒã®ã‚¹ã‚³ã‚¢
        'sharpness': results.get('sharpness', {}).get('img2', 0),
        'sharpness_diff_pct': results.get('sharpness', {}).get('difference_pct', 0),

        # 6. ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ - è¶…è§£åƒç”»åƒã®ã‚¹ã‚³ã‚¢
        'contrast': results.get('contrast', {}).get('img2', 0),
        'contrast_diff_pct': results.get('contrast', {}).get('difference_pct', 0),

        # 7. ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ - è¶…è§£åƒç”»åƒã®ã‚¹ã‚³ã‚¢
        'entropy': results.get('entropy', {}).get('img2', 0),

        # 8. ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ« - è¶…è§£åƒç”»åƒã®ã‚¹ã‚³ã‚¢ï¼ˆä½ã„ã»ã©è‰¯ã„ï¼‰
        'noise': results.get('noise', {}).get('img2', 0),

        # 9. ã‚¨ãƒƒã‚¸ä¿æŒç‡ - è¶…è§£åƒç”»åƒã®ã‚¹ã‚³ã‚¢
        'edge_density': results.get('edges', {}).get('img2_density', 0),
        'edge_diff_pct': results.get('edges', {}).get('difference_pct', 0),

        # 10. ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ - è¶…è§£åƒç”»åƒã®ã‚¹ã‚³ã‚¢ï¼ˆä½ã„ã»ã©è‰¯ã„ï¼‰
        'artifact_block': results.get('artifacts', {}).get('img2_block_noise', 0),
        'artifact_ringing': results.get('artifacts', {}).get('img2_ringing', 0),
        'artifact_total': (results.get('artifacts', {}).get('img2_block_noise', 0) +
                          results.get('artifacts', {}).get('img2_ringing', 0)),

        # 11. è‰²å·®ï¼ˆÎ”Eï¼‰- è¶…è§£åƒç”»åƒã®ã‚¹ã‚³ã‚¢ï¼ˆä½ã„ã»ã©è‰¯ã„ï¼‰
        'delta_e': delta_e_score,

        # 12. é«˜å‘¨æ³¢æˆåˆ† - è¶…è§£åƒç”»åƒã®ã‚¹ã‚³ã‚¢
        'high_freq_ratio': results.get('frequency_analysis', {}).get('img2', {}).get('high_freq_ratio', 0),

        # 13. ãƒ†ã‚¯ã‚¹ãƒãƒ£ - è¶…è§£åƒç”»åƒã®ã‚¹ã‚³ã‚¢
        'texture_complexity': results.get('texture', {}).get('img2', {}).get('texture_complexity', 0),

        # 14. å±€æ‰€å“è³ª
        'local_quality_mean': results.get('local_quality', {}).get('mean_ssim', 0),
        'local_quality_std': results.get('local_quality', {}).get('std_ssim', 0),
        'local_quality_min': results.get('local_quality', {}).get('min_ssim', 0),

        # 15. ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ç›¸é–¢
        'histogram_corr': results.get('histogram_correlation', 0),

        # 16. LABæ˜åº¦ - è¶…è§£åƒç”»åƒã®ã‚¹ã‚³ã‚¢
        'lab_L_mean': results.get('color_distribution', {}).get('img2', {}).get('LAB', {}).get('L_mean', 0),

        # 17. ç·åˆã‚¹ã‚³ã‚¢ - è¶…è§£åƒç”»åƒã®ã‚¹ã‚³ã‚¢
        'total_score': results.get('total_score', {}).get('img2', 0),
    }

    return row


def save_results_to_csv(all_results, output_csv, append_mode=False):
    """
    çµæœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜

    Args:
        all_results: åˆ†æçµæœã®ãƒªã‚¹ãƒˆ
        output_csv: å‡ºåŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        append_mode: True = è¿½åŠ ãƒ¢ãƒ¼ãƒ‰, False = ä¸Šæ›¸ããƒ¢ãƒ¼ãƒ‰
    """

    # DataFrameã«å¤‰æ›
    df_new = pd.DataFrame(all_results)

    if append_mode and Path(output_csv).exists():
        # è¿½åŠ ãƒ¢ãƒ¼ãƒ‰: æ—¢å­˜CSVã‚’èª­ã¿è¾¼ã‚“ã§çµåˆ
        print(f"\nğŸ“Š è¿½åŠ ãƒ¢ãƒ¼ãƒ‰ã§ä¿å­˜ä¸­...")
        df_existing = pd.read_csv(output_csv, encoding='utf-8-sig')

        # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆåŒã˜image_id + modelã®å ´åˆã¯æ–°ãƒ‡ãƒ¼ã‚¿ã§ä¸Šæ›¸ãï¼‰
        df_combined = pd.concat([df_existing, df_new], ignore_index=True)
        df_combined = df_combined.drop_duplicates(subset=['image_id', 'model'], keep='last')

        df_combined.to_csv(output_csv, index=False, encoding='utf-8-sig')

        print(f"   æ—¢å­˜ãƒ‡ãƒ¼ã‚¿: {len(df_existing)}è¡Œ")
        print(f"   æ–°è¦ãƒ‡ãƒ¼ã‚¿: {len(df_new)}è¡Œ")
        print(f"   çµåˆå¾Œ: {len(df_combined)}è¡Œ")
        print(f"   ãƒ•ã‚¡ã‚¤ãƒ«: {output_csv}")
        print(f"   ç”»åƒæ•°: {df_combined['image_id'].nunique()}")
        print(f"   ãƒ¢ãƒ‡ãƒ«æ•°: {df_combined['model'].nunique()}")
    else:
        # ä¸Šæ›¸ããƒ¢ãƒ¼ãƒ‰
        if append_mode:
            print(f"\nğŸ“Š è¿½åŠ ãƒ¢ãƒ¼ãƒ‰ã§ã™ãŒæ—¢å­˜CSVãŒãªã„ãŸã‚æ–°è¦ä½œæˆã—ã¾ã™")
        else:
            print(f"\nğŸ“Š ä¸Šæ›¸ããƒ¢ãƒ¼ãƒ‰ã§ä¿å­˜ä¸­...")

        df_new.to_csv(output_csv, index=False, encoding='utf-8-sig')

        print(f"   ãƒ•ã‚¡ã‚¤ãƒ«: {output_csv}")
        print(f"   ç”»åƒæ•°: {df_new['image_id'].nunique()}")
        print(f"   ãƒ¢ãƒ‡ãƒ«æ•°: {df_new['model'].nunique()}")
        print(f"   ç·è¡Œæ•°: {len(df_new)}")


def display_summary_statistics(all_results):
    """
    ç°¡æ˜“çµ±è¨ˆã‚’è¡¨ç¤º
    """

    df = pd.DataFrame(all_results)

    print(f"\nğŸ“ˆ ãƒ¢ãƒ‡ãƒ«åˆ¥å¹³å‡ã‚¹ã‚³ã‚¢:")
    print(f"{'='*80}")

    # ä¸»è¦æŒ‡æ¨™ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    grouped = df.groupby('model').agg({
        'ssim': 'mean',
        'psnr': 'mean',
        'lpips': 'mean',
        'total_score': 'mean',
        'noise': 'mean',
        'artifact_total': 'mean'
    }).round(4)

    # åˆ—åã‚’æ•´å½¢
    grouped.columns = ['SSIM', 'PSNR', 'LPIPS', 'ç·åˆã‚¹ã‚³ã‚¢', 'ãƒã‚¤ã‚º', 'ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ']

    # ã‚½ãƒ¼ãƒˆï¼ˆç·åˆã‚¹ã‚³ã‚¢é™é †ï¼‰
    grouped = grouped.sort_values('ç·åˆã‚¹ã‚³ã‚¢', ascending=False)

    print(grouped.to_string())
    print(f"{'='*80}\n")

    # ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    print(f"ğŸ† ç·åˆã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
    for i, (model, score) in enumerate(grouped['ç·åˆã‚¹ã‚³ã‚¢'].items(), 1):
        print(f"   {i}ä½: {model:20s} - {score:.2f}ç‚¹")


def create_config_template():
    """
    è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ
    """

    # CPUæ•°ã‚’å–å¾—
    num_cpus = cpu_count()
    recommended_workers = max(1, num_cpus - 1)

    template = {
        "original_dir": "dataset/original/",
        "upscaled_dirs": {
            "upscayl_model1": "dataset/upscayl_model1/",
            "upscayl_model2": "dataset/upscayl_model2/",
            "upscayl_model3": "dataset/upscayl_model3/",
            "chainer_combo1": "dataset/chainer_combo1/",
            "chainer_combo2": "dataset/chainer_combo2/"
        },
        "output_csv": "results/batch_analysis.csv",
        "output_detail_dir": "results/detailed/",
        "num_workers": recommended_workers,
        "checkpoint_interval": 1000,
        "evaluation_mode": "academic"
    }

    config_path = 'batch_config.json'

    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(template, f, indent=2, ensure_ascii=False)

    print(f"\nâœ… è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½œæˆå®Œäº†: {config_path}")
    print(f"\nğŸ“ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print(f"   1. {config_path} ã‚’ç·¨é›†ã—ã¦ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã‚’è¨­å®š")
    print(f"   2. python batch_analyzer.py {config_path} ã‚’å®Ÿè¡Œ")
    print(f"\nğŸ’¡ ãƒ’ãƒ³ãƒˆ:")
    print(f"   - original_dir: å…ƒç”»åƒï¼ˆ1000pxï¼‰ã®ãƒ•ã‚©ãƒ«ãƒ€")
    print(f"   - upscaled_dirs: å„ãƒ¢ãƒ‡ãƒ«ã®è¶…è§£åƒçµæœãƒ•ã‚©ãƒ«ãƒ€")
    print(f"   - num_workers: ä¸¦åˆ—å‡¦ç†æ•°ï¼ˆç¾åœ¨ã®CPU: {num_cpus}ã‚³ã‚¢ã€æ¨å¥¨: {recommended_workers}ï¼‰")
    print(f"   - checkpoint_interval: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜é–“éš”ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1000ã‚µãƒ³ãƒ—ãƒ«ï¼‰")
    print(f"   - evaluation_mode: è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ï¼ˆimage/document/academic/developerï¼‰")
    print(f"   - åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆimage001.pngç­‰ï¼‰ã§å¯¾å¿œä»˜ã‘ã•ã‚Œã¾ã™")
    print(f"\nâš¡ 15000ã‚µãƒ³ãƒ—ãƒ«å‡¦ç†ã®å ´åˆ:")
    print(f"   - æ¨å®šæ™‚é–“: ç´„{15000 * 15 / recommended_workers / 3600:.1f}æ™‚é–“ (1ã‚µãƒ³ãƒ—ãƒ«15ç§’æƒ³å®š)")
    print(f"   - ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã§ä¸­æ–­ãƒ»å†é–‹å¯èƒ½\n")


if __name__ == '__main__':
    import sys

    if len(sys.argv) < 2:
        print(f"\n{'='*60}")
        print(f"ãƒãƒƒãƒå‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ - ä½¿ã„æ–¹")
        print(f"{'='*60}")
        print(f"\nğŸ“‹ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ:")
        print(f"   python batch_analyzer.py --create-config")
        print(f"\nğŸš€ ãƒãƒƒãƒå‡¦ç†ã‚’å®Ÿè¡Œ:")
        print(f"   python batch_analyzer.py batch_config.json")
        print(f"\n{'='*60}\n")
        sys.exit(1)

    if sys.argv[1] == '--create-config':
        create_config_template()
    else:
        config_file = sys.argv[1]
        if not os.path.exists(config_file):
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_file}")
            sys.exit(1)
        batch_analyze(config_file)
