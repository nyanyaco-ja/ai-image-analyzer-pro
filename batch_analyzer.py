"""
ãƒãƒƒãƒå‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼šå¤§é‡ã®ç”»åƒãƒšã‚¢ã‚’è‡ªå‹•åˆ†æã—ã¦CSVå‡ºåŠ›

ä½¿ã„æ–¹:
1. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ:
   python batch_analyzer.py --create-config

2. batch_config.json ã‚’ç·¨é›†ã—ã¦ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã‚’è¨­å®š

3. ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ:
   python batch_analyzer.py batch_config.json
"""

import os
import csv
import json
from pathlib import Path
from tqdm import tqdm
import pandas as pd
from advanced_image_analyzer import analyze_images

def batch_analyze(config_file, progress_callback=None):
    """
    ãƒãƒƒãƒå‡¦ç†ã®å®Ÿè¡Œ

    Args:
        config_file: è¨­å®šJSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        progress_callback: é€²æ—é€šçŸ¥ç”¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•° (current, total, message)
    """

    # è¨­å®šèª­ã¿è¾¼ã¿
    with open(config_file, 'r', encoding='utf-8') as f:
        config = json.load(f)

    original_dir = Path(config['original_dir'])
    upscaled_dirs = {k: Path(v) for k, v in config['upscaled_dirs'].items()}
    output_csv = config['output_csv']
    output_detail_dir = Path(config.get('output_detail_dir', 'results/detailed/'))
    limit = config.get('limit', 0)  # 0 = å…¨ã¦å‡¦ç†
    append_mode = config.get('append_mode', False)  # False = ä¸Šæ›¸ã, True = è¿½åŠ 
    evaluation_mode = config.get('evaluation_mode', 'image')  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ç”»åƒãƒ¢ãƒ¼ãƒ‰

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_detail_dir.mkdir(parents=True, exist_ok=True)
    Path(output_csv).parent.mkdir(parents=True, exist_ok=True)

    # å…ƒç”»åƒãƒªã‚¹ãƒˆå–å¾—ï¼ˆPNGæ¨å¥¨ã€JPGã¯è­¦å‘Šï¼‰
    png_images = sorted(list(original_dir.glob('*.png')))
    jpg_images = sorted(list(original_dir.glob('*.jpg')) + list(original_dir.glob('*.jpeg')))

    # JPGç”»åƒãŒã‚ã‚‹å ´åˆã¯è­¦å‘Š
    if len(jpg_images) > 0:
        print(f"\n{'='*60}")
        print(f"âš ï¸  è­¦å‘Š: JPGãƒ•ã‚¡ã‚¤ãƒ«ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ ({len(jpg_images)}æš)")
        print(f"{'='*60}")
        print(f"JPGã¯éå¯é€†åœ§ç¸®å½¢å¼ã®ãŸã‚ã€ã™ã§ã«ç”»è³ªãŒåŠ£åŒ–ã—ã¦ã„ã¾ã™ã€‚")
        print(f"æ­£ç¢ºãªè©•ä¾¡ã®ãŸã‚ã«ã¯ã€å…ƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰PNGå½¢å¼ã§å†å‡ºåŠ›ã™ã‚‹ã“ã¨ã‚’å¼·ãæ¨å¥¨ã—ã¾ã™ã€‚")
        print(f"")
        print(f"ã€JPGã®å•é¡Œç‚¹ã€‘")
        print(f"  - ãƒ–ãƒ­ãƒƒã‚¯ãƒã‚¤ã‚ºï¼ˆ8Ã—8ãƒ”ã‚¯ã‚»ãƒ«å˜ä½ã®åœ§ç¸®ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆï¼‰")
        print(f"  - é«˜å‘¨æ³¢æˆåˆ†ã®æå¤±ï¼ˆç´°ã‹ã„ãƒ‡ã‚£ãƒ†ãƒ¼ãƒ«ãŒæ¶ˆå¤±ï¼‰")
        print(f"  - è‰²æƒ…å ±ã®åŠ£åŒ–ï¼ˆè‰²æ»²ã¿ã€ãƒãƒ³ãƒ‡ã‚£ãƒ³ã‚°ï¼‰")
        print(f"  - AIè¶…è§£åƒã®åŠ£åŒ–ã¨åŒºåˆ¥ä¸å¯èƒ½")
        print(f"")
        print(f"ã€æ¨å¥¨å¯¾å¿œã€‘")
        print(f"  1. å…ƒã®ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢/ã‚«ãƒ¡ãƒ©ã‹ã‚‰PNGå½¢å¼ã§å†å‡ºåŠ›")
        print(f"  2. TIFFç­‰ã®å¯é€†å½¢å¼ã‹ã‚‰å¤‰æ›")
        print(f"  3. åŒ»ç™‚ç”¨é€”ãƒ»è«–æ–‡ç™ºè¡¨ã«ã¯å¿…ãšPNGå½¢å¼ã‚’ä½¿ç”¨")
        print(f"{'='*60}\n")

    original_images = sorted(png_images + jpg_images)

    # å‡¦ç†æšæ•°åˆ¶é™
    if limit > 0 and limit < len(original_images):
        original_images = original_images[:limit]
        print(f"âš ï¸  åˆ†å‰²å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: æœ€åˆã®{limit}æšã®ã¿å‡¦ç†ã—ã¾ã™")

    if len(original_images) == 0:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: å…ƒç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {original_dir}")
        return

    # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰è¡¨ç¤ºç”¨è¾æ›¸
    mode_names = {
        'image': 'ğŸ“¸ ç”»åƒãƒ¢ãƒ¼ãƒ‰ï¼ˆåŒ»ç™‚ç”»åƒãƒ»å†™çœŸãªã©ï¼‰',
        'document': 'ğŸ“„ æ–‡æ›¸ãƒ¢ãƒ¼ãƒ‰ï¼ˆã‚«ãƒ«ãƒ†ãƒ»å¥‘ç´„æ›¸ãªã©ï¼‰',
        'academic': 'ğŸ“š å­¦è¡“è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ï¼ˆè«–æ–‡ç”¨ãƒ»æ¨™æº–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯äº’æ›ï¼‰',
        'developer': 'ğŸ”§ é–‹ç™ºè€…ãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰'
    }

    print(f"\n{'='*60}")
    print(f"ğŸš€ ãƒãƒƒãƒå‡¦ç†é–‹å§‹")
    print(f"{'='*60}")
    print(f"ğŸ“ å…ƒç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {original_dir}")
    print(f"ğŸ–¼ï¸  å…ƒç”»åƒæ•°: {len(original_images)}æš")
    print(f"ğŸ¤– è¶…è§£åƒãƒ¢ãƒ‡ãƒ«æ•°: {len(upscaled_dirs)}ç¨®é¡")
    for model_name in upscaled_dirs.keys():
        print(f"   - {model_name}")
    print(f"ğŸ’¾ å‡ºåŠ›CSV: {output_csv}")
    print(f"âš™ï¸  è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰: {mode_names.get(evaluation_mode, evaluation_mode)}")
    print(f"{'='*60}\n")

    # è¶…è§£åƒãƒ¢ãƒ‡ãƒ«ãƒ•ã‚©ãƒ«ãƒ€ã®JPGæ¤œå‡º
    jpg_detected_models = []
    for model_name, upscaled_dir in upscaled_dirs.items():
        jpg_count = len(list(upscaled_dir.glob('*.jpg'))) + len(list(upscaled_dir.glob('*.jpeg')))
        if jpg_count > 0:
            jpg_detected_models.append((model_name, jpg_count))

    if len(jpg_detected_models) > 0:
        print(f"\n{'='*60}")
        print(f"âš ï¸  è­¦å‘Š: è¶…è§£åƒçµæœã«JPGãƒ•ã‚¡ã‚¤ãƒ«ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
        print(f"{'='*60}")
        for model_name, jpg_count in jpg_detected_models:
            print(f"  - {model_name}: {jpg_count}æšã®JPGãƒ•ã‚¡ã‚¤ãƒ«")
        print(f"")
        print(f"JPGã¯éå¯é€†åœ§ç¸®ã®ãŸã‚ã€AIå‡¦ç†ã®å“è³ªã‚’æ­£ç¢ºã«è©•ä¾¡ã§ãã¾ã›ã‚“ã€‚")
        print(f"å…ƒã®AIè¶…è§£åƒãƒ„ãƒ¼ãƒ«ã§PNGå½¢å¼ã§å‡ºåŠ›ã—ç›´ã™ã“ã¨ã‚’å¼·ãæ¨å¥¨ã—ã¾ã™ã€‚")
        print(f"{'='*60}\n")

    # çµæœã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
    all_results = []
    total_pairs = len(original_images) * len(upscaled_dirs)
    processed = 0
    errors = 0

    # å„å…ƒç”»åƒã«å¯¾ã—ã¦å‡¦ç†
    for idx, orig_img_path in enumerate(tqdm(original_images, desc="å…ƒç”»åƒå‡¦ç†ä¸­")):
        image_id = orig_img_path.stem

        # å„è¶…è§£åƒãƒ¢ãƒ‡ãƒ«ã®çµæœã¨æ¯”è¼ƒ
        for model_name, upscaled_dir in upscaled_dirs.items():
            # è¶…è§£åƒç”»åƒã®ãƒ‘ã‚¹ã‚’æ¢ã™ï¼ˆPNG/JPGä¸¡å¯¾å¿œï¼‰
            upscaled_path = None
            tried_extensions = []
            for ext in ['.png', '.jpg', '.jpeg']:
                candidate = upscaled_dir / f"{image_id}{ext}"
                tried_extensions.append(str(candidate))
                if candidate.exists():
                    upscaled_path = candidate
                    break

            if upscaled_path is None:
                msg = f"âš ï¸  è¶…è§£åƒç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_name}/{image_id}"
                print(msg)
                print(f"   æ¢ç´¢ã—ãŸãƒ‘ã‚¹:")
                for tried_path in tried_extensions:
                    print(f"     - {tried_path}")
                errors += 1
                # é€²æ—é€šçŸ¥ï¼ˆã‚¨ãƒ©ãƒ¼ã‚‚ã‚«ã‚¦ãƒ³ãƒˆï¼‰
                if progress_callback:
                    progress_callback(processed + errors, total_pairs, msg)
                continue

            # åˆ†æå®Ÿè¡Œ
            output_subdir = output_detail_dir / model_name / image_id

            try:
                # analyze_images(å…ƒç”»åƒ, è¶…è§£åƒç”»åƒ, å‡ºåŠ›å…ˆ, ä½è§£åƒåº¦å…ƒç”»åƒ, è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰)
                # ã“ã“ã§ã¯å…ƒç”»åƒ=ä½è§£åƒåº¦ã¨ã—ã¦ä½¿ç”¨
                results = analyze_images(
                    str(orig_img_path),      # ç”»åƒ1: å…ƒç”»åƒï¼ˆåŸºæº–ï¼‰
                    str(upscaled_path),      # ç”»åƒ2: AIè¶…è§£åƒ
                    str(output_subdir),      # å‡ºåŠ›å…ˆ
                    str(orig_img_path),      # å…ƒç”»åƒï¼ˆoriginal_pathï¼‰ã¨ã—ã¦åŒã˜ã‚‚ã®ã‚’ä½¿ç”¨
                    evaluation_mode          # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ï¼ˆGUIã‹ã‚‰æ¸¡ã•ã‚Œã‚‹ï¼‰
                )

                # 17é …ç›®ã®ã‚¹ã‚³ã‚¢ã‚’æŠ½å‡º
                row = extract_metrics_for_csv(
                    image_id,
                    model_name,
                    results,
                    str(orig_img_path),
                    str(upscaled_path)
                )

                all_results.append(row)
                processed += 1

                # é€²æ—é€šçŸ¥ï¼ˆå‡¦ç†å®Œäº†å¾Œï¼‰
                if progress_callback:
                    progress_callback(processed, total_pairs, f"å®Œäº†: {image_id} - {model_name}")

            except Exception as e:
                msg = f"âŒ ã‚¨ãƒ©ãƒ¼: {image_id} - {model_name}: {str(e)}"
                print(f"\n{msg}")
                errors += 1
                # é€²æ—é€šçŸ¥ï¼ˆã‚¨ãƒ©ãƒ¼ã‚‚ã‚«ã‚¦ãƒ³ãƒˆï¼‰
                if progress_callback:
                    progress_callback(processed + errors, total_pairs, msg)
                continue

    # çµæœã‚’CSVä¿å­˜
    if len(all_results) > 0:
        save_results_to_csv(all_results, output_csv, append_mode)

        # ãƒ¢ãƒ‡ãƒ«åˆ¥ã®å‡¦ç†ä»¶æ•°ã‚’é›†è¨ˆ
        model_counts = {}
        for row in all_results:
            model = row['model']
            model_counts[model] = model_counts.get(model, 0) + 1

        print(f"\n{'='*60}")
        print(f"âœ… ãƒãƒƒãƒå‡¦ç†å®Œäº†ï¼")
        print(f"{'='*60}")
        print(f"âœ”ï¸  æˆåŠŸ: {processed} / {total_pairs}")
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {errors} / {total_pairs}")
        print(f"\nğŸ“Š ãƒ¢ãƒ‡ãƒ«åˆ¥å‡¦ç†ä»¶æ•°:")
        for model, count in model_counts.items():
            print(f"   {model}: {count}ä»¶")
        print(f"\nğŸ“„ çµæœCSV: {output_csv}")
        print(f"ğŸ“Š è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {output_detail_dir}")
        print(f"{'='*60}\n")

        # ç°¡æ˜“çµ±è¨ˆã‚’è¡¨ç¤º
        display_summary_statistics(all_results)
    else:
        print(f"\nâŒ å‡¦ç†å¯èƒ½ãªç”»åƒãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")


def extract_metrics_for_csv(image_id, model_name, results, original_path, upscaled_path):
    """
    åˆ†æçµæœã‹ã‚‰17é …ç›®+ãƒ¡ã‚¿æƒ…å ±ã‚’æŠ½å‡ºã—ã¦CSVç”¨ã®è¡Œãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
    """

    # SSIM/PSNR/è‰²å·®ã¯å…ƒç”»åƒã‚ã‚Šã®å ´åˆdictå½¢å¼
    def safe_extract(data, key1, key2=None):
        """dictå‹ã¨floatå‹ã®ä¸¡æ–¹ã«å¯¾å¿œ"""
        value = data.get(key1, 0)
        if isinstance(value, dict):
            if key2:
                return value.get(key2, 0)
            # dictã®å ´åˆã€img2ï¼ˆè¶…è§£åƒï¼‰ã®ã‚¹ã‚³ã‚¢ã‚’è¿”ã™
            return value.get('img2_vs_original', 0)
        return value if isinstance(value, (int, float)) else 0

    # è‰²å·®ã®å–å¾—
    delta_e_data = results.get('color_distribution', {}).get('delta_e', 0)
    if isinstance(delta_e_data, dict):
        delta_e_score = delta_e_data.get('img2_vs_original', 0)
    else:
        delta_e_score = delta_e_data if isinstance(delta_e_data, (int, float)) else 0

    row = {
        # ãƒ¡ã‚¿æƒ…å ±
        'image_id': image_id,
        'model': model_name,
        'original_path': original_path,
        'upscaled_path': upscaled_path,

        # 1. SSIMï¼ˆæ§‹é€ é¡ä¼¼æ€§ï¼‰- è¶…è§£åƒç”»åƒã®ã‚¹ã‚³ã‚¢
        'ssim': safe_extract(results, 'ssim', 'img2_vs_original'),

        # 2. MS-SSIM
        'ms_ssim': results.get('ms_ssim', 0),

        # 3. PSNRï¼ˆä¿¡å·å¯¾é›‘éŸ³æ¯”ï¼‰- è¶…è§£åƒç”»åƒã®ã‚¹ã‚³ã‚¢
        'psnr': safe_extract(results, 'psnr', 'img2_vs_original'),

        # 4. LPIPSï¼ˆçŸ¥è¦šçš„é¡ä¼¼åº¦ï¼‰
        'lpips': results.get('lpips', 0),

        # 5. ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹ - è¶…è§£åƒç”»åƒã®ã‚¹ã‚³ã‚¢
        'sharpness': results.get('sharpness', {}).get('img2', 0),
        'sharpness_diff_pct': results.get('sharpness', {}).get('difference_pct', 0),

        # 6. ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ - è¶…è§£åƒç”»åƒã®ã‚¹ã‚³ã‚¢
        'contrast': results.get('contrast', {}).get('img2', 0),
        'contrast_diff_pct': results.get('contrast', {}).get('difference_pct', 0),

        # 7. ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ - è¶…è§£åƒç”»åƒã®ã‚¹ã‚³ã‚¢
        'entropy': results.get('entropy', {}).get('img2', 0),

        # 8. ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ« - è¶…è§£åƒç”»åƒã®ã‚¹ã‚³ã‚¢ï¼ˆä½ã„ã»ã©è‰¯ã„ï¼‰
        'noise': results.get('noise', {}).get('img2', 0),

        # 9. ã‚¨ãƒƒã‚¸ä¿æŒç‡ - è¶…è§£åƒç”»åƒã®ã‚¹ã‚³ã‚¢
        'edge_density': results.get('edges', {}).get('img2_density', 0),
        'edge_diff_pct': results.get('edges', {}).get('difference_pct', 0),

        # 10. ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ - è¶…è§£åƒç”»åƒã®ã‚¹ã‚³ã‚¢ï¼ˆä½ã„ã»ã©è‰¯ã„ï¼‰
        'artifact_block': results.get('artifacts', {}).get('img2_block_noise', 0),
        'artifact_ringing': results.get('artifacts', {}).get('img2_ringing', 0),
        'artifact_total': (results.get('artifacts', {}).get('img2_block_noise', 0) +
                          results.get('artifacts', {}).get('img2_ringing', 0)),

        # 11. è‰²å·®ï¼ˆÎ”Eï¼‰- è¶…è§£åƒç”»åƒã®ã‚¹ã‚³ã‚¢ï¼ˆä½ã„ã»ã©è‰¯ã„ï¼‰
        'delta_e': delta_e_score,

        # 12. é«˜å‘¨æ³¢æˆåˆ† - è¶…è§£åƒç”»åƒã®ã‚¹ã‚³ã‚¢
        'high_freq_ratio': results.get('frequency_analysis', {}).get('img2', {}).get('high_freq_ratio', 0),

        # 13. ãƒ†ã‚¯ã‚¹ãƒãƒ£ - è¶…è§£åƒç”»åƒã®ã‚¹ã‚³ã‚¢
        'texture_complexity': results.get('texture', {}).get('img2', {}).get('texture_complexity', 0),

        # 14. å±€æ‰€å“è³ª
        'local_quality_mean': results.get('local_quality', {}).get('mean_ssim', 0),
        'local_quality_std': results.get('local_quality', {}).get('std_ssim', 0),
        'local_quality_min': results.get('local_quality', {}).get('min_ssim', 0),

        # 15. ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ç›¸é–¢
        'histogram_corr': results.get('histogram_correlation', 0),

        # 16. LABæ˜åº¦ - è¶…è§£åƒç”»åƒã®ã‚¹ã‚³ã‚¢
        'lab_L_mean': results.get('color_distribution', {}).get('img2', {}).get('LAB', {}).get('L_mean', 0),

        # 17. ç·åˆã‚¹ã‚³ã‚¢ - è¶…è§£åƒç”»åƒã®ã‚¹ã‚³ã‚¢
        'total_score': results.get('total_score', {}).get('img2', 0),
    }

    return row


def save_results_to_csv(all_results, output_csv, append_mode=False):
    """
    çµæœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜

    Args:
        all_results: åˆ†æçµæœã®ãƒªã‚¹ãƒˆ
        output_csv: å‡ºåŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        append_mode: True = è¿½åŠ ãƒ¢ãƒ¼ãƒ‰, False = ä¸Šæ›¸ããƒ¢ãƒ¼ãƒ‰
    """

    # DataFrameã«å¤‰æ›
    df_new = pd.DataFrame(all_results)

    if append_mode and Path(output_csv).exists():
        # è¿½åŠ ãƒ¢ãƒ¼ãƒ‰: æ—¢å­˜CSVã‚’èª­ã¿è¾¼ã‚“ã§çµåˆ
        print(f"\nğŸ“Š è¿½åŠ ãƒ¢ãƒ¼ãƒ‰ã§ä¿å­˜ä¸­...")
        df_existing = pd.read_csv(output_csv, encoding='utf-8-sig')

        # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆåŒã˜image_id + modelã®å ´åˆã¯æ–°ãƒ‡ãƒ¼ã‚¿ã§ä¸Šæ›¸ãï¼‰
        df_combined = pd.concat([df_existing, df_new], ignore_index=True)
        df_combined = df_combined.drop_duplicates(subset=['image_id', 'model'], keep='last')

        df_combined.to_csv(output_csv, index=False, encoding='utf-8-sig')

        print(f"   æ—¢å­˜ãƒ‡ãƒ¼ã‚¿: {len(df_existing)}è¡Œ")
        print(f"   æ–°è¦ãƒ‡ãƒ¼ã‚¿: {len(df_new)}è¡Œ")
        print(f"   çµåˆå¾Œ: {len(df_combined)}è¡Œ")
        print(f"   ãƒ•ã‚¡ã‚¤ãƒ«: {output_csv}")
        print(f"   ç”»åƒæ•°: {df_combined['image_id'].nunique()}")
        print(f"   ãƒ¢ãƒ‡ãƒ«æ•°: {df_combined['model'].nunique()}")
    else:
        # ä¸Šæ›¸ããƒ¢ãƒ¼ãƒ‰
        if append_mode:
            print(f"\nğŸ“Š è¿½åŠ ãƒ¢ãƒ¼ãƒ‰ã§ã™ãŒæ—¢å­˜CSVãŒãªã„ãŸã‚æ–°è¦ä½œæˆã—ã¾ã™")
        else:
            print(f"\nğŸ“Š ä¸Šæ›¸ããƒ¢ãƒ¼ãƒ‰ã§ä¿å­˜ä¸­...")

        df_new.to_csv(output_csv, index=False, encoding='utf-8-sig')

        print(f"   ãƒ•ã‚¡ã‚¤ãƒ«: {output_csv}")
        print(f"   ç”»åƒæ•°: {df_new['image_id'].nunique()}")
        print(f"   ãƒ¢ãƒ‡ãƒ«æ•°: {df_new['model'].nunique()}")
        print(f"   ç·è¡Œæ•°: {len(df_new)}")


def display_summary_statistics(all_results):
    """
    ç°¡æ˜“çµ±è¨ˆã‚’è¡¨ç¤º
    """

    df = pd.DataFrame(all_results)

    print(f"\nğŸ“ˆ ãƒ¢ãƒ‡ãƒ«åˆ¥å¹³å‡ã‚¹ã‚³ã‚¢:")
    print(f"{'='*80}")

    # ä¸»è¦æŒ‡æ¨™ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    grouped = df.groupby('model').agg({
        'ssim': 'mean',
        'psnr': 'mean',
        'lpips': 'mean',
        'total_score': 'mean',
        'noise': 'mean',
        'artifact_total': 'mean'
    }).round(4)

    # åˆ—åã‚’æ•´å½¢
    grouped.columns = ['SSIM', 'PSNR', 'LPIPS', 'ç·åˆã‚¹ã‚³ã‚¢', 'ãƒã‚¤ã‚º', 'ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ']

    # ã‚½ãƒ¼ãƒˆï¼ˆç·åˆã‚¹ã‚³ã‚¢é™é †ï¼‰
    grouped = grouped.sort_values('ç·åˆã‚¹ã‚³ã‚¢', ascending=False)

    print(grouped.to_string())
    print(f"{'='*80}\n")

    # ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    print(f"ğŸ† ç·åˆã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
    for i, (model, score) in enumerate(grouped['ç·åˆã‚¹ã‚³ã‚¢'].items(), 1):
        print(f"   {i}ä½: {model:20s} - {score:.2f}ç‚¹")


def create_config_template():
    """
    è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ
    """

    template = {
        "original_dir": "dataset/original/",
        "upscaled_dirs": {
            "upscayl_model1": "dataset/upscayl_model1/",
            "upscayl_model2": "dataset/upscayl_model2/",
            "upscayl_model3": "dataset/upscayl_model3/",
            "chainer_combo1": "dataset/chainer_combo1/",
            "chainer_combo2": "dataset/chainer_combo2/"
        },
        "output_csv": "results/batch_analysis.csv",
        "output_detail_dir": "results/detailed/"
    }

    config_path = 'batch_config.json'

    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(template, f, indent=2, ensure_ascii=False)

    print(f"\nâœ… è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½œæˆå®Œäº†: {config_path}")
    print(f"\nğŸ“ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print(f"   1. {config_path} ã‚’ç·¨é›†ã—ã¦ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã‚’è¨­å®š")
    print(f"   2. python batch_analyzer.py {config_path} ã‚’å®Ÿè¡Œ")
    print(f"\nğŸ’¡ ãƒ’ãƒ³ãƒˆ:")
    print(f"   - original_dir: å…ƒç”»åƒï¼ˆ1000pxï¼‰ã®ãƒ•ã‚©ãƒ«ãƒ€")
    print(f"   - upscaled_dirs: å„ãƒ¢ãƒ‡ãƒ«ã®è¶…è§£åƒçµæœãƒ•ã‚©ãƒ«ãƒ€")
    print(f"   - åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆimage001.pngç­‰ï¼‰ã§å¯¾å¿œä»˜ã‘ã•ã‚Œã¾ã™\n")


if __name__ == '__main__':
    import sys

    if len(sys.argv) < 2:
        print(f"\n{'='*60}")
        print(f"ãƒãƒƒãƒå‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ - ä½¿ã„æ–¹")
        print(f"{'='*60}")
        print(f"\nğŸ“‹ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ:")
        print(f"   python batch_analyzer.py --create-config")
        print(f"\nğŸš€ ãƒãƒƒãƒå‡¦ç†ã‚’å®Ÿè¡Œ:")
        print(f"   python batch_analyzer.py batch_config.json")
        print(f"\n{'='*60}\n")
        sys.exit(1)

    if sys.argv[1] == '--create-config':
        create_config_template()
    else:
        config_file = sys.argv[1]
        if not os.path.exists(config_file):
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_file}")
            sys.exit(1)
        batch_analyze(config_file)
