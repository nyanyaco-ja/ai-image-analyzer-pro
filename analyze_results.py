"""
çµ±è¨ˆåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼šãƒãƒƒãƒå‡¦ç†çµæœã‹ã‚‰é–¾å€¤ã‚’æ±ºå®šï¼ˆ26ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œç‰ˆï¼‰

26ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡º:
- 9ã¤ã®çµ„ã¿åˆã‚ã›ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆè¤‡åˆç•°å¸¸ï¼‰
- 17ã®å˜ç‹¬é–¾å€¤åˆ¤å®šï¼ˆå„æŒ‡æ¨™ã®ç•°å¸¸å€¤ï¼‰

ä½¿ã„æ–¹:
python analyze_results.py results/batch_analysis.csv
"""

import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['MS Gothic', 'Yu Gothic', 'Meiryo', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False


def analyze_batch_results(csv_file):
    """
    ãƒãƒƒãƒå‡¦ç†çµæœã®çµ±è¨ˆåˆ†æ
    """

    # CSVèª­ã¿è¾¼ã¿
    df = pd.read_csv(csv_file)

    print(f"\n{'='*80}")
    print(f"ğŸ“Š çµ±è¨ˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
    print(f"{'='*80}")
    print(f"ğŸ“„ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«: {csv_file}")
    print(f"ğŸ“· ç”»åƒæ•°: {df['image_id'].nunique()}")
    print(f"ğŸ¤– ãƒ¢ãƒ‡ãƒ«æ•°: {df['model'].nunique()}")
    print(f"ğŸ“Š ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(df)}")
    print(f"{'='*80}\n")

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_dir = Path('analysis_output')
    output_dir.mkdir(exist_ok=True)

    # 1. åŸºæœ¬çµ±è¨ˆé‡
    print_basic_statistics(df)

    # 2. ãƒ¢ãƒ‡ãƒ«åˆ¥æ¯”è¼ƒ
    compare_models(df, output_dir)

    # 3. ç›¸é–¢åˆ†æ
    analyze_correlations(df, output_dir)

    # 4. é–¾å€¤ææ¡ˆ
    suggest_thresholds(df, output_dir)

    # 5. ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ææ¡ˆ
    suggest_hallucination_logic(df, output_dir)

    # 6. ç ”ç©¶ç”¨ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆ
    generate_research_plots(df, output_dir)

    print(f"\nâœ… åˆ†æå®Œäº†ï¼")
    print(f"ğŸ“ çµæœä¿å­˜å…ˆ: {output_dir}/")


def print_basic_statistics(df):
    """
    åŸºæœ¬çµ±è¨ˆé‡ã®è¡¨ç¤º
    """

    print(f"\nğŸ“ˆ ä¸»è¦æŒ‡æ¨™ã®åŸºæœ¬çµ±è¨ˆé‡:")
    print(f"{'='*80}")

    # 17é …ç›®ã™ã¹ã¦
    metrics = ['ssim', 'ms_ssim', 'psnr', 'lpips', 'sharpness', 'contrast',
               'entropy', 'noise', 'edge_density', 'artifact_total', 'delta_e',
               'high_freq_ratio', 'texture_complexity', 'local_quality_mean',
               'histogram_corr', 'lab_L_mean', 'total_score']

    stats = df[metrics].describe().T
    stats.columns = ['ä»¶æ•°', 'å¹³å‡', 'æ¨™æº–åå·®', 'æœ€å°', '25%', '50%', '75%', 'æœ€å¤§']

    print(stats.round(4).to_string())
    print(f"{'='*80}\n")


def compare_models(df, output_dir):
    """
    ãƒ¢ãƒ‡ãƒ«åˆ¥æ¯”è¼ƒ
    """

    print(f"\nğŸ† ãƒ¢ãƒ‡ãƒ«åˆ¥ãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
    print(f"{'='*80}")

    # ä¸»è¦æŒ‡æ¨™ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    model_comparison = df.groupby('model').agg({
        'ssim': ['mean', 'std'],
        'psnr': ['mean', 'std'],
        'lpips': ['mean', 'std'],
        'total_score': ['mean', 'std'],
        'noise': ['mean', 'std'],
        'artifact_total': ['mean', 'std'],
        'sharpness': ['mean', 'std'],
        'edge_density': ['mean', 'std']
    }).round(4)

    # ç·åˆã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
    model_comparison = model_comparison.sort_values(('total_score', 'mean'), ascending=False)

    print(model_comparison.to_string())
    print(f"{'='*80}\n")

    # CSVä¿å­˜
    model_comparison.to_csv(output_dir / 'model_comparison.csv', encoding='utf-8-sig')

    # å¯è¦–åŒ–ï¼šãƒ¢ãƒ‡ãƒ«åˆ¥ç·åˆã‚¹ã‚³ã‚¢
    plt.figure(figsize=(12, 6))
    model_scores = df.groupby('model')['total_score'].mean().sort_values(ascending=False)

    plt.bar(range(len(model_scores)), model_scores.values)
    plt.xticks(range(len(model_scores)), model_scores.index, rotation=45, ha='right')
    plt.ylabel('ç·åˆã‚¹ã‚³ã‚¢ï¼ˆå¹³å‡ï¼‰')
    plt.title('ãƒ¢ãƒ‡ãƒ«åˆ¥ç·åˆã‚¹ã‚³ã‚¢æ¯”è¼ƒ')
    plt.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'model_scores.png', dpi=150)
    plt.close()

    print(f"ğŸ“Š ã‚°ãƒ©ãƒ•ä¿å­˜: {output_dir}/model_scores.png")


def analyze_correlations(df, output_dir):
    """
    17é …ç›®é–“ã®ç›¸é–¢åˆ†æ
    """

    print(f"\nğŸ”— ç›¸é–¢åˆ†æ:")
    print(f"{'='*80}")

    # æ•°å€¤åˆ—ã®ã¿æŠ½å‡º
    numeric_cols = ['ssim', 'psnr', 'lpips', 'ms_ssim', 'sharpness', 'contrast',
                    'entropy', 'noise', 'edge_density', 'artifact_total', 'delta_e',
                    'high_freq_ratio', 'texture_complexity', 'local_quality_mean',
                    'histogram_corr', 'lab_L_mean', 'total_score']

    corr_matrix = df[numeric_cols].corr()

    # ç›¸é–¢è¡Œåˆ—ã‚’ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—è¡¨ç¤º
    plt.figure(figsize=(14, 12))
    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,
                square=True, linewidths=0.5, cbar_kws={"shrink": 0.8})
    plt.title('17é …ç›®ã®ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(output_dir / 'correlation_matrix.png', dpi=150)
    plt.close()

    print(f"ğŸ“Š ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ä¿å­˜: {output_dir}/correlation_matrix.png")

    # é«˜ç›¸é–¢ãƒšã‚¢ã‚’è¡¨ç¤º
    print(f"\nğŸ”¥ é«˜ç›¸é–¢ãƒšã‚¢ï¼ˆ|r| > 0.7ï¼‰:")
    high_corr = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            corr_value = corr_matrix.iloc[i, j]
            if abs(corr_value) > 0.7:
                high_corr.append({
                    'metric1': corr_matrix.columns[i],
                    'metric2': corr_matrix.columns[j],
                    'correlation': corr_value
                })

    if high_corr:
        high_corr_df = pd.DataFrame(high_corr).sort_values('correlation', ascending=False)
        print(high_corr_df.to_string(index=False))
    else:
        print("   (ãªã—)")

    print(f"{'='*80}\n")


def suggest_thresholds(df, output_dir):
    """
    æ ¹æ‹ ã®ã‚ã‚‹é–¾å€¤ã‚’ææ¡ˆ
    """

    print(f"\nğŸ’¡ æ¨å¥¨é–¾å€¤ã®ææ¡ˆ:")
    print(f"{'='*80}")

    thresholds = {}

    # å„æŒ‡æ¨™ã®çµ±è¨ˆå€¤ã‹ã‚‰é–¾å€¤ã‚’æ±ºå®š
    # 17é …ç›®ã™ã¹ã¦ã®é–¾å€¤ã‚’ææ¡ˆ
    metrics_config = {
        'ssim': {'direction': 'high', 'percentile': 25, 'name': 'SSIMï¼ˆæ§‹é€ é¡ä¼¼æ€§ï¼‰'},
        'ms_ssim': {'direction': 'high', 'percentile': 25, 'name': 'MS-SSIMï¼ˆãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«SSIMï¼‰'},
        'psnr': {'direction': 'high', 'percentile': 25, 'name': 'PSNRï¼ˆä¿¡å·å¯¾é›‘éŸ³æ¯”ï¼‰'},
        'lpips': {'direction': 'low', 'percentile': 75, 'name': 'LPIPSï¼ˆçŸ¥è¦šçš„é¡ä¼¼åº¦ï¼‰'},
        'sharpness': {'direction': 'high', 'percentile': 25, 'name': 'ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹'},
        'contrast': {'direction': 'high', 'percentile': 25, 'name': 'ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ'},
        'entropy': {'direction': 'high', 'percentile': 25, 'name': 'ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆæƒ…å ±é‡ï¼‰'},
        'noise': {'direction': 'low', 'percentile': 75, 'name': 'ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«'},
        'edge_density': {'direction': 'high', 'percentile': 25, 'name': 'ã‚¨ãƒƒã‚¸å¯†åº¦'},
        'artifact_total': {'direction': 'low', 'percentile': 75, 'name': 'ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ'},
        'delta_e': {'direction': 'low', 'percentile': 75, 'name': 'è‰²å·®ï¼ˆÎ”Eï¼‰'},
        'high_freq_ratio': {'direction': 'high', 'percentile': 25, 'name': 'é«˜å‘¨æ³¢æˆåˆ†æ¯”ç‡'},
        'texture_complexity': {'direction': 'high', 'percentile': 25, 'name': 'ãƒ†ã‚¯ã‚¹ãƒãƒ£è¤‡é›‘åº¦'},
        'local_quality_mean': {'direction': 'high', 'percentile': 25, 'name': 'å±€æ‰€å“è³ªå¹³å‡'},
        'histogram_corr': {'direction': 'high', 'percentile': 25, 'name': 'ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ç›¸é–¢'},
        'lab_L_mean': {'direction': 'neutral', 'percentile': 50, 'name': 'LABæ˜åº¦ï¼ˆå‚è€ƒå€¤ï¼‰'},
        'total_score': {'direction': 'high', 'percentile': 25, 'name': 'ç·åˆã‚¹ã‚³ã‚¢'},
    }

    for metric, config in metrics_config.items():
        data = df[metric].dropna()

        if config['direction'] == 'neutral':
            # ä¸­ç«‹çš„ãªæŒ‡æ¨™ï¼ˆæ˜åº¦ãªã©ï¼‰ï¼šä¸­å¤®å€¤ã‚’å‚è€ƒå€¤ã¨ã—ã¦è¡¨ç¤º
            threshold = np.percentile(data, config['percentile'])
            condition = f"å‚è€ƒå€¤: {threshold:.4f}"
        elif config['direction'] == 'high':
            # é«˜ã„æ–¹ãŒè‰¯ã„æŒ‡æ¨™ï¼š25ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ä»¥ä¸Šã‚’æ¨å¥¨
            threshold = np.percentile(data, config['percentile'])
            condition = f">= {threshold:.4f}"
        else:
            # ä½ã„æ–¹ãŒè‰¯ã„æŒ‡æ¨™ï¼š75ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ä»¥ä¸‹ã‚’æ¨å¥¨
            threshold = np.percentile(data, config['percentile'])
            condition = f"<= {threshold:.4f}"

        thresholds[metric] = {
            'name': config['name'],
            'threshold': threshold,
            'condition': condition,
            'mean': data.mean(),
            'std': data.std(),
            'min': data.min(),
            'max': data.max()
        }

        print(f"{config['name']:30s}: {condition:20s} (å¹³å‡: {data.mean():.4f}, æ¨™æº–åå·®: {data.std():.4f})")

    print(f"{'='*80}")
    print(f"ğŸ’¡ è§£é‡ˆ:")
    print(f"   - ã“ã‚Œã‚‰ã®é–¾å€¤ã¯ã€å…¨ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆåˆ†å¸ƒã«åŸºã¥ã„ã¦ã„ã¾ã™")
    print(f"   - 25ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ« = ä¸Šä½75%ã®å“è³ªã‚’ã€Œåˆæ ¼ã€ã¨ã™ã‚‹åŸºæº–")
    print(f"   - 75ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ« = ä¸‹ä½75%ã®å“è³ªã‚’ã€Œåˆæ ¼ã€ã¨ã™ã‚‹åŸºæº–")
    print(f"{'='*80}\n")

    # JSONä¿å­˜
    import json
    with open(output_dir / 'recommended_thresholds.json', 'w', encoding='utf-8') as f:
        json.dump(thresholds, f, indent=2, ensure_ascii=False)

    print(f"ğŸ’¾ é–¾å€¤ä¿å­˜: {output_dir}/recommended_thresholds.json\n")


def suggest_hallucination_logic(df, output_dir):
    """
    ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ã®ææ¡ˆï¼ˆ26ãƒ‘ã‚¿ãƒ¼ãƒ³å®Œå…¨å¯¾å¿œç‰ˆï¼‰
    - 9ã¤ã®çµ„ã¿åˆã‚ã›ãƒ‘ã‚¿ãƒ¼ãƒ³
    - 17ã®å˜ç‹¬é–¾å€¤åˆ¤å®š
    """

    print(f"\nğŸ” ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ã®ææ¡ˆï¼ˆ26ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰:")
    print(f"{'='*80}")

    # æ¤œå‡ºã‚«ã‚¦ãƒ³ãƒˆç”¨
    detection_count = pd.Series(0, index=df.index)
    detected_patterns = {idx: [] for idx in df.index}
    pattern_stats = {}

    # ========== çµ„ã¿åˆã‚ã›ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ9ã¤ï¼‰ ==========
    print(f"\nã€çµ„ã¿åˆã‚ã›ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆè¤‡åˆç•°å¸¸æ¤œå‡ºï¼‰ã€‘")
    print(f"{'='*80}")

    # === ãƒ‘ã‚¿ãƒ¼ãƒ³1: SSIMé«˜ Ã— PSNRä½ï¼ˆ2æ–¹å¼çµ±åˆï¼‰ ===
    # æ–¹æ³•A: å›ºå®šé–¾å€¤
    pattern1a = df[(df['ssim'] > 0.97) & (df['psnr'] < 25)]
    # æ–¹æ³•B: å‹•çš„é–¾å€¤
    ssim_high = df['ssim'].quantile(0.75)
    psnr_low = df['psnr'].quantile(0.25)
    pattern1b = df[(df['ssim'] >= ssim_high) & (df['psnr'] <= psnr_low)]
    # çµ±åˆ
    pattern1 = pd.concat([pattern1a, pattern1b]).drop_duplicates()
    detection_count[pattern1.index] += 1
    for idx in pattern1.index:
        detected_patterns[idx].append('P1:SSIMé«˜Ã—PSNRä½')
    pattern_stats['P1'] = {'count': len(pattern1), 'rate': len(pattern1)/len(df)*100}

    print(f"P1: SSIMé«˜ Ã— PSNRä½ï¼ˆæ§‹é€ é¡ä¼¼ã ãŒãƒ”ã‚¯ã‚»ãƒ«å€¤ç›¸é•ï¼‰")
    print(f"    å›ºå®šé–¾å€¤ (SSIM>0.97 & PSNR<25): {len(pattern1a)}ä»¶")
    print(f"    å‹•çš„é–¾å€¤ (SSIMâ‰¥{ssim_high:.4f} & PSNRâ‰¤{psnr_low:.2f}): {len(pattern1b)}ä»¶")
    print(f"    çµ±åˆå¾Œ: {len(pattern1)}ä»¶ ({len(pattern1)/len(df)*100:.1f}%)")
    print(f"    ãƒªã‚¹ã‚¯: ä¸­ï½é«˜ï¼ˆAIãŒæ§‹é€ ã‚’æ¨¡å€£ã—ãŸå¯èƒ½æ€§ï¼‰")

    # === ãƒ‘ã‚¿ãƒ¼ãƒ³2: ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹é«˜ Ã— ãƒã‚¤ã‚ºé«˜ ===
    sharp_high = df['sharpness'].quantile(0.75)
    noise_high = df['noise'].quantile(0.75)
    pattern2 = df[(df['sharpness'] > sharp_high) & (df['noise'] > noise_high)]
    detection_count[pattern2.index] += 1
    for idx in pattern2.index:
        detected_patterns[idx].append('P2:ã‚·ãƒ£ãƒ¼ãƒ—é«˜Ã—ãƒã‚¤ã‚ºé«˜')
    pattern_stats['P2'] = {'count': len(pattern2), 'rate': len(pattern2)/len(df)*100}

    print(f"\nP2: ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹é«˜ Ã— ãƒã‚¤ã‚ºé«˜ï¼ˆéå‰°å‡¦ç†ï¼‰")
    print(f"    æ¡ä»¶: ã‚·ãƒ£ãƒ¼ãƒ—>{sharp_high:.2f} & ãƒã‚¤ã‚º>{noise_high:.2f}")
    print(f"    è©²å½“: {len(pattern2)}ä»¶ ({len(pattern2)/len(df)*100:.1f}%)")
    print(f"    ãƒªã‚¹ã‚¯: ä¸­ï¼ˆéåº¦ãªã‚·ãƒ£ãƒ¼ãƒ—åŒ–ã«ã‚ˆã‚‹ãƒã‚¤ã‚ºå¢—å¹…ï¼‰")

    # === ãƒ‘ã‚¿ãƒ¼ãƒ³3: ã‚¨ãƒƒã‚¸å¯†åº¦é«˜ Ã— å±€æ‰€å“è³ªä½ ===
    edge_90 = df['edge_density'].quantile(0.90)
    quality_25 = df['local_quality_mean'].quantile(0.25)
    pattern3 = df[(df['edge_density'] > edge_90) & (df['local_quality_mean'] < quality_25)]
    detection_count[pattern3.index] += 1
    for idx in pattern3.index:
        detected_patterns[idx].append('P3:ã‚¨ãƒƒã‚¸é«˜Ã—å“è³ªä½')
    pattern_stats['P3'] = {'count': len(pattern3), 'rate': len(pattern3)/len(df)*100}

    print(f"\nP3: ã‚¨ãƒƒã‚¸å¯†åº¦é«˜ Ã— å±€æ‰€å“è³ªä½ï¼ˆä¸è‡ªç„¶ãªã‚¨ãƒƒã‚¸ï¼‰")
    print(f"    æ¡ä»¶: ã‚¨ãƒƒã‚¸>{edge_90:.2f} & å±€æ‰€å“è³ª<{quality_25:.4f}")
    print(f"    è©²å½“: {len(pattern3)}ä»¶ ({len(pattern3)/len(df)*100:.1f}%)")
    print(f"    ãƒªã‚¹ã‚¯: ä¸­ï½é«˜ï¼ˆã‚¨ãƒƒã‚¸è¿½åŠ ãŒä¸å‡ä¸€ï¼‰")

    # === ãƒ‘ã‚¿ãƒ¼ãƒ³4: ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆç•°å¸¸é«˜ ===
    artifact_90 = df['artifact_total'].quantile(0.90)
    pattern4 = df[df['artifact_total'] > artifact_90]
    detection_count[pattern4.index] += 1
    for idx in pattern4.index:
        detected_patterns[idx].append('P4:Artifactsé«˜')
    pattern_stats['P4'] = {'count': len(pattern4), 'rate': len(pattern4)/len(df)*100}

    print(f"\nP4: ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆç•°å¸¸é«˜ï¼ˆGANç‰¹æœ‰ã®æ­ªã¿ï¼‰")
    print(f"    æ¡ä»¶: Artifacts>{artifact_90:.2f}")
    print(f"    è©²å½“: {len(pattern4)}ä»¶ ({len(pattern4)/len(df)*100:.1f}%)")
    print(f"    ãƒªã‚¹ã‚¯: é«˜ï¼ˆãƒªãƒ³ã‚®ãƒ³ã‚°ãƒ»ãƒ–ãƒ­ãƒƒã‚¯ãƒã‚¤ã‚ºï¼‰")

    # === ãƒ‘ã‚¿ãƒ¼ãƒ³5: LPIPSé«˜ Ã— SSIMé«˜ ===
    lpips_75 = df['lpips'].quantile(0.75)
    ssim_75 = df['ssim'].quantile(0.75)
    pattern5 = df[(df['lpips'] > lpips_75) & (df['ssim'] > ssim_75)]
    detection_count[pattern5.index] += 1
    for idx in pattern5.index:
        detected_patterns[idx].append('P5:LPIPSé«˜Ã—SSIMé«˜')
    pattern_stats['P5'] = {'count': len(pattern5), 'rate': len(pattern5)/len(df)*100}

    print(f"\nP5: LPIPSé«˜ Ã— SSIMé«˜ï¼ˆçŸ¥è¦šã¨æ§‹é€ ã®çŸ›ç›¾ï¼‰")
    print(f"    æ¡ä»¶: LPIPS>{lpips_75:.4f} & SSIM>{ssim_75:.4f}")
    print(f"    è©²å½“: {len(pattern5)}ä»¶ ({len(pattern5)/len(df)*100:.1f}%)")
    print(f"    ãƒªã‚¹ã‚¯: ä¸­ï¼ˆæ§‹é€ ã¯ä¼¼ã¦ã„ã‚‹ãŒçŸ¥è¦šçš„ã«ç•°ãªã‚‹ï¼‰")

    # === ãƒ‘ã‚¿ãƒ¼ãƒ³6: å±€æ‰€å“è³ªã°ã‚‰ã¤ãå¤§ ===
    if 'local_quality_std' in df.columns:
        quality_std_75 = df['local_quality_std'].quantile(0.75)
        pattern6 = df[df['local_quality_std'] > quality_std_75]
        detection_count[pattern6.index] += 1
        for idx in pattern6.index:
            detected_patterns[idx].append('P6:å“è³ªã°ã‚‰ã¤ãå¤§')
        pattern_stats['P6'] = {'count': len(pattern6), 'rate': len(pattern6)/len(df)*100}

        print(f"\nP6: å±€æ‰€å“è³ªã°ã‚‰ã¤ãå¤§ï¼ˆä¸å‡ä¸€ãªå‡¦ç†ï¼‰")
        print(f"    æ¡ä»¶: å±€æ‰€SSIMæ¨™æº–åå·®>{quality_std_75:.4f}")
        print(f"    è©²å½“: {len(pattern6)}ä»¶ ({len(pattern6)/len(df)*100:.1f}%)")
        print(f"    ãƒªã‚¹ã‚¯: ä¸­ï½é«˜ï¼ˆé ˜åŸŸã«ã‚ˆã£ã¦å“è³ªãŒç•°ãªã‚‹ï¼‰")
    else:
        pattern_stats['P6'] = {'count': 0, 'rate': 0}
        print(f"\nP6: å±€æ‰€å“è³ªã°ã‚‰ã¤ãå¤§ â†’ ãƒ‡ãƒ¼ã‚¿ãªã—ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")

    # === ãƒ‘ã‚¿ãƒ¼ãƒ³7: Entropyä½ Ã— High-Freqé«˜ ===
    entropy_25 = df['entropy'].quantile(0.25)
    highfreq_75 = df['high_freq_ratio'].quantile(0.75)
    pattern7 = df[(df['entropy'] < entropy_25) & (df['high_freq_ratio'] > highfreq_75)]
    detection_count[pattern7.index] += 1
    for idx in pattern7.index:
        detected_patterns[idx].append('P7:Entropyä½Ã—é«˜å‘¨æ³¢é«˜')
    pattern_stats['P7'] = {'count': len(pattern7), 'rate': len(pattern7)/len(df)*100}

    print(f"\nP7: Entropyä½ Ã— é«˜å‘¨æ³¢é«˜ï¼ˆåå¾©ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰")
    print(f"    æ¡ä»¶: Entropy<{entropy_25:.3f} & é«˜å‘¨æ³¢>{highfreq_75:.4f}")
    print(f"    è©²å½“: {len(pattern7)}ä»¶ ({len(pattern7)/len(df)*100:.1f}%)")
    print(f"    ãƒªã‚¹ã‚¯: ä¸­ï¼ˆäººå·¥çš„ãªåå¾©ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰")

    # === ãƒ‘ã‚¿ãƒ¼ãƒ³8: Contrastç•°å¸¸ Ã— Histogramç›¸é–¢ä½ ===
    contrast_90 = df['contrast'].quantile(0.90)
    histcorr_25 = df['histogram_corr'].quantile(0.25)
    pattern8 = df[(df['contrast'] > contrast_90) & (df['histogram_corr'] < histcorr_25)]
    detection_count[pattern8.index] += 1
    for idx in pattern8.index:
        detected_patterns[idx].append('P8:Contrastç•°å¸¸Ã—Histç›¸é–¢ä½')
    pattern_stats['P8'] = {'count': len(pattern8), 'rate': len(pattern8)/len(df)*100}

    print(f"\nP8: Contrastç•°å¸¸ Ã— Histogramç›¸é–¢ä½ï¼ˆæ¿ƒåº¦åˆ†å¸ƒå´©å£Šï¼‰")
    print(f"    æ¡ä»¶: Contrast>{contrast_90:.2f} & Histç›¸é–¢<{histcorr_25:.4f}")
    print(f"    è©²å½“: {len(pattern8)}ä»¶ ({len(pattern8)/len(df)*100:.1f}%)")
    print(f"    ãƒªã‚¹ã‚¯: ä¸­ï¼ˆã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå¼·èª¿ã§æ¿ƒåº¦åˆ†å¸ƒãŒå´©ã‚Œã¦ã„ã‚‹ï¼‰")

    # === ãƒ‘ã‚¿ãƒ¼ãƒ³9: MS-SSIMä½ Ã— ç·åˆã‚¹ã‚³ã‚¢ä½ ===
    msssim_25 = df['ms_ssim'].quantile(0.25)
    total_25 = df['total_score'].quantile(0.25)
    pattern9 = df[(df['ms_ssim'] < msssim_25) & (df['total_score'] < total_25)]
    detection_count[pattern9.index] += 1
    for idx in pattern9.index:
        detected_patterns[idx].append('P9:MS-SSIMä½Ã—ç·åˆä½')
    pattern_stats['P9'] = {'count': len(pattern9), 'rate': len(pattern9)/len(df)*100}

    print(f"\nP9: MS-SSIMä½ Ã— ç·åˆã‚¹ã‚³ã‚¢ä½ï¼ˆç·åˆçš„ä½å“è³ªï¼‰")
    print(f"    æ¡ä»¶: MS-SSIM<{msssim_25:.4f} & ç·åˆ<{total_25:.2f}")
    print(f"    è©²å½“: {len(pattern9)}ä»¶ ({len(pattern9)/len(df)*100:.1f}%)")
    print(f"    ãƒªã‚¹ã‚¯: é«˜ï¼ˆè¤‡æ•°ã‚¹ã‚±ãƒ¼ãƒ«ã§å“è³ªåŠ£åŒ–ï¼‰")

    # ========== å˜ç‹¬ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ17é …ç›®ï¼‰ ==========
    print(f"\n{'='*80}")
    print(f"ã€å˜ç‹¬é–¾å€¤åˆ¤å®šãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ17é …ç›®ï¼‰ã€‘")
    print(f"{'='*80}")

    single_pattern_count = 0

    # é«˜ã„æ–¹ãŒè‰¯ã„æŒ‡æ¨™ï¼ˆç•°å¸¸ã«ä½ã„ = ä¸‹ä½10%ï¼‰
    high_is_good = [
        ('ssim', 'SSIMä½'), ('ms_ssim', 'MS-SSIMä½'), ('psnr', 'PSNRä½'),
        ('sharpness', 'Sharpnessä½'), ('contrast', 'Contrastä½'), ('entropy', 'Entropyä½'),
        ('edge_density', 'EdgeDensityä½'), ('high_freq_ratio', 'HighFreqä½'),
        ('texture_complexity', 'Textureä½'), ('local_quality_mean', 'LocalQualityä½'),
        ('histogram_corr', 'HistCorrä½'), ('total_score', 'TotalScoreä½')
    ]

    print(f"\né«˜ã„æ–¹ãŒè‰¯ã„æŒ‡æ¨™ï¼ˆä¸‹ä½10%ã‚’ç•°å¸¸æ¤œå‡ºï¼‰:")
    for col, name in high_is_good:
        if col in df.columns:
            threshold = df[col].quantile(0.10)
            detected = df[df[col] < threshold]
            detection_count[detected.index] += 1
            for idx in detected.index:
                detected_patterns[idx].append(f'å˜ç‹¬:{name}')
            single_pattern_count += len(detected)
            print(f"  {name:20s}: <{threshold:8.4f} â†’ {len(detected):4d}ä»¶ ({len(detected)/len(df)*100:5.1f}%)")

    # ä½ã„æ–¹ãŒè‰¯ã„æŒ‡æ¨™ï¼ˆç•°å¸¸ã«é«˜ã„ = ä¸Šä½10%ï¼‰
    low_is_good = [
        ('lpips', 'LPIPSé«˜'), ('noise', 'Noiseé«˜'),
        ('artifact_total', 'Artifactsé«˜'), ('delta_e', 'DeltaEé«˜')
    ]

    print(f"\nä½ã„æ–¹ãŒè‰¯ã„æŒ‡æ¨™ï¼ˆä¸Šä½10%ã‚’ç•°å¸¸æ¤œå‡ºï¼‰:")
    for col, name in low_is_good:
        if col in df.columns:
            threshold = df[col].quantile(0.90)
            detected = df[df[col] > threshold]
            detection_count[detected.index] += 1
            for idx in detected.index:
                detected_patterns[idx].append(f'å˜ç‹¬:{name}')
            single_pattern_count += len(detected)
            print(f"  {name:20s}: >{threshold:8.4f} â†’ {len(detected):4d}ä»¶ ({len(detected)/len(df)*100:5.1f}%)")

    if 'local_quality_std' in df.columns:
        threshold = df['local_quality_std'].quantile(0.90)
        detected = df[df['local_quality_std'] > threshold]
        detection_count[detected.index] += 1
        for idx in detected.index:
            detected_patterns[idx].append(f'å˜ç‹¬:LocalQualityStdé«˜')
        single_pattern_count += len(detected)
        print(f"  {'LocalQualityStdé«˜':20s}: >{threshold:8.4f} â†’ {len(detected):4d}ä»¶ ({len(detected)/len(df)*100:5.1f}%)")

    print(f"\nå˜ç‹¬ãƒ‘ã‚¿ãƒ¼ãƒ³åˆè¨ˆæ¤œå‡º: {single_pattern_count}ä»¶ï¼ˆå»¶ã¹æ•°ï¼‰")

    # ========== ç·åˆãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢è¨ˆç®— ==========
    print(f"\n{'='*80}")
    print(f"ğŸ“Š ç·åˆãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ï¼ˆ26ãƒ‘ã‚¿ãƒ¼ãƒ³çµ±åˆï¼‰")
    print(f"{'='*80}")

    # ä¿¡é ¼åº¦åˆ†é¡ï¼ˆå¤šæ•°æ±ºï¼‰
    high_confidence = df[detection_count >= 5]  # 5ãƒ‘ã‚¿ãƒ¼ãƒ³ä»¥ä¸Š
    medium_confidence = df[(detection_count >= 3) & (detection_count < 5)]  # 3-4ãƒ‘ã‚¿ãƒ¼ãƒ³
    low_confidence = df[(detection_count >= 1) & (detection_count < 3)]  # 1-2ãƒ‘ã‚¿ãƒ¼ãƒ³
    no_detection = df[detection_count == 0]  # æ¤œå‡ºãªã—ï¼ˆæ­£å¸¸ï¼‰

    print(f"\nä¿¡é ¼åº¦åˆ¥åˆ†é¡:")
    print(f"  é«˜ä¿¡é ¼åº¦(5+ãƒ‘ã‚¿ãƒ¼ãƒ³): {len(high_confidence):5d}ä»¶ ({len(high_confidence)/len(df)*100:5.1f}%)")
    print(f"  ä¸­ä¿¡é ¼åº¦(3-4ãƒ‘ã‚¿ãƒ¼ãƒ³): {len(medium_confidence):5d}ä»¶ ({len(medium_confidence)/len(df)*100:5.1f}%)")
    print(f"  ä½ä¿¡é ¼åº¦(1-2ãƒ‘ã‚¿ãƒ¼ãƒ³): {len(low_confidence):5d}ä»¶ ({len(low_confidence)/len(df)*100:5.1f}%)")
    print(f"  æ­£å¸¸(æ¤œå‡º0):          {len(no_detection):5d}ä»¶ ({len(no_detection)/len(df)*100:5.1f}%)")

    # DataFrameã«çµæœã‚’è¿½åŠ 
    df['detection_count'] = detection_count
    df['detected_patterns'] = df.index.map(lambda idx: ', '.join(detected_patterns[idx]) if detected_patterns[idx] else 'None')

    # ä¿¡é ¼åº¦ãƒ¬ãƒ™ãƒ«
    df['confidence_level'] = 'Normal'
    df.loc[detection_count >= 1, 'confidence_level'] = 'Low'
    df.loc[detection_count >= 3, 'confidence_level'] = 'Medium'
    df.loc[detection_count >= 5, 'confidence_level'] = 'High'

    # ãƒªã‚¹ã‚¯ä»˜ãCSVä¿å­˜
    output_csv = output_dir / 'results_with_26pattern_detection.csv'
    df.to_csv(output_csv, index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ 26ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºçµæœä¿å­˜: {output_csv}")

    # ã‚µãƒãƒªãƒ¼CSVä¿å­˜
    summary_data = {
        'pattern_name': [],
        'detection_count': [],
        'detection_rate_%': []
    }

    for p_name, stats in pattern_stats.items():
        summary_data['pattern_name'].append(p_name)
        summary_data['detection_count'].append(stats['count'])
        summary_data['detection_rate_%'].append(stats['rate'])

    summary_df = pd.DataFrame(summary_data)
    summary_path = output_dir / 'pattern_detection_summary.csv'
    summary_df.to_csv(summary_path, index=False, encoding='utf-8-sig')
    print(f"ğŸ’¾ ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã‚µãƒãƒªãƒ¼ä¿å­˜: {summary_path}")

    print(f"{'='*80}\n")


def generate_research_plots(df, output_dir):
    """
    ç ”ç©¶ç”¨ãƒ—ãƒ­ãƒƒãƒˆç”»åƒã‚’ç”Ÿæˆï¼ˆè«–æ–‡ãƒ»ç™ºè¡¨ç”¨ï¼‰
    """

    print(f"\nğŸ“Š ç ”ç©¶ç”¨ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆä¸­:")
    print(f"{'='*80}")

    # 1. ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹ vs PSNR æ•£å¸ƒå›³ï¼ˆAIãƒ¢ãƒ‡ãƒ«ã®æˆ¦ç•¥ã‚’ç¤ºã™ï¼‰
    plt.figure(figsize=(12, 8))

    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['psnr'], model_data['sharpness'],
                   label=model, alpha=0.6, s=50)

    plt.xlabel('PSNRï¼ˆå¿ å®Ÿåº¦ï¼‰[dB]', fontsize=14, fontweight='bold')
    plt.ylabel('ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹ï¼ˆé®®æ˜åº¦ï¼‰', fontsize=14, fontweight='bold')
    plt.title('AIãƒ¢ãƒ‡ãƒ«ã®æˆ¦ç•¥ãƒãƒƒãƒ—ï¼šå¿ å®Ÿåº¦ vs é®®æ˜åº¦', fontsize=16, fontweight='bold')
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)

    # æˆ¦ç•¥é ˜åŸŸã®æ³¨é‡ˆ
    plt.axhline(y=df['sharpness'].median(), color='red', linestyle='--', alpha=0.3, label='ä¸­å¤®å€¤')
    plt.axvline(x=df['psnr'].median(), color='red', linestyle='--', alpha=0.3)

    # é ˜åŸŸãƒ©ãƒ™ãƒ«
    max_psnr = df['psnr'].max()
    max_sharp = df['sharpness'].max()
    plt.text(max_psnr * 0.95, max_sharp * 0.95, 'ç†æƒ³é ˜åŸŸ\nï¼ˆé«˜å¿ å®Ÿãƒ»é«˜é®®æ˜ï¼‰',
             fontsize=10, ha='right', va='top', bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))
    plt.text(df['psnr'].min() * 1.05, max_sharp * 0.95, 'éå‰°å‡¦ç†é ˜åŸŸ\nï¼ˆä½å¿ å®Ÿãƒ»é«˜é®®æ˜ï¼‰\nãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ç–‘ã„',
             fontsize=10, ha='left', va='top', bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.5))

    plt.tight_layout()
    plot1_path = output_dir / 'strategy_map_sharpness_vs_psnr.png'
    plt.savefig(plot1_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹ vs PSNR æ•£å¸ƒå›³: {plot1_path}")


    # 2. LPIPS ç®±ã²ã’å›³ï¼ˆå®‰å®šæ€§ã‚’ç¤ºã™ï¼‰
    plt.figure(figsize=(10, 6))

    lpips_data = [df[df['model'] == model]['lpips'].values for model in df['model'].unique()]
    models = df['model'].unique()

    bp = plt.boxplot(lpips_data, labels=models, patch_artist=True,
                     showmeans=True, meanline=True)

    # ã‚«ãƒ©ãƒ¼ãƒªãƒ³ã‚°
    colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow', 'lightpink']
    for patch, color in zip(bp['boxes'], colors):
        patch.set_facecolor(color)

    plt.ylabel('LPIPSï¼ˆçŸ¥è¦šçš„é¡ä¼¼åº¦ï¼‰', fontsize=14, fontweight='bold')
    plt.xlabel('AIãƒ¢ãƒ‡ãƒ«', fontsize=14, fontweight='bold')
    plt.title('ãƒ¢ãƒ‡ãƒ«åˆ¥ LPIPS åˆ†å¸ƒï¼ˆå®‰å®šæ€§è©•ä¾¡ï¼‰\nç®±ãŒå°ã•ã„ = å®‰å®š', fontsize=16, fontweight='bold')
    plt.grid(axis='y', alpha=0.3)
    plt.xticks(rotation=45, ha='right')

    plt.tight_layout()
    plot2_path = output_dir / 'stability_lpips_boxplot.png'
    plt.savefig(plot2_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… LPIPS ç®±ã²ã’å›³: {plot2_path}")


    # 3. SSIM vs PSNR æ•£å¸ƒå›³ï¼ˆç›¸é–¢ç¢ºèªãƒ»ç•°å¸¸æ¤œå‡ºï¼‰
    plt.figure(figsize=(12, 8))

    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['ssim'], model_data['psnr'],
                   label=model, alpha=0.6, s=50)

    plt.xlabel('SSIMï¼ˆæ§‹é€ é¡ä¼¼æ€§ï¼‰', fontsize=14, fontweight='bold')
    plt.ylabel('PSNRï¼ˆä¿¡å·å¯¾é›‘éŸ³æ¯”ï¼‰[dB]', fontsize=14, fontweight='bold')
    plt.title('SSIM vs PSNR ç›¸é–¢åˆ†æ\nç›¸é–¢ã‹ã‚‰å¤–ã‚Œã‚‹ç‚¹ = ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³å€™è£œ', fontsize=16, fontweight='bold')
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)

    # è¿‘ä¼¼ç›´ç·š
    from scipy import stats
    slope, intercept, r_value, p_value, std_err = stats.linregress(df['ssim'], df['psnr'])
    x_line = np.array([df['ssim'].min(), df['ssim'].max()])
    y_line = slope * x_line + intercept
    plt.plot(x_line, y_line, 'r--', label=f'å›å¸°ç›´ç·š (RÂ²={r_value**2:.3f})', linewidth=2)
    plt.legend(fontsize=12)

    plt.tight_layout()
    plot3_path = output_dir / 'correlation_ssim_vs_psnr.png'
    plt.savefig(plot3_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… SSIM vs PSNR æ•£å¸ƒå›³: {plot3_path}")


    # 4. ãƒã‚¤ã‚º vs ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ æ•£å¸ƒå›³
    plt.figure(figsize=(12, 8))

    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['noise'], model_data['artifact_total'],
                   label=model, alpha=0.6, s=50)

    plt.xlabel('ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«', fontsize=14, fontweight='bold')
    plt.ylabel('ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆï¼ˆãƒ–ãƒ­ãƒƒã‚¯+ãƒªãƒ³ã‚®ãƒ³ã‚°ï¼‰', fontsize=14, fontweight='bold')
    plt.title('ãƒã‚¤ã‚º vs ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ\nå·¦ä¸‹ãŒç†æƒ³ï¼ˆä¸¡æ–¹å°‘ãªã„ï¼‰', fontsize=16, fontweight='bold')
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)

    # ç†æƒ³é ˜åŸŸã®è¡¨ç¤º
    low_noise = df['noise'].quantile(0.25)
    low_artifact = df['artifact_total'].quantile(0.25)
    plt.axvline(x=low_noise, color='green', linestyle='--', alpha=0.3)
    plt.axhline(y=low_artifact, color='green', linestyle='--', alpha=0.3)
    plt.fill_between([0, low_noise], 0, low_artifact, alpha=0.1, color='green', label='ç†æƒ³é ˜åŸŸ')
    plt.legend(fontsize=12)

    plt.tight_layout()
    plot4_path = output_dir / 'quality_noise_vs_artifact.png'
    plt.savefig(plot4_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ãƒã‚¤ã‚º vs ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ: {plot4_path}")


    # 5. ãƒ¢ãƒ‡ãƒ«åˆ¥ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆï¼ˆä¸»è¦6æŒ‡æ¨™ï¼‰
    fig = plt.figure(figsize=(14, 10))

    categories = ['SSIM', 'PSNR', 'ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹', 'ã‚¨ãƒƒã‚¸å¯†åº¦', 'ãƒã‚¤ã‚º\nï¼ˆåè»¢ï¼‰', 'ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ\nï¼ˆåè»¢ï¼‰']
    num_vars = len(categories)

    # æ­£è¦åŒ–ï¼ˆ0-1ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
    df_norm = df.copy()
    df_norm['ssim_norm'] = df['ssim']
    df_norm['psnr_norm'] = df['psnr'] / df['psnr'].max()
    df_norm['sharpness_norm'] = df['sharpness'] / df['sharpness'].max()
    df_norm['edge_norm'] = df['edge_density'] / df['edge_density'].max()
    df_norm['noise_norm'] = 1 - (df['noise'] / df['noise'].max())  # åè»¢ï¼ˆå°‘ãªã„æ–¹ãŒè‰¯ã„ï¼‰
    df_norm['artifact_norm'] = 1 - (df['artifact_total'] / df['artifact_total'].max())  # åè»¢

    angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()
    angles += angles[:1]

    ax = fig.add_subplot(111, polar=True)

    for model in df['model'].unique():
        model_data = df_norm[df_norm['model'] == model]
        values = [
            model_data['ssim_norm'].mean(),
            model_data['psnr_norm'].mean(),
            model_data['sharpness_norm'].mean(),
            model_data['edge_norm'].mean(),
            model_data['noise_norm'].mean(),
            model_data['artifact_norm'].mean()
        ]
        values += values[:1]

        ax.plot(angles, values, 'o-', linewidth=2, label=model)
        ax.fill(angles, values, alpha=0.1)

    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(categories, fontsize=12)
    ax.set_ylim(0, 1)
    ax.set_title('ãƒ¢ãƒ‡ãƒ«åˆ¥æ€§èƒ½ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆï¼‰\nå¤–å´ã»ã©é«˜æ€§èƒ½', fontsize=16, fontweight='bold', pad=20)
    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=12)
    ax.grid(True)

    plt.tight_layout()
    plot5_path = output_dir / 'radar_chart_model_comparison.png'
    plt.savefig(plot5_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ãƒ¢ãƒ‡ãƒ«åˆ¥ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ: {plot5_path}")


    # 6. 17é …ç›®ã®ãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆï¼ˆåˆ†å¸ƒã®å¯è¦–åŒ–ï¼‰
    fig, axes = plt.subplots(3, 6, figsize=(24, 12))
    fig.suptitle('17é …ç›®ã®åˆ†å¸ƒï¼ˆãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆï¼‰', fontsize=20, fontweight='bold')

    metrics_for_violin = [
        'ssim', 'ms_ssim', 'psnr', 'lpips', 'sharpness', 'contrast',
        'entropy', 'noise', 'edge_density', 'artifact_total', 'delta_e',
        'high_freq_ratio', 'texture_complexity', 'local_quality_mean',
        'histogram_corr', 'lab_L_mean', 'total_score'
    ]

    for i, metric in enumerate(metrics_for_violin):
        ax = axes[i // 6, i % 6]

        violin_data = [df[df['model'] == model][metric].values for model in df['model'].unique()]
        parts = ax.violinplot(violin_data, showmeans=True, showmedians=True)

        # ã‚«ãƒ©ãƒ¼ãƒªãƒ³ã‚°
        for pc in parts['bodies']:
            pc.set_facecolor('lightblue')
            pc.set_alpha(0.7)

        ax.set_xticks(range(1, len(df['model'].unique()) + 1))
        ax.set_xticklabels(df['model'].unique(), rotation=45, ha='right', fontsize=8)
        ax.set_title(metric, fontsize=10, fontweight='bold')
        ax.grid(axis='y', alpha=0.3)

    # ä½™ã£ãŸè»¸ã‚’éè¡¨ç¤º
    for i in range(len(metrics_for_violin), 18):
        axes[i // 6, i % 6].axis('off')

    plt.tight_layout()
    plot6_path = output_dir / 'violin_plots_all_metrics.png'
    plt.savefig(plot6_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… 17é …ç›®ãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆ: {plot6_path}")

    # ===== ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºç³»ãƒ—ãƒ­ãƒƒãƒˆ =====

    # 7. SSIMé«˜ Ã— PSNRä½ ã®ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ç–‘ã„å¯è¦–åŒ–
    plt.figure(figsize=(12, 8))

    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['ssim'], model_data['psnr'],
                   label=model, alpha=0.6, s=50)

    # ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ç–‘ã„é ˜åŸŸã‚’èµ¤ã§å¼·èª¿
    ssim_high = df['ssim'].quantile(0.75)
    psnr_low = df['psnr'].quantile(0.25)
    hallucination_candidates = df[(df['ssim'] >= ssim_high) & (df['psnr'] <= psnr_low)]

    if len(hallucination_candidates) > 0:
        plt.scatter(hallucination_candidates['ssim'], hallucination_candidates['psnr'],
                   color='red', s=200, marker='x', linewidths=3,
                   label=f'ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ç–‘ã„ ({len(hallucination_candidates)}ä»¶)', zorder=10)

    plt.axhline(y=psnr_low, color='orange', linestyle='--', alpha=0.5, label=f'PSNRé–¾å€¤ ({psnr_low:.1f})')
    plt.axvline(x=ssim_high, color='orange', linestyle='--', alpha=0.5, label=f'SSIMé–¾å€¤ ({ssim_high:.3f})')

    plt.xlabel('SSIMï¼ˆæ§‹é€ é¡ä¼¼æ€§ï¼‰', fontsize=14, fontweight='bold')
    plt.ylabel('PSNR [dB]', fontsize=14, fontweight='bold')
    plt.title('ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºï¼šSSIMé«˜ & PSNRä½\nå³ä¸‹é ˜åŸŸ = æ§‹é€ ã‚’æ¨¡å€£ã—ãŸãŒå¿ å®Ÿæ€§ãŒä½ã„',
              fontsize=16, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'hallucination_ssim_high_psnr_low.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºâ‘ ï¼ˆSSIMÃ—PSNRï¼‰")


    # 8. ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹ Ã— ãƒã‚¤ã‚ºï¼ˆéå‰°å‡¦ç†æ¤œå‡ºï¼‰
    plt.figure(figsize=(12, 8))

    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['sharpness'], model_data['noise'],
                   label=model, alpha=0.6, s=50)

    sharp_high = df['sharpness'].quantile(0.75)
    noise_high = df['noise'].quantile(0.75)
    over_processed = df[(df['sharpness'] >= sharp_high) & (df['noise'] >= noise_high)]

    if len(over_processed) > 0:
        plt.scatter(over_processed['sharpness'], over_processed['noise'],
                   color='red', s=200, marker='x', linewidths=3,
                   label=f'éå‰°å‡¦ç†ç–‘ã„ ({len(over_processed)}ä»¶)', zorder=10)

    plt.axhline(y=noise_high, color='orange', linestyle='--', alpha=0.5)
    plt.axvline(x=sharp_high, color='orange', linestyle='--', alpha=0.5)

    plt.xlabel('ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹', fontsize=14, fontweight='bold')
    plt.ylabel('ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«', fontsize=14, fontweight='bold')
    plt.title('éå‰°å‡¦ç†æ¤œå‡ºï¼šé«˜ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹ & é«˜ãƒã‚¤ã‚º\nå³ä¸Šé ˜åŸŸ = ã‚·ãƒ£ãƒ¼ãƒ—åŒ–ã§ãƒã‚¤ã‚ºå¢—å¹…',
              fontsize=16, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'hallucination_sharpness_vs_noise.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºâ‘¡ï¼ˆã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹Ã—ãƒã‚¤ã‚ºï¼‰")


    # 9. ã‚¨ãƒƒã‚¸å¯†åº¦ Ã— å±€æ‰€å“è³ªæ¨™æº–åå·®
    plt.figure(figsize=(12, 8))

    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['edge_density'], model_data['local_quality_std'],
                   label=model, alpha=0.6, s=50)

    edge_high = df['edge_density'].quantile(0.75)
    local_std_high = df['local_quality_std'].quantile(0.75)
    unnatural_edges = df[(df['edge_density'] >= edge_high) & (df['local_quality_std'] >= local_std_high)]

    if len(unnatural_edges) > 0:
        plt.scatter(unnatural_edges['edge_density'], unnatural_edges['local_quality_std'],
                   color='red', s=200, marker='x', linewidths=3,
                   label=f'ä¸è‡ªç„¶ãªã‚¨ãƒƒã‚¸ç–‘ã„ ({len(unnatural_edges)}ä»¶)', zorder=10)

    plt.xlabel('ã‚¨ãƒƒã‚¸å¯†åº¦', fontsize=14, fontweight='bold')
    plt.ylabel('å±€æ‰€å“è³ªæ¨™æº–åå·®', fontsize=14, fontweight='bold')
    plt.title('ä¸è‡ªç„¶ãªã‚¨ãƒƒã‚¸æ¤œå‡ºï¼šã‚¨ãƒƒã‚¸å¢—åŠ  & å±€æ‰€å“è³ªã°ã‚‰ã¤ãå¤§\nå³ä¸Šé ˜åŸŸ = ã‚¨ãƒƒã‚¸è¿½åŠ ãŒä¸å‡ä¸€',
              fontsize=16, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'hallucination_edge_vs_local_std.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºâ‘¢ï¼ˆã‚¨ãƒƒã‚¸Ã—å±€æ‰€å“è³ªï¼‰")


    # 10. é«˜å‘¨æ³¢æˆåˆ† Ã— ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
    plt.figure(figsize=(12, 8))

    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['high_freq_ratio'], model_data['entropy'],
                   label=model, alpha=0.6, s=50)

    # å›å¸°ç›´ç·š
    from scipy import stats
    slope, intercept, r_value, p_value, std_err = stats.linregress(df['high_freq_ratio'], df['entropy'])
    x_line = np.array([df['high_freq_ratio'].min(), df['high_freq_ratio'].max()])
    y_line = slope * x_line + intercept
    plt.plot(x_line, y_line, 'r--', label=f'å›å¸°ç›´ç·š (RÂ²={r_value**2:.3f})', linewidth=2)

    plt.xlabel('é«˜å‘¨æ³¢æˆåˆ†æ¯”ç‡', fontsize=14, fontweight='bold')
    plt.ylabel('ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆæƒ…å ±é‡ï¼‰', fontsize=14, fontweight='bold')
    plt.title('äººå·¥ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºï¼šé«˜å‘¨æ³¢ vs ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼\nç›¸é–¢ã‹ã‚‰å¤–ã‚Œã‚‹ = åå¾©ãƒ‘ã‚¿ãƒ¼ãƒ³ç–‘ã„',
              fontsize=16, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'hallucination_highfreq_vs_entropy.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºâ‘£ï¼ˆé«˜å‘¨æ³¢Ã—ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼‰")


    # ===== å“è³ªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ç³»ãƒ—ãƒ­ãƒƒãƒˆ =====

    # 11. SSIM Ã— ãƒã‚¤ã‚º
    plt.figure(figsize=(12, 8))
    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['ssim'], model_data['noise'],
                   label=model, alpha=0.6, s=50)
    plt.xlabel('SSIMï¼ˆæ§‹é€ é¡ä¼¼æ€§ï¼‰', fontsize=14, fontweight='bold')
    plt.ylabel('ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«', fontsize=14, fontweight='bold')
    plt.title('å“è³ªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼šæ§‹é€ é¡ä¼¼æ€§ vs ãƒã‚¤ã‚º', fontsize=16, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'tradeoff_ssim_vs_noise.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•â‘ ï¼ˆSSIMÃ—ãƒã‚¤ã‚ºï¼‰")


    # 12. PSNR Ã— ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ
    plt.figure(figsize=(12, 8))
    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['psnr'], model_data['contrast'],
                   label=model, alpha=0.6, s=50)
    plt.xlabel('PSNR [dB]', fontsize=14, fontweight='bold')
    plt.ylabel('ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ', fontsize=14, fontweight='bold')
    plt.title('å“è³ªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼šå¿ å®Ÿåº¦ vs ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå¼·èª¿', fontsize=16, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'tradeoff_psnr_vs_contrast.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•â‘¡ï¼ˆPSNRÃ—ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆï¼‰")


    # 13. ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹ Ã— ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ
    plt.figure(figsize=(12, 8))
    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['sharpness'], model_data['artifact_total'],
                   label=model, alpha=0.6, s=50)
    plt.xlabel('ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹', fontsize=14, fontweight='bold')
    plt.ylabel('ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆï¼ˆãƒ–ãƒ­ãƒƒã‚¯+ãƒªãƒ³ã‚®ãƒ³ã‚°ï¼‰', fontsize=14, fontweight='bold')
    plt.title('å“è³ªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼šé®®æ˜åŒ– vs æ­ªã¿', fontsize=16, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'tradeoff_sharpness_vs_artifact.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•â‘¢ï¼ˆã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹Ã—ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆï¼‰")


    # 14. LPIPS Ã— MS-SSIM
    plt.figure(figsize=(12, 8))
    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['lpips'], model_data['ms_ssim'],
                   label=model, alpha=0.6, s=50)
    plt.xlabel('LPIPSï¼ˆçŸ¥è¦šçš„é¡ä¼¼åº¦ï¼‰', fontsize=14, fontweight='bold')
    plt.ylabel('MS-SSIM', fontsize=14, fontweight='bold')
    plt.title('çŸ¥è¦š vs æ§‹é€ ï¼šLPIPS vs MS-SSIM\nè² ã®ç›¸é–¢ãŒæœŸå¾…ã•ã‚Œã‚‹', fontsize=16, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'tradeoff_lpips_vs_msssim.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•â‘£ï¼ˆLPIPSÃ—MS-SSIMï¼‰")


    # 15. ãƒ†ã‚¯ã‚¹ãƒãƒ£ Ã— é«˜å‘¨æ³¢æˆåˆ†
    plt.figure(figsize=(12, 8))
    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['texture_complexity'], model_data['high_freq_ratio'],
                   label=model, alpha=0.6, s=50)
    plt.xlabel('ãƒ†ã‚¯ã‚¹ãƒãƒ£è¤‡é›‘åº¦', fontsize=14, fontweight='bold')
    plt.ylabel('é«˜å‘¨æ³¢æˆåˆ†æ¯”ç‡', fontsize=14, fontweight='bold')
    plt.title('ãƒ†ã‚¯ã‚¹ãƒãƒ£ vs å‘¨æ³¢æ•°æˆåˆ†ã®ä¸€è²«æ€§', fontsize=16, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'tradeoff_texture_vs_highfreq.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•â‘¤ï¼ˆãƒ†ã‚¯ã‚¹ãƒãƒ£Ã—é«˜å‘¨æ³¢ï¼‰")


    # ===== åŒ»ç™‚ç”»åƒç‰¹åŒ–ç³»ãƒ—ãƒ­ãƒƒãƒˆ =====

    # 16. ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ Ã— ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ç›¸é–¢
    plt.figure(figsize=(12, 8))
    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['contrast'], model_data['histogram_corr'],
                   label=model, alpha=0.6, s=50)
    plt.xlabel('ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ', fontsize=14, fontweight='bold')
    plt.ylabel('ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ç›¸é–¢', fontsize=14, fontweight='bold')
    plt.title('åŒ»ç™‚ç”»åƒå“è³ªï¼šã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå¼·èª¿ãŒæ¿ƒåº¦åˆ†å¸ƒã‚’å´©ã—ã¦ã„ãªã„ã‹', fontsize=16, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'medical_contrast_vs_histogram.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… åŒ»ç™‚ç‰¹åŒ–â‘ ï¼ˆã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆÃ—ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼‰")


    # 17. ã‚¨ãƒƒã‚¸å¯†åº¦ Ã— å±€æ‰€å“è³ªå¹³å‡
    plt.figure(figsize=(12, 8))
    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['edge_density'], model_data['local_quality_mean'],
                   label=model, alpha=0.6, s=50)
    plt.xlabel('ã‚¨ãƒƒã‚¸å¯†åº¦', fontsize=14, fontweight='bold')
    plt.ylabel('å±€æ‰€å“è³ªå¹³å‡', fontsize=14, fontweight='bold')
    plt.title('åŒ»ç™‚ç”»åƒå“è³ªï¼šã‚¨ãƒƒã‚¸ä¿æŒã¨å±€æ‰€å“è³ªã®é–¢ä¿‚', fontsize=16, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'medical_edge_vs_local_quality.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… åŒ»ç™‚ç‰¹åŒ–â‘¡ï¼ˆã‚¨ãƒƒã‚¸Ã—å±€æ‰€å“è³ªï¼‰")


    # 18. ãƒã‚¤ã‚º Ã— å±€æ‰€å“è³ªæ¨™æº–åå·®
    plt.figure(figsize=(12, 8))
    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['noise'], model_data['local_quality_std'],
                   label=model, alpha=0.6, s=50)
    plt.xlabel('ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«', fontsize=14, fontweight='bold')
    plt.ylabel('å±€æ‰€å“è³ªæ¨™æº–åå·®', fontsize=14, fontweight='bold')
    plt.title('åŒ»ç™‚ç”»åƒå“è³ªï¼šãƒã‚¤ã‚ºã®å±€æ‰€çš„ååœ¨æ€§', fontsize=16, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'medical_noise_vs_local_std.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… åŒ»ç™‚ç‰¹åŒ–â‘¢ï¼ˆãƒã‚¤ã‚ºÃ—å±€æ‰€å“è³ªSDï¼‰")


    # 19. è‰²å·®Î”E Ã— LABæ˜åº¦
    plt.figure(figsize=(12, 8))
    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['delta_e'], model_data['lab_L_mean'],
                   label=model, alpha=0.6, s=50)
    plt.xlabel('è‰²å·® Î”E', fontsize=14, fontweight='bold')
    plt.ylabel('LABæ˜åº¦', fontsize=14, fontweight='bold')
    plt.title('åŒ»ç™‚ç”»åƒå“è³ªï¼šè‰²å¤‰åŒ–ã¨æ˜åº¦ã®é–¢ä¿‚ï¼ˆç—…ç†ç”»åƒã§é‡è¦ï¼‰', fontsize=16, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'medical_deltae_vs_lab.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… åŒ»ç™‚ç‰¹åŒ–â‘£ï¼ˆè‰²å·®Ã—LABæ˜åº¦ï¼‰")


    # ===== åˆ†å¸ƒãƒ»PCAç³»ãƒ—ãƒ­ãƒƒãƒˆ =====

    # 20. ç·åˆã‚¹ã‚³ã‚¢ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼ˆãƒ¢ãƒ‡ãƒ«åˆ¥ï¼‰
    plt.figure(figsize=(12, 6))
    for model in df['model'].unique():
        model_data = df[df['model'] == model]['total_score']
        plt.hist(model_data, bins=20, alpha=0.5, label=model, edgecolor='black')
    plt.xlabel('ç·åˆã‚¹ã‚³ã‚¢', fontsize=14, fontweight='bold')
    plt.ylabel('é »åº¦', fontsize=14, fontweight='bold')
    plt.title('ç·åˆã‚¹ã‚³ã‚¢åˆ†å¸ƒï¼ˆãƒ¢ãƒ‡ãƒ«åˆ¥ï¼‰', fontsize=16, fontweight='bold')
    plt.legend(fontsize=12)
    plt.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'distribution_total_score_histogram.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… åˆ†å¸ƒâ‘ ï¼ˆç·åˆã‚¹ã‚³ã‚¢ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼‰")


    # 21. ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ãƒ—ãƒ­ãƒƒãƒˆ
    from sklearn.decomposition import PCA
    from sklearn.preprocessing import StandardScaler

    # 17é …ç›®ã‚’æ¨™æº–åŒ–
    metrics_for_pca = ['ssim', 'ms_ssim', 'psnr', 'lpips', 'sharpness', 'contrast',
                       'entropy', 'noise', 'edge_density', 'artifact_total', 'delta_e',
                       'high_freq_ratio', 'texture_complexity', 'local_quality_mean',
                       'histogram_corr', 'lab_L_mean', 'total_score']

    X = df[metrics_for_pca].fillna(0)
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # PCAå®Ÿè¡Œ
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X_scaled)

    plt.figure(figsize=(12, 8))
    for model in df['model'].unique():
        mask = df['model'] == model
        plt.scatter(X_pca[mask, 0], X_pca[mask, 1],
                   label=model, alpha=0.6, s=50)

    plt.xlabel(f'ç¬¬1ä¸»æˆåˆ† ({pca.explained_variance_ratio_[0]*100:.1f}%)', fontsize=14, fontweight='bold')
    plt.ylabel(f'ç¬¬2ä¸»æˆåˆ† ({pca.explained_variance_ratio_[1]*100:.1f}%)', fontsize=14, fontweight='bold')
    plt.title(f'ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ï¼š17é …ç›®ã‚’2æ¬¡å…ƒã«åœ§ç¸®\nç´¯ç©å¯„ä¸ç‡: {sum(pca.explained_variance_ratio_)*100:.1f}%',
              fontsize=16, fontweight='bold')
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'pca_2d_projection.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… åˆ†å¸ƒâ‘¡ï¼ˆPCA 2æ¬¡å…ƒãƒ—ãƒ­ãƒƒãƒˆï¼‰")


    # 22. ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ—ãƒ­ãƒƒãƒˆï¼ˆä¸»è¦æŒ‡æ¨™ï¼‰
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    fig.suptitle('ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ãƒãƒ³ãƒ‰ï¼ˆ25%-75%ï¼‰ãƒ—ãƒ­ãƒƒãƒˆ', fontsize=18, fontweight='bold')

    key_metrics = ['ssim', 'psnr', 'sharpness', 'noise', 'edge_density', 'total_score']
    metric_names = ['SSIM', 'PSNR [dB]', 'ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹', 'ãƒã‚¤ã‚º', 'ã‚¨ãƒƒã‚¸å¯†åº¦', 'ç·åˆã‚¹ã‚³ã‚¢']

    for idx, (metric, name) in enumerate(zip(key_metrics, metric_names)):
        ax = axes[idx // 3, idx % 3]

        models = df['model'].unique()
        positions = range(len(models))

        for i, model in enumerate(models):
            model_data = df[df['model'] == model][metric]
            q25 = model_data.quantile(0.25)
            q50 = model_data.quantile(0.50)
            q75 = model_data.quantile(0.75)

            ax.plot([i, i], [q25, q75], 'b-', linewidth=8, alpha=0.3)
            ax.plot(i, q50, 'ro', markersize=10)

        ax.set_xticks(positions)
        ax.set_xticklabels(models, rotation=45, ha='right', fontsize=9)
        ax.set_ylabel(name, fontsize=11, fontweight='bold')
        ax.grid(axis='y', alpha=0.3)
        ax.set_title(f'{name}ã®åˆ†å¸ƒ', fontsize=12)

    plt.tight_layout()
    plt.savefig(output_dir / 'percentile_bands.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… åˆ†å¸ƒâ‘¢ï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ãƒãƒ³ãƒ‰ï¼‰")


    # 23. å¯„ä¸ç‡ã‚°ãƒ©ãƒ•ï¼ˆPCAï¼‰
    pca_full = PCA()
    pca_full.fit(X_scaled)

    plt.figure(figsize=(12, 6))
    cumsum = np.cumsum(pca_full.explained_variance_ratio_)
    plt.plot(range(1, len(cumsum)+1), cumsum, 'bo-', linewidth=2, markersize=8)
    plt.axhline(y=0.95, color='r', linestyle='--', label='95%ãƒ©ã‚¤ãƒ³')
    plt.xlabel('ä¸»æˆåˆ†æ•°', fontsize=14, fontweight='bold')
    plt.ylabel('ç´¯ç©å¯„ä¸ç‡', fontsize=14, fontweight='bold')
    plt.title('PCAç´¯ç©å¯„ä¸ç‡ï¼šä½•æ¬¡å…ƒã§95%èª¬æ˜ã§ãã‚‹ã‹', fontsize=16, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.legend(fontsize=12)
    plt.tight_layout()
    plt.savefig(output_dir / 'pca_cumulative_variance.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… åˆ†å¸ƒâ‘£ï¼ˆPCAå¯„ä¸ç‡ï¼‰")

    print(f"{'='*80}")
    print(f"âœ… å…¨23ç¨®é¡ã®ç ”ç©¶ç”¨ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆå®Œäº†")
    print(f"   è«–æ–‡ãƒ»ç™ºè¡¨è³‡æ–™ã«ãã®ã¾ã¾ä½¿ç”¨ã§ãã¾ã™ï¼ˆ300dpié«˜è§£åƒåº¦ï¼‰\n")


if __name__ == '__main__':
    if len(sys.argv) < 2:
        print(f"\nä½¿ã„æ–¹:")
        print(f"  python analyze_results.py results/batch_analysis.csv\n")
        sys.exit(1)

    csv_file = sys.argv[1]

    if not Path(csv_file).exists():
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_file}")
        sys.exit(1)

    analyze_batch_results(csv_file)
