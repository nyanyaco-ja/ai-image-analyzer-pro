"""
çµ±è¨ˆåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼šãƒãƒƒãƒå‡¦ç†çµæœã‹ã‚‰é–¾å€¤ã‚’æ±ºå®š

ä½¿ã„æ–¹:
python analyze_results.py results/batch_analysis.csv
"""

import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['MS Gothic', 'Yu Gothic', 'Meiryo', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False


def analyze_batch_results(csv_file):
    """
    ãƒãƒƒãƒå‡¦ç†çµæœã®çµ±è¨ˆåˆ†æ
    """

    # CSVèª­ã¿è¾¼ã¿
    df = pd.read_csv(csv_file)

    print(f"\n{'='*80}")
    print(f"ğŸ“Š çµ±è¨ˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
    print(f"{'='*80}")
    print(f"ğŸ“„ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«: {csv_file}")
    print(f"ğŸ“· ç”»åƒæ•°: {df['image_id'].nunique()}")
    print(f"ğŸ¤– ãƒ¢ãƒ‡ãƒ«æ•°: {df['model'].nunique()}")
    print(f"ğŸ“Š ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(df)}")
    print(f"{'='*80}\n")

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_dir = Path('analysis_output')
    output_dir.mkdir(exist_ok=True)

    # 1. åŸºæœ¬çµ±è¨ˆé‡
    print_basic_statistics(df)

    # 2. ãƒ¢ãƒ‡ãƒ«åˆ¥æ¯”è¼ƒ
    compare_models(df, output_dir)

    # 3. ç›¸é–¢åˆ†æ
    analyze_correlations(df, output_dir)

    # 4. é–¾å€¤ææ¡ˆ
    suggest_thresholds(df, output_dir)

    # 5. ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ææ¡ˆ
    suggest_hallucination_logic(df, output_dir)

    print(f"\nâœ… åˆ†æå®Œäº†ï¼")
    print(f"ğŸ“ çµæœä¿å­˜å…ˆ: {output_dir}/")


def print_basic_statistics(df):
    """
    åŸºæœ¬çµ±è¨ˆé‡ã®è¡¨ç¤º
    """

    print(f"\nğŸ“ˆ ä¸»è¦æŒ‡æ¨™ã®åŸºæœ¬çµ±è¨ˆé‡:")
    print(f"{'='*80}")

    # 17é …ç›®ã™ã¹ã¦
    metrics = ['ssim', 'ms_ssim', 'psnr', 'lpips', 'sharpness', 'contrast',
               'entropy', 'noise', 'edge_density', 'artifact_total', 'delta_e',
               'high_freq_ratio', 'texture_complexity', 'local_quality_mean',
               'histogram_corr', 'lab_L_mean', 'total_score']

    stats = df[metrics].describe().T
    stats.columns = ['ä»¶æ•°', 'å¹³å‡', 'æ¨™æº–åå·®', 'æœ€å°', '25%', '50%', '75%', 'æœ€å¤§']

    print(stats.round(4).to_string())
    print(f"{'='*80}\n")


def compare_models(df, output_dir):
    """
    ãƒ¢ãƒ‡ãƒ«åˆ¥æ¯”è¼ƒ
    """

    print(f"\nğŸ† ãƒ¢ãƒ‡ãƒ«åˆ¥ãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
    print(f"{'='*80}")

    # ä¸»è¦æŒ‡æ¨™ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    model_comparison = df.groupby('model').agg({
        'ssim': ['mean', 'std'],
        'psnr': ['mean', 'std'],
        'lpips': ['mean', 'std'],
        'total_score': ['mean', 'std'],
        'noise': ['mean', 'std'],
        'artifact_total': ['mean', 'std'],
        'sharpness': ['mean', 'std'],
        'edge_density': ['mean', 'std']
    }).round(4)

    # ç·åˆã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
    model_comparison = model_comparison.sort_values(('total_score', 'mean'), ascending=False)

    print(model_comparison.to_string())
    print(f"{'='*80}\n")

    # CSVä¿å­˜
    model_comparison.to_csv(output_dir / 'model_comparison.csv', encoding='utf-8-sig')

    # å¯è¦–åŒ–ï¼šãƒ¢ãƒ‡ãƒ«åˆ¥ç·åˆã‚¹ã‚³ã‚¢
    plt.figure(figsize=(12, 6))
    model_scores = df.groupby('model')['total_score'].mean().sort_values(ascending=False)

    plt.bar(range(len(model_scores)), model_scores.values)
    plt.xticks(range(len(model_scores)), model_scores.index, rotation=45, ha='right')
    plt.ylabel('ç·åˆã‚¹ã‚³ã‚¢ï¼ˆå¹³å‡ï¼‰')
    plt.title('ãƒ¢ãƒ‡ãƒ«åˆ¥ç·åˆã‚¹ã‚³ã‚¢æ¯”è¼ƒ')
    plt.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'model_scores.png', dpi=150)
    plt.close()

    print(f"ğŸ“Š ã‚°ãƒ©ãƒ•ä¿å­˜: {output_dir}/model_scores.png")


def analyze_correlations(df, output_dir):
    """
    17é …ç›®é–“ã®ç›¸é–¢åˆ†æ
    """

    print(f"\nğŸ”— ç›¸é–¢åˆ†æ:")
    print(f"{'='*80}")

    # æ•°å€¤åˆ—ã®ã¿æŠ½å‡º
    numeric_cols = ['ssim', 'psnr', 'lpips', 'ms_ssim', 'sharpness', 'contrast',
                    'entropy', 'noise', 'edge_density', 'artifact_total', 'delta_e',
                    'high_freq_ratio', 'texture_complexity', 'local_quality_mean',
                    'histogram_corr', 'lab_L_mean', 'total_score']

    corr_matrix = df[numeric_cols].corr()

    # ç›¸é–¢è¡Œåˆ—ã‚’ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—è¡¨ç¤º
    plt.figure(figsize=(14, 12))
    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,
                square=True, linewidths=0.5, cbar_kws={"shrink": 0.8})
    plt.title('17é …ç›®ã®ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(output_dir / 'correlation_matrix.png', dpi=150)
    plt.close()

    print(f"ğŸ“Š ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ä¿å­˜: {output_dir}/correlation_matrix.png")

    # é«˜ç›¸é–¢ãƒšã‚¢ã‚’è¡¨ç¤º
    print(f"\nğŸ”¥ é«˜ç›¸é–¢ãƒšã‚¢ï¼ˆ|r| > 0.7ï¼‰:")
    high_corr = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            corr_value = corr_matrix.iloc[i, j]
            if abs(corr_value) > 0.7:
                high_corr.append({
                    'metric1': corr_matrix.columns[i],
                    'metric2': corr_matrix.columns[j],
                    'correlation': corr_value
                })

    if high_corr:
        high_corr_df = pd.DataFrame(high_corr).sort_values('correlation', ascending=False)
        print(high_corr_df.to_string(index=False))
    else:
        print("   (ãªã—)")

    print(f"{'='*80}\n")


def suggest_thresholds(df, output_dir):
    """
    æ ¹æ‹ ã®ã‚ã‚‹é–¾å€¤ã‚’ææ¡ˆ
    """

    print(f"\nğŸ’¡ æ¨å¥¨é–¾å€¤ã®ææ¡ˆ:")
    print(f"{'='*80}")

    thresholds = {}

    # å„æŒ‡æ¨™ã®çµ±è¨ˆå€¤ã‹ã‚‰é–¾å€¤ã‚’æ±ºå®š
    # 17é …ç›®ã™ã¹ã¦ã®é–¾å€¤ã‚’ææ¡ˆ
    metrics_config = {
        'ssim': {'direction': 'high', 'percentile': 25, 'name': 'SSIMï¼ˆæ§‹é€ é¡ä¼¼æ€§ï¼‰'},
        'ms_ssim': {'direction': 'high', 'percentile': 25, 'name': 'MS-SSIMï¼ˆãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«SSIMï¼‰'},
        'psnr': {'direction': 'high', 'percentile': 25, 'name': 'PSNRï¼ˆä¿¡å·å¯¾é›‘éŸ³æ¯”ï¼‰'},
        'lpips': {'direction': 'low', 'percentile': 75, 'name': 'LPIPSï¼ˆçŸ¥è¦šçš„é¡ä¼¼åº¦ï¼‰'},
        'sharpness': {'direction': 'high', 'percentile': 25, 'name': 'ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹'},
        'contrast': {'direction': 'high', 'percentile': 25, 'name': 'ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ'},
        'entropy': {'direction': 'high', 'percentile': 25, 'name': 'ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆæƒ…å ±é‡ï¼‰'},
        'noise': {'direction': 'low', 'percentile': 75, 'name': 'ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«'},
        'edge_density': {'direction': 'high', 'percentile': 25, 'name': 'ã‚¨ãƒƒã‚¸å¯†åº¦'},
        'artifact_total': {'direction': 'low', 'percentile': 75, 'name': 'ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ'},
        'delta_e': {'direction': 'low', 'percentile': 75, 'name': 'è‰²å·®ï¼ˆÎ”Eï¼‰'},
        'high_freq_ratio': {'direction': 'high', 'percentile': 25, 'name': 'é«˜å‘¨æ³¢æˆåˆ†æ¯”ç‡'},
        'texture_complexity': {'direction': 'high', 'percentile': 25, 'name': 'ãƒ†ã‚¯ã‚¹ãƒãƒ£è¤‡é›‘åº¦'},
        'local_quality_mean': {'direction': 'high', 'percentile': 25, 'name': 'å±€æ‰€å“è³ªå¹³å‡'},
        'histogram_corr': {'direction': 'high', 'percentile': 25, 'name': 'ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ç›¸é–¢'},
        'lab_L_mean': {'direction': 'neutral', 'percentile': 50, 'name': 'LABæ˜åº¦ï¼ˆå‚è€ƒå€¤ï¼‰'},
        'total_score': {'direction': 'high', 'percentile': 25, 'name': 'ç·åˆã‚¹ã‚³ã‚¢'},
    }

    for metric, config in metrics_config.items():
        data = df[metric].dropna()

        if config['direction'] == 'neutral':
            # ä¸­ç«‹çš„ãªæŒ‡æ¨™ï¼ˆæ˜åº¦ãªã©ï¼‰ï¼šä¸­å¤®å€¤ã‚’å‚è€ƒå€¤ã¨ã—ã¦è¡¨ç¤º
            threshold = np.percentile(data, config['percentile'])
            condition = f"å‚è€ƒå€¤: {threshold:.4f}"
        elif config['direction'] == 'high':
            # é«˜ã„æ–¹ãŒè‰¯ã„æŒ‡æ¨™ï¼š25ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ä»¥ä¸Šã‚’æ¨å¥¨
            threshold = np.percentile(data, config['percentile'])
            condition = f">= {threshold:.4f}"
        else:
            # ä½ã„æ–¹ãŒè‰¯ã„æŒ‡æ¨™ï¼š75ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ä»¥ä¸‹ã‚’æ¨å¥¨
            threshold = np.percentile(data, config['percentile'])
            condition = f"<= {threshold:.4f}"

        thresholds[metric] = {
            'name': config['name'],
            'threshold': threshold,
            'condition': condition,
            'mean': data.mean(),
            'std': data.std(),
            'min': data.min(),
            'max': data.max()
        }

        print(f"{config['name']:30s}: {condition:20s} (å¹³å‡: {data.mean():.4f}, æ¨™æº–åå·®: {data.std():.4f})")

    print(f"{'='*80}")
    print(f"ğŸ’¡ è§£é‡ˆ:")
    print(f"   - ã“ã‚Œã‚‰ã®é–¾å€¤ã¯ã€å…¨ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆåˆ†å¸ƒã«åŸºã¥ã„ã¦ã„ã¾ã™")
    print(f"   - 25ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ« = ä¸Šä½75%ã®å“è³ªã‚’ã€Œåˆæ ¼ã€ã¨ã™ã‚‹åŸºæº–")
    print(f"   - 75ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ« = ä¸‹ä½75%ã®å“è³ªã‚’ã€Œåˆæ ¼ã€ã¨ã™ã‚‹åŸºæº–")
    print(f"{'='*80}\n")

    # JSONä¿å­˜
    import json
    with open(output_dir / 'recommended_thresholds.json', 'w', encoding='utf-8') as f:
        json.dump(thresholds, f, indent=2, ensure_ascii=False)

    print(f"ğŸ’¾ é–¾å€¤ä¿å­˜: {output_dir}/recommended_thresholds.json\n")


def suggest_hallucination_logic(df, output_dir):
    """
    ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ã®ææ¡ˆ
    """

    print(f"\nğŸ” ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ã®ææ¡ˆ:")
    print(f"{'='*80}")

    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: SSIMé«˜ã„ã®ã«PSNRä½ã„ï¼ˆæ§‹é€ ã¯ä¼¼ã¦ã‚‹ãŒãƒ”ã‚¯ã‚»ãƒ«å€¤ãŒé•ã†ï¼‰
    ssim_high = df['ssim'].quantile(0.75)
    psnr_low = df['psnr'].quantile(0.25)

    pattern1 = df[(df['ssim'] >= ssim_high) & (df['psnr'] <= psnr_low)]
    pattern1_rate = len(pattern1) / len(df) * 100

    print(f"ã€ãƒ‘ã‚¿ãƒ¼ãƒ³1ã€‘SSIMé«˜ & PSNRä½ (æ§‹é€ é¡ä¼¼ã ãŒãƒ”ã‚¯ã‚»ãƒ«å€¤ç›¸é•)")
    print(f"   æ¡ä»¶: SSIM >= {ssim_high:.4f} AND PSNR <= {psnr_low:.2f}")
    print(f"   è©²å½“ç‡: {pattern1_rate:.1f}% ({len(pattern1)}/{len(df)}ä»¶)")
    print(f"   ãƒªã‚¹ã‚¯: ä¸­ï½é«˜ï¼ˆAIãŒæ§‹é€ ã‚’æ¨¡å€£ã—ãŸå¯èƒ½æ€§ï¼‰")

    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹é«˜ã„ãŒãƒã‚¤ã‚ºã‚‚é«˜ã„ï¼ˆéå‰°å‡¦ç†ï¼‰
    sharp_high = df['sharpness'].quantile(0.75)
    noise_high = df['noise'].quantile(0.75)

    pattern2 = df[(df['sharpness'] >= sharp_high) & (df['noise'] >= noise_high)]
    pattern2_rate = len(pattern2) / len(df) * 100

    print(f"\nã€ãƒ‘ã‚¿ãƒ¼ãƒ³2ã€‘ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹é«˜ & ãƒã‚¤ã‚ºé«˜ (éå‰°å‡¦ç†)")
    print(f"   æ¡ä»¶: ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹ >= {sharp_high:.2f} AND ãƒã‚¤ã‚º >= {noise_high:.2f}")
    print(f"   è©²å½“ç‡: {pattern2_rate:.1f}% ({len(pattern2)}/{len(df)}ä»¶)")
    print(f"   ãƒªã‚¹ã‚¯: ä¸­ï¼ˆéåº¦ãªã‚·ãƒ£ãƒ¼ãƒ—åŒ–ã«ã‚ˆã‚‹ãƒã‚¤ã‚ºå¢—å¹…ï¼‰")

    # ãƒ‘ã‚¿ãƒ¼ãƒ³3: ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆé«˜ï¼ˆGANç‰¹æœ‰ï¼‰
    artifact_high = df['artifact_total'].quantile(0.90)

    pattern3 = df[df['artifact_total'] >= artifact_high]
    pattern3_rate = len(pattern3) / len(df) * 100

    print(f"\nã€ãƒ‘ã‚¿ãƒ¼ãƒ³3ã€‘ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆé«˜ (GANç‰¹æœ‰ã®æ­ªã¿)")
    print(f"   æ¡ä»¶: ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ >= {artifact_high:.2f}")
    print(f"   è©²å½“ç‡: {pattern3_rate:.1f}% ({len(pattern3)}/{len(df)}ä»¶)")
    print(f"   ãƒªã‚¹ã‚¯: é«˜ï¼ˆãƒªãƒ³ã‚®ãƒ³ã‚°ãƒ»ãƒ–ãƒ­ãƒƒã‚¯ãƒã‚¤ã‚ºã«ã‚ˆã‚‹è¨ºæ–­é˜»å®³ï¼‰")

    # ãƒ‘ã‚¿ãƒ¼ãƒ³4: å±€æ‰€å“è³ªã®ã°ã‚‰ã¤ãå¤§
    local_std_high = df['local_quality_std'].quantile(0.75)

    pattern4 = df[df['local_quality_std'] >= local_std_high]
    pattern4_rate = len(pattern4) / len(df) * 100

    print(f"\nã€ãƒ‘ã‚¿ãƒ¼ãƒ³4ã€‘å±€æ‰€å“è³ªã®ã°ã‚‰ã¤ãå¤§ (ä¸å‡ä¸€ãªå‡¦ç†)")
    print(f"   æ¡ä»¶: å±€æ‰€SSIMæ¨™æº–åå·® >= {local_std_high:.4f}")
    print(f"   è©²å½“ç‡: {pattern4_rate:.1f}% ({len(pattern4)}/{len(df)}ä»¶)")
    print(f"   ãƒªã‚¹ã‚¯: ä¸­ï½é«˜ï¼ˆé ˜åŸŸã«ã‚ˆã£ã¦å“è³ªãŒç•°ãªã‚‹ = ä¸€éƒ¨ã«ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼‰")

    print(f"{'='*80}\n")

    # ç·åˆãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
    print(f"ğŸ“Š ç·åˆãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ã®ææ¡ˆ:")
    print(f"{'='*80}")

    df['hallucination_risk_score'] = 0

    # ãƒ‘ã‚¿ãƒ¼ãƒ³1è©²å½“: +25ç‚¹
    df.loc[(df['ssim'] >= ssim_high) & (df['psnr'] <= psnr_low), 'hallucination_risk_score'] += 25

    # ãƒ‘ã‚¿ãƒ¼ãƒ³2è©²å½“: +20ç‚¹
    df.loc[(df['sharpness'] >= sharp_high) & (df['noise'] >= noise_high), 'hallucination_risk_score'] += 20

    # ãƒ‘ã‚¿ãƒ¼ãƒ³3è©²å½“: +30ç‚¹
    df.loc[df['artifact_total'] >= artifact_high, 'hallucination_risk_score'] += 30

    # ãƒ‘ã‚¿ãƒ¼ãƒ³4è©²å½“: +25ç‚¹
    df.loc[df['local_quality_std'] >= local_std_high, 'hallucination_risk_score'] += 25

    # ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«åˆ†é¡
    df['risk_level'] = pd.cut(df['hallucination_risk_score'],
                               bins=[0, 10, 30, 50, 100],
                               labels=['MINIMAL', 'LOW', 'MEDIUM', 'HIGH'])

    # ãƒªã‚¹ã‚¯åˆ†å¸ƒ
    risk_dist = df['risk_level'].value_counts().sort_index()
    print(f"\nãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒªã‚¹ã‚¯åˆ†å¸ƒ:")
    for level, count in risk_dist.items():
        pct = count / len(df) * 100
        print(f"   {level:10s}: {count:4d}ä»¶ ({pct:5.1f}%)")

    # ãƒªã‚¹ã‚¯ä»˜ãCSVä¿å­˜
    output_csv = output_dir / 'results_with_risk_score.csv'
    df.to_csv(output_csv, index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ä»˜ãçµæœä¿å­˜: {output_csv}")

    print(f"{'='*80}\n")


if __name__ == '__main__':
    if len(sys.argv) < 2:
        print(f"\nä½¿ã„æ–¹:")
        print(f"  python analyze_results.py results/batch_analysis.csv\n")
        sys.exit(1)

    csv_file = sys.argv[1]

    if not Path(csv_file).exists():
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_file}")
        sys.exit(1)

    analyze_batch_results(csv_file)
