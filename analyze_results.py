"""
çµ±è¨ˆåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼šãƒãƒƒãƒå‡¦ç†çµæœã‹ã‚‰é–¾å€¤ã‚’æ±ºå®š

ä½¿ã„æ–¹:
python analyze_results.py results/batch_analysis.csv
"""

import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['MS Gothic', 'Yu Gothic', 'Meiryo', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False


def analyze_batch_results(csv_file):
    """
    ãƒãƒƒãƒå‡¦ç†çµæœã®çµ±è¨ˆåˆ†æ
    """

    # CSVèª­ã¿è¾¼ã¿
    df = pd.read_csv(csv_file)

    print(f"\n{'='*80}")
    print(f"ğŸ“Š çµ±è¨ˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
    print(f"{'='*80}")
    print(f"ğŸ“„ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«: {csv_file}")
    print(f"ğŸ“· ç”»åƒæ•°: {df['image_id'].nunique()}")
    print(f"ğŸ¤– ãƒ¢ãƒ‡ãƒ«æ•°: {df['model'].nunique()}")
    print(f"ğŸ“Š ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(df)}")
    print(f"{'='*80}\n")

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_dir = Path('analysis_output')
    output_dir.mkdir(exist_ok=True)

    # 1. åŸºæœ¬çµ±è¨ˆé‡
    print_basic_statistics(df)

    # 2. ãƒ¢ãƒ‡ãƒ«åˆ¥æ¯”è¼ƒ
    compare_models(df, output_dir)

    # 3. ç›¸é–¢åˆ†æ
    analyze_correlations(df, output_dir)

    # 4. é–¾å€¤ææ¡ˆ
    suggest_thresholds(df, output_dir)

    # 5. ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ææ¡ˆ
    suggest_hallucination_logic(df, output_dir)

    # 6. ç ”ç©¶ç”¨ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆ
    generate_research_plots(df, output_dir)

    print(f"\nâœ… åˆ†æå®Œäº†ï¼")
    print(f"ğŸ“ çµæœä¿å­˜å…ˆ: {output_dir}/")


def print_basic_statistics(df):
    """
    åŸºæœ¬çµ±è¨ˆé‡ã®è¡¨ç¤º
    """

    print(f"\nğŸ“ˆ ä¸»è¦æŒ‡æ¨™ã®åŸºæœ¬çµ±è¨ˆé‡:")
    print(f"{'='*80}")

    # 17é …ç›®ã™ã¹ã¦
    metrics = ['ssim', 'ms_ssim', 'psnr', 'lpips', 'sharpness', 'contrast',
               'entropy', 'noise', 'edge_density', 'artifact_total', 'delta_e',
               'high_freq_ratio', 'texture_complexity', 'local_quality_mean',
               'histogram_corr', 'lab_L_mean', 'total_score']

    stats = df[metrics].describe().T
    stats.columns = ['ä»¶æ•°', 'å¹³å‡', 'æ¨™æº–åå·®', 'æœ€å°', '25%', '50%', '75%', 'æœ€å¤§']

    print(stats.round(4).to_string())
    print(f"{'='*80}\n")


def compare_models(df, output_dir):
    """
    ãƒ¢ãƒ‡ãƒ«åˆ¥æ¯”è¼ƒ
    """

    print(f"\nğŸ† ãƒ¢ãƒ‡ãƒ«åˆ¥ãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
    print(f"{'='*80}")

    # ä¸»è¦æŒ‡æ¨™ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    model_comparison = df.groupby('model').agg({
        'ssim': ['mean', 'std'],
        'psnr': ['mean', 'std'],
        'lpips': ['mean', 'std'],
        'total_score': ['mean', 'std'],
        'noise': ['mean', 'std'],
        'artifact_total': ['mean', 'std'],
        'sharpness': ['mean', 'std'],
        'edge_density': ['mean', 'std']
    }).round(4)

    # ç·åˆã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
    model_comparison = model_comparison.sort_values(('total_score', 'mean'), ascending=False)

    print(model_comparison.to_string())
    print(f"{'='*80}\n")

    # CSVä¿å­˜
    model_comparison.to_csv(output_dir / 'model_comparison.csv', encoding='utf-8-sig')

    # å¯è¦–åŒ–ï¼šãƒ¢ãƒ‡ãƒ«åˆ¥ç·åˆã‚¹ã‚³ã‚¢
    plt.figure(figsize=(12, 6))
    model_scores = df.groupby('model')['total_score'].mean().sort_values(ascending=False)

    plt.bar(range(len(model_scores)), model_scores.values)
    plt.xticks(range(len(model_scores)), model_scores.index, rotation=45, ha='right')
    plt.ylabel('ç·åˆã‚¹ã‚³ã‚¢ï¼ˆå¹³å‡ï¼‰')
    plt.title('ãƒ¢ãƒ‡ãƒ«åˆ¥ç·åˆã‚¹ã‚³ã‚¢æ¯”è¼ƒ')
    plt.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'model_scores.png', dpi=150)
    plt.close()

    print(f"ğŸ“Š ã‚°ãƒ©ãƒ•ä¿å­˜: {output_dir}/model_scores.png")


def analyze_correlations(df, output_dir):
    """
    17é …ç›®é–“ã®ç›¸é–¢åˆ†æ
    """

    print(f"\nğŸ”— ç›¸é–¢åˆ†æ:")
    print(f"{'='*80}")

    # æ•°å€¤åˆ—ã®ã¿æŠ½å‡º
    numeric_cols = ['ssim', 'psnr', 'lpips', 'ms_ssim', 'sharpness', 'contrast',
                    'entropy', 'noise', 'edge_density', 'artifact_total', 'delta_e',
                    'high_freq_ratio', 'texture_complexity', 'local_quality_mean',
                    'histogram_corr', 'lab_L_mean', 'total_score']

    corr_matrix = df[numeric_cols].corr()

    # ç›¸é–¢è¡Œåˆ—ã‚’ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—è¡¨ç¤º
    plt.figure(figsize=(14, 12))
    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,
                square=True, linewidths=0.5, cbar_kws={"shrink": 0.8})
    plt.title('17é …ç›®ã®ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(output_dir / 'correlation_matrix.png', dpi=150)
    plt.close()

    print(f"ğŸ“Š ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ä¿å­˜: {output_dir}/correlation_matrix.png")

    # é«˜ç›¸é–¢ãƒšã‚¢ã‚’è¡¨ç¤º
    print(f"\nğŸ”¥ é«˜ç›¸é–¢ãƒšã‚¢ï¼ˆ|r| > 0.7ï¼‰:")
    high_corr = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            corr_value = corr_matrix.iloc[i, j]
            if abs(corr_value) > 0.7:
                high_corr.append({
                    'metric1': corr_matrix.columns[i],
                    'metric2': corr_matrix.columns[j],
                    'correlation': corr_value
                })

    if high_corr:
        high_corr_df = pd.DataFrame(high_corr).sort_values('correlation', ascending=False)
        print(high_corr_df.to_string(index=False))
    else:
        print("   (ãªã—)")

    print(f"{'='*80}\n")


def suggest_thresholds(df, output_dir):
    """
    æ ¹æ‹ ã®ã‚ã‚‹é–¾å€¤ã‚’ææ¡ˆ
    """

    print(f"\nğŸ’¡ æ¨å¥¨é–¾å€¤ã®ææ¡ˆ:")
    print(f"{'='*80}")

    thresholds = {}

    # å„æŒ‡æ¨™ã®çµ±è¨ˆå€¤ã‹ã‚‰é–¾å€¤ã‚’æ±ºå®š
    # 17é …ç›®ã™ã¹ã¦ã®é–¾å€¤ã‚’ææ¡ˆ
    metrics_config = {
        'ssim': {'direction': 'high', 'percentile': 25, 'name': 'SSIMï¼ˆæ§‹é€ é¡ä¼¼æ€§ï¼‰'},
        'ms_ssim': {'direction': 'high', 'percentile': 25, 'name': 'MS-SSIMï¼ˆãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«SSIMï¼‰'},
        'psnr': {'direction': 'high', 'percentile': 25, 'name': 'PSNRï¼ˆä¿¡å·å¯¾é›‘éŸ³æ¯”ï¼‰'},
        'lpips': {'direction': 'low', 'percentile': 75, 'name': 'LPIPSï¼ˆçŸ¥è¦šçš„é¡ä¼¼åº¦ï¼‰'},
        'sharpness': {'direction': 'high', 'percentile': 25, 'name': 'ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹'},
        'contrast': {'direction': 'high', 'percentile': 25, 'name': 'ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ'},
        'entropy': {'direction': 'high', 'percentile': 25, 'name': 'ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆæƒ…å ±é‡ï¼‰'},
        'noise': {'direction': 'low', 'percentile': 75, 'name': 'ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«'},
        'edge_density': {'direction': 'high', 'percentile': 25, 'name': 'ã‚¨ãƒƒã‚¸å¯†åº¦'},
        'artifact_total': {'direction': 'low', 'percentile': 75, 'name': 'ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ'},
        'delta_e': {'direction': 'low', 'percentile': 75, 'name': 'è‰²å·®ï¼ˆÎ”Eï¼‰'},
        'high_freq_ratio': {'direction': 'high', 'percentile': 25, 'name': 'é«˜å‘¨æ³¢æˆåˆ†æ¯”ç‡'},
        'texture_complexity': {'direction': 'high', 'percentile': 25, 'name': 'ãƒ†ã‚¯ã‚¹ãƒãƒ£è¤‡é›‘åº¦'},
        'local_quality_mean': {'direction': 'high', 'percentile': 25, 'name': 'å±€æ‰€å“è³ªå¹³å‡'},
        'histogram_corr': {'direction': 'high', 'percentile': 25, 'name': 'ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ç›¸é–¢'},
        'lab_L_mean': {'direction': 'neutral', 'percentile': 50, 'name': 'LABæ˜åº¦ï¼ˆå‚è€ƒå€¤ï¼‰'},
        'total_score': {'direction': 'high', 'percentile': 25, 'name': 'ç·åˆã‚¹ã‚³ã‚¢'},
    }

    for metric, config in metrics_config.items():
        data = df[metric].dropna()

        if config['direction'] == 'neutral':
            # ä¸­ç«‹çš„ãªæŒ‡æ¨™ï¼ˆæ˜åº¦ãªã©ï¼‰ï¼šä¸­å¤®å€¤ã‚’å‚è€ƒå€¤ã¨ã—ã¦è¡¨ç¤º
            threshold = np.percentile(data, config['percentile'])
            condition = f"å‚è€ƒå€¤: {threshold:.4f}"
        elif config['direction'] == 'high':
            # é«˜ã„æ–¹ãŒè‰¯ã„æŒ‡æ¨™ï¼š25ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ä»¥ä¸Šã‚’æ¨å¥¨
            threshold = np.percentile(data, config['percentile'])
            condition = f">= {threshold:.4f}"
        else:
            # ä½ã„æ–¹ãŒè‰¯ã„æŒ‡æ¨™ï¼š75ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ä»¥ä¸‹ã‚’æ¨å¥¨
            threshold = np.percentile(data, config['percentile'])
            condition = f"<= {threshold:.4f}"

        thresholds[metric] = {
            'name': config['name'],
            'threshold': threshold,
            'condition': condition,
            'mean': data.mean(),
            'std': data.std(),
            'min': data.min(),
            'max': data.max()
        }

        print(f"{config['name']:30s}: {condition:20s} (å¹³å‡: {data.mean():.4f}, æ¨™æº–åå·®: {data.std():.4f})")

    print(f"{'='*80}")
    print(f"ğŸ’¡ è§£é‡ˆ:")
    print(f"   - ã“ã‚Œã‚‰ã®é–¾å€¤ã¯ã€å…¨ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆåˆ†å¸ƒã«åŸºã¥ã„ã¦ã„ã¾ã™")
    print(f"   - 25ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ« = ä¸Šä½75%ã®å“è³ªã‚’ã€Œåˆæ ¼ã€ã¨ã™ã‚‹åŸºæº–")
    print(f"   - 75ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ« = ä¸‹ä½75%ã®å“è³ªã‚’ã€Œåˆæ ¼ã€ã¨ã™ã‚‹åŸºæº–")
    print(f"{'='*80}\n")

    # JSONä¿å­˜
    import json
    with open(output_dir / 'recommended_thresholds.json', 'w', encoding='utf-8') as f:
        json.dump(thresholds, f, indent=2, ensure_ascii=False)

    print(f"ğŸ’¾ é–¾å€¤ä¿å­˜: {output_dir}/recommended_thresholds.json\n")


def suggest_hallucination_logic(df, output_dir):
    """
    ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ã®ææ¡ˆ
    """

    print(f"\nğŸ” ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ã®ææ¡ˆ:")
    print(f"{'='*80}")

    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: SSIMé«˜ã„ã®ã«PSNRä½ã„ï¼ˆæ§‹é€ ã¯ä¼¼ã¦ã‚‹ãŒãƒ”ã‚¯ã‚»ãƒ«å€¤ãŒé•ã†ï¼‰
    ssim_high = df['ssim'].quantile(0.75)
    psnr_low = df['psnr'].quantile(0.25)

    pattern1 = df[(df['ssim'] >= ssim_high) & (df['psnr'] <= psnr_low)]
    pattern1_rate = len(pattern1) / len(df) * 100

    print(f"ã€ãƒ‘ã‚¿ãƒ¼ãƒ³1ã€‘SSIMé«˜ & PSNRä½ (æ§‹é€ é¡ä¼¼ã ãŒãƒ”ã‚¯ã‚»ãƒ«å€¤ç›¸é•)")
    print(f"   æ¡ä»¶: SSIM >= {ssim_high:.4f} AND PSNR <= {psnr_low:.2f}")
    print(f"   è©²å½“ç‡: {pattern1_rate:.1f}% ({len(pattern1)}/{len(df)}ä»¶)")
    print(f"   ãƒªã‚¹ã‚¯: ä¸­ï½é«˜ï¼ˆAIãŒæ§‹é€ ã‚’æ¨¡å€£ã—ãŸå¯èƒ½æ€§ï¼‰")

    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹é«˜ã„ãŒãƒã‚¤ã‚ºã‚‚é«˜ã„ï¼ˆéå‰°å‡¦ç†ï¼‰
    sharp_high = df['sharpness'].quantile(0.75)
    noise_high = df['noise'].quantile(0.75)

    pattern2 = df[(df['sharpness'] >= sharp_high) & (df['noise'] >= noise_high)]
    pattern2_rate = len(pattern2) / len(df) * 100

    print(f"\nã€ãƒ‘ã‚¿ãƒ¼ãƒ³2ã€‘ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹é«˜ & ãƒã‚¤ã‚ºé«˜ (éå‰°å‡¦ç†)")
    print(f"   æ¡ä»¶: ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹ >= {sharp_high:.2f} AND ãƒã‚¤ã‚º >= {noise_high:.2f}")
    print(f"   è©²å½“ç‡: {pattern2_rate:.1f}% ({len(pattern2)}/{len(df)}ä»¶)")
    print(f"   ãƒªã‚¹ã‚¯: ä¸­ï¼ˆéåº¦ãªã‚·ãƒ£ãƒ¼ãƒ—åŒ–ã«ã‚ˆã‚‹ãƒã‚¤ã‚ºå¢—å¹…ï¼‰")

    # ãƒ‘ã‚¿ãƒ¼ãƒ³3: ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆé«˜ï¼ˆGANç‰¹æœ‰ï¼‰
    artifact_high = df['artifact_total'].quantile(0.90)

    pattern3 = df[df['artifact_total'] >= artifact_high]
    pattern3_rate = len(pattern3) / len(df) * 100

    print(f"\nã€ãƒ‘ã‚¿ãƒ¼ãƒ³3ã€‘ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆé«˜ (GANç‰¹æœ‰ã®æ­ªã¿)")
    print(f"   æ¡ä»¶: ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ >= {artifact_high:.2f}")
    print(f"   è©²å½“ç‡: {pattern3_rate:.1f}% ({len(pattern3)}/{len(df)}ä»¶)")
    print(f"   ãƒªã‚¹ã‚¯: é«˜ï¼ˆãƒªãƒ³ã‚®ãƒ³ã‚°ãƒ»ãƒ–ãƒ­ãƒƒã‚¯ãƒã‚¤ã‚ºã«ã‚ˆã‚‹è¨ºæ–­é˜»å®³ï¼‰")

    # ãƒ‘ã‚¿ãƒ¼ãƒ³4: å±€æ‰€å“è³ªã®ã°ã‚‰ã¤ãå¤§
    local_std_high = df['local_quality_std'].quantile(0.75)

    pattern4 = df[df['local_quality_std'] >= local_std_high]
    pattern4_rate = len(pattern4) / len(df) * 100

    print(f"\nã€ãƒ‘ã‚¿ãƒ¼ãƒ³4ã€‘å±€æ‰€å“è³ªã®ã°ã‚‰ã¤ãå¤§ (ä¸å‡ä¸€ãªå‡¦ç†)")
    print(f"   æ¡ä»¶: å±€æ‰€SSIMæ¨™æº–åå·® >= {local_std_high:.4f}")
    print(f"   è©²å½“ç‡: {pattern4_rate:.1f}% ({len(pattern4)}/{len(df)}ä»¶)")
    print(f"   ãƒªã‚¹ã‚¯: ä¸­ï½é«˜ï¼ˆé ˜åŸŸã«ã‚ˆã£ã¦å“è³ªãŒç•°ãªã‚‹ = ä¸€éƒ¨ã«ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼‰")

    print(f"{'='*80}\n")

    # ç·åˆãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
    print(f"ğŸ“Š ç·åˆãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ã®ææ¡ˆ:")
    print(f"{'='*80}")

    df['hallucination_risk_score'] = 0

    # ãƒ‘ã‚¿ãƒ¼ãƒ³1è©²å½“: +25ç‚¹
    df.loc[(df['ssim'] >= ssim_high) & (df['psnr'] <= psnr_low), 'hallucination_risk_score'] += 25

    # ãƒ‘ã‚¿ãƒ¼ãƒ³2è©²å½“: +20ç‚¹
    df.loc[(df['sharpness'] >= sharp_high) & (df['noise'] >= noise_high), 'hallucination_risk_score'] += 20

    # ãƒ‘ã‚¿ãƒ¼ãƒ³3è©²å½“: +30ç‚¹
    df.loc[df['artifact_total'] >= artifact_high, 'hallucination_risk_score'] += 30

    # ãƒ‘ã‚¿ãƒ¼ãƒ³4è©²å½“: +25ç‚¹
    df.loc[df['local_quality_std'] >= local_std_high, 'hallucination_risk_score'] += 25

    # ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«åˆ†é¡
    df['risk_level'] = pd.cut(df['hallucination_risk_score'],
                               bins=[0, 10, 30, 50, 100],
                               labels=['MINIMAL', 'LOW', 'MEDIUM', 'HIGH'])

    # ãƒªã‚¹ã‚¯åˆ†å¸ƒ
    risk_dist = df['risk_level'].value_counts().sort_index()
    print(f"\nãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒªã‚¹ã‚¯åˆ†å¸ƒ:")
    for level, count in risk_dist.items():
        pct = count / len(df) * 100
        print(f"   {level:10s}: {count:4d}ä»¶ ({pct:5.1f}%)")

    # ãƒªã‚¹ã‚¯ä»˜ãCSVä¿å­˜
    output_csv = output_dir / 'results_with_risk_score.csv'
    df.to_csv(output_csv, index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ä»˜ãçµæœä¿å­˜: {output_csv}")

    print(f"{'='*80}\n")


def generate_research_plots(df, output_dir):
    """
    ç ”ç©¶ç”¨ãƒ—ãƒ­ãƒƒãƒˆç”»åƒã‚’ç”Ÿæˆï¼ˆè«–æ–‡ãƒ»ç™ºè¡¨ç”¨ï¼‰
    """

    print(f"\nğŸ“Š ç ”ç©¶ç”¨ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆä¸­:")
    print(f"{'='*80}")

    # 1. ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹ vs PSNR æ•£å¸ƒå›³ï¼ˆAIãƒ¢ãƒ‡ãƒ«ã®æˆ¦ç•¥ã‚’ç¤ºã™ï¼‰
    plt.figure(figsize=(12, 8))

    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['psnr'], model_data['sharpness'],
                   label=model, alpha=0.6, s=50)

    plt.xlabel('PSNRï¼ˆå¿ å®Ÿåº¦ï¼‰[dB]', fontsize=14, fontweight='bold')
    plt.ylabel('ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹ï¼ˆé®®æ˜åº¦ï¼‰', fontsize=14, fontweight='bold')
    plt.title('AIãƒ¢ãƒ‡ãƒ«ã®æˆ¦ç•¥ãƒãƒƒãƒ—ï¼šå¿ å®Ÿåº¦ vs é®®æ˜åº¦', fontsize=16, fontweight='bold')
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)

    # æˆ¦ç•¥é ˜åŸŸã®æ³¨é‡ˆ
    plt.axhline(y=df['sharpness'].median(), color='red', linestyle='--', alpha=0.3, label='ä¸­å¤®å€¤')
    plt.axvline(x=df['psnr'].median(), color='red', linestyle='--', alpha=0.3)

    # é ˜åŸŸãƒ©ãƒ™ãƒ«
    max_psnr = df['psnr'].max()
    max_sharp = df['sharpness'].max()
    plt.text(max_psnr * 0.95, max_sharp * 0.95, 'ç†æƒ³é ˜åŸŸ\nï¼ˆé«˜å¿ å®Ÿãƒ»é«˜é®®æ˜ï¼‰',
             fontsize=10, ha='right', va='top', bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))
    plt.text(df['psnr'].min() * 1.05, max_sharp * 0.95, 'éå‰°å‡¦ç†é ˜åŸŸ\nï¼ˆä½å¿ å®Ÿãƒ»é«˜é®®æ˜ï¼‰\nãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ç–‘ã„',
             fontsize=10, ha='left', va='top', bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.5))

    plt.tight_layout()
    plot1_path = output_dir / 'strategy_map_sharpness_vs_psnr.png'
    plt.savefig(plot1_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹ vs PSNR æ•£å¸ƒå›³: {plot1_path}")


    # 2. LPIPS ç®±ã²ã’å›³ï¼ˆå®‰å®šæ€§ã‚’ç¤ºã™ï¼‰
    plt.figure(figsize=(10, 6))

    lpips_data = [df[df['model'] == model]['lpips'].values for model in df['model'].unique()]
    models = df['model'].unique()

    bp = plt.boxplot(lpips_data, labels=models, patch_artist=True,
                     showmeans=True, meanline=True)

    # ã‚«ãƒ©ãƒ¼ãƒªãƒ³ã‚°
    colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow', 'lightpink']
    for patch, color in zip(bp['boxes'], colors):
        patch.set_facecolor(color)

    plt.ylabel('LPIPSï¼ˆçŸ¥è¦šçš„é¡ä¼¼åº¦ï¼‰', fontsize=14, fontweight='bold')
    plt.xlabel('AIãƒ¢ãƒ‡ãƒ«', fontsize=14, fontweight='bold')
    plt.title('ãƒ¢ãƒ‡ãƒ«åˆ¥ LPIPS åˆ†å¸ƒï¼ˆå®‰å®šæ€§è©•ä¾¡ï¼‰\nç®±ãŒå°ã•ã„ = å®‰å®š', fontsize=16, fontweight='bold')
    plt.grid(axis='y', alpha=0.3)
    plt.xticks(rotation=45, ha='right')

    plt.tight_layout()
    plot2_path = output_dir / 'stability_lpips_boxplot.png'
    plt.savefig(plot2_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… LPIPS ç®±ã²ã’å›³: {plot2_path}")


    # 3. SSIM vs PSNR æ•£å¸ƒå›³ï¼ˆç›¸é–¢ç¢ºèªãƒ»ç•°å¸¸æ¤œå‡ºï¼‰
    plt.figure(figsize=(12, 8))

    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['ssim'], model_data['psnr'],
                   label=model, alpha=0.6, s=50)

    plt.xlabel('SSIMï¼ˆæ§‹é€ é¡ä¼¼æ€§ï¼‰', fontsize=14, fontweight='bold')
    plt.ylabel('PSNRï¼ˆä¿¡å·å¯¾é›‘éŸ³æ¯”ï¼‰[dB]', fontsize=14, fontweight='bold')
    plt.title('SSIM vs PSNR ç›¸é–¢åˆ†æ\nç›¸é–¢ã‹ã‚‰å¤–ã‚Œã‚‹ç‚¹ = ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³å€™è£œ', fontsize=16, fontweight='bold')
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)

    # è¿‘ä¼¼ç›´ç·š
    from scipy import stats
    slope, intercept, r_value, p_value, std_err = stats.linregress(df['ssim'], df['psnr'])
    x_line = np.array([df['ssim'].min(), df['ssim'].max()])
    y_line = slope * x_line + intercept
    plt.plot(x_line, y_line, 'r--', label=f'å›å¸°ç›´ç·š (RÂ²={r_value**2:.3f})', linewidth=2)
    plt.legend(fontsize=12)

    plt.tight_layout()
    plot3_path = output_dir / 'correlation_ssim_vs_psnr.png'
    plt.savefig(plot3_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… SSIM vs PSNR æ•£å¸ƒå›³: {plot3_path}")


    # 4. ãƒã‚¤ã‚º vs ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ æ•£å¸ƒå›³
    plt.figure(figsize=(12, 8))

    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['noise'], model_data['artifact_total'],
                   label=model, alpha=0.6, s=50)

    plt.xlabel('ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«', fontsize=14, fontweight='bold')
    plt.ylabel('ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆï¼ˆãƒ–ãƒ­ãƒƒã‚¯+ãƒªãƒ³ã‚®ãƒ³ã‚°ï¼‰', fontsize=14, fontweight='bold')
    plt.title('ãƒã‚¤ã‚º vs ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ\nå·¦ä¸‹ãŒç†æƒ³ï¼ˆä¸¡æ–¹å°‘ãªã„ï¼‰', fontsize=16, fontweight='bold')
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)

    # ç†æƒ³é ˜åŸŸã®è¡¨ç¤º
    low_noise = df['noise'].quantile(0.25)
    low_artifact = df['artifact_total'].quantile(0.25)
    plt.axvline(x=low_noise, color='green', linestyle='--', alpha=0.3)
    plt.axhline(y=low_artifact, color='green', linestyle='--', alpha=0.3)
    plt.fill_between([0, low_noise], 0, low_artifact, alpha=0.1, color='green', label='ç†æƒ³é ˜åŸŸ')
    plt.legend(fontsize=12)

    plt.tight_layout()
    plot4_path = output_dir / 'quality_noise_vs_artifact.png'
    plt.savefig(plot4_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ãƒã‚¤ã‚º vs ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ: {plot4_path}")


    # 5. ãƒ¢ãƒ‡ãƒ«åˆ¥ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆï¼ˆä¸»è¦6æŒ‡æ¨™ï¼‰
    fig = plt.figure(figsize=(14, 10))

    categories = ['SSIM', 'PSNR', 'ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹', 'ã‚¨ãƒƒã‚¸å¯†åº¦', 'ãƒã‚¤ã‚º\nï¼ˆåè»¢ï¼‰', 'ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ\nï¼ˆåè»¢ï¼‰']
    num_vars = len(categories)

    # æ­£è¦åŒ–ï¼ˆ0-1ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
    df_norm = df.copy()
    df_norm['ssim_norm'] = df['ssim']
    df_norm['psnr_norm'] = df['psnr'] / df['psnr'].max()
    df_norm['sharpness_norm'] = df['sharpness'] / df['sharpness'].max()
    df_norm['edge_norm'] = df['edge_density'] / df['edge_density'].max()
    df_norm['noise_norm'] = 1 - (df['noise'] / df['noise'].max())  # åè»¢ï¼ˆå°‘ãªã„æ–¹ãŒè‰¯ã„ï¼‰
    df_norm['artifact_norm'] = 1 - (df['artifact_total'] / df['artifact_total'].max())  # åè»¢

    angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()
    angles += angles[:1]

    ax = fig.add_subplot(111, polar=True)

    for model in df['model'].unique():
        model_data = df_norm[df_norm['model'] == model]
        values = [
            model_data['ssim_norm'].mean(),
            model_data['psnr_norm'].mean(),
            model_data['sharpness_norm'].mean(),
            model_data['edge_norm'].mean(),
            model_data['noise_norm'].mean(),
            model_data['artifact_norm'].mean()
        ]
        values += values[:1]

        ax.plot(angles, values, 'o-', linewidth=2, label=model)
        ax.fill(angles, values, alpha=0.1)

    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(categories, fontsize=12)
    ax.set_ylim(0, 1)
    ax.set_title('ãƒ¢ãƒ‡ãƒ«åˆ¥æ€§èƒ½ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆï¼‰\nå¤–å´ã»ã©é«˜æ€§èƒ½', fontsize=16, fontweight='bold', pad=20)
    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=12)
    ax.grid(True)

    plt.tight_layout()
    plot5_path = output_dir / 'radar_chart_model_comparison.png'
    plt.savefig(plot5_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ãƒ¢ãƒ‡ãƒ«åˆ¥ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ: {plot5_path}")


    # 6. 17é …ç›®ã®ãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆï¼ˆåˆ†å¸ƒã®å¯è¦–åŒ–ï¼‰
    fig, axes = plt.subplots(3, 6, figsize=(24, 12))
    fig.suptitle('17é …ç›®ã®åˆ†å¸ƒï¼ˆãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆï¼‰', fontsize=20, fontweight='bold')

    metrics_for_violin = [
        'ssim', 'ms_ssim', 'psnr', 'lpips', 'sharpness', 'contrast',
        'entropy', 'noise', 'edge_density', 'artifact_total', 'delta_e',
        'high_freq_ratio', 'texture_complexity', 'local_quality_mean',
        'histogram_corr', 'lab_L_mean', 'total_score'
    ]

    for i, metric in enumerate(metrics_for_violin):
        ax = axes[i // 6, i % 6]

        violin_data = [df[df['model'] == model][metric].values for model in df['model'].unique()]
        parts = ax.violinplot(violin_data, showmeans=True, showmedians=True)

        # ã‚«ãƒ©ãƒ¼ãƒªãƒ³ã‚°
        for pc in parts['bodies']:
            pc.set_facecolor('lightblue')
            pc.set_alpha(0.7)

        ax.set_xticks(range(1, len(df['model'].unique()) + 1))
        ax.set_xticklabels(df['model'].unique(), rotation=45, ha='right', fontsize=8)
        ax.set_title(metric, fontsize=10, fontweight='bold')
        ax.grid(axis='y', alpha=0.3)

    # ä½™ã£ãŸè»¸ã‚’éè¡¨ç¤º
    for i in range(len(metrics_for_violin), 18):
        axes[i // 6, i % 6].axis('off')

    plt.tight_layout()
    plot6_path = output_dir / 'violin_plots_all_metrics.png'
    plt.savefig(plot6_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… 17é …ç›®ãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆ: {plot6_path}")

    print(f"{'='*80}")
    print(f"âœ… å…¨6ç¨®é¡ã®ç ”ç©¶ç”¨ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆå®Œäº†")
    print(f"   è«–æ–‡ãƒ»ç™ºè¡¨è³‡æ–™ã«ãã®ã¾ã¾ä½¿ç”¨ã§ãã¾ã™ï¼ˆ300dpié«˜è§£åƒåº¦ï¼‰\n")


if __name__ == '__main__':
    if len(sys.argv) < 2:
        print(f"\nä½¿ã„æ–¹:")
        print(f"  python analyze_results.py results/batch_analysis.csv\n")
        sys.exit(1)

    csv_file = sys.argv[1]

    if not Path(csv_file).exists():
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_file}")
        sys.exit(1)

    analyze_batch_results(csv_file)
