"""
çµ±è¨ˆåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼šãƒãƒƒãƒå‡¦ç†çµæœã‹ã‚‰é–¾å€¤ã‚’æ±ºå®š

ä½¿ã„æ–¹:
python analyze_results.py results/batch_analysis.csv
"""

import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['MS Gothic', 'Yu Gothic', 'Meiryo', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False


def analyze_batch_results(csv_file):
    """
    ãƒãƒƒãƒå‡¦ç†çµæœã®çµ±è¨ˆåˆ†æ
    """

    # CSVèª­ã¿è¾¼ã¿
    df = pd.read_csv(csv_file)

    print(f"\n{'='*80}")
    print(f"ğŸ“Š çµ±è¨ˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
    print(f"{'='*80}")
    print(f"ğŸ“„ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«: {csv_file}")
    print(f"ğŸ“· ç”»åƒæ•°: {df['image_id'].nunique()}")
    print(f"ğŸ¤– ãƒ¢ãƒ‡ãƒ«æ•°: {df['model'].nunique()}")
    print(f"ğŸ“Š ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(df)}")
    print(f"{'='*80}\n")

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_dir = Path('analysis_output')
    output_dir.mkdir(exist_ok=True)

    # 1. åŸºæœ¬çµ±è¨ˆé‡
    print_basic_statistics(df)

    # 2. ãƒ¢ãƒ‡ãƒ«åˆ¥æ¯”è¼ƒ
    compare_models(df, output_dir)

    # 3. ç›¸é–¢åˆ†æ
    analyze_correlations(df, output_dir)

    # 4. é–¾å€¤ææ¡ˆ
    suggest_thresholds(df, output_dir)

    # 5. ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ææ¡ˆ
    suggest_hallucination_logic(df, output_dir)

    # 6. ç ”ç©¶ç”¨ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆ
    generate_research_plots(df, output_dir)

    print(f"\nâœ… åˆ†æå®Œäº†ï¼")
    print(f"ğŸ“ çµæœä¿å­˜å…ˆ: {output_dir}/")


def print_basic_statistics(df):
    """
    åŸºæœ¬çµ±è¨ˆé‡ã®è¡¨ç¤º
    """

    print(f"\nğŸ“ˆ ä¸»è¦æŒ‡æ¨™ã®åŸºæœ¬çµ±è¨ˆé‡:")
    print(f"{'='*80}")

    # 17é …ç›®ã™ã¹ã¦
    metrics = ['ssim', 'ms_ssim', 'psnr', 'lpips', 'sharpness', 'contrast',
               'entropy', 'noise', 'edge_density', 'artifact_total', 'delta_e',
               'high_freq_ratio', 'texture_complexity', 'local_quality_mean',
               'histogram_corr', 'lab_L_mean', 'total_score']

    stats = df[metrics].describe().T
    stats.columns = ['ä»¶æ•°', 'å¹³å‡', 'æ¨™æº–åå·®', 'æœ€å°', '25%', '50%', '75%', 'æœ€å¤§']

    print(stats.round(4).to_string())
    print(f"{'='*80}\n")


def compare_models(df, output_dir):
    """
    ãƒ¢ãƒ‡ãƒ«åˆ¥æ¯”è¼ƒ
    """

    print(f"\nğŸ† ãƒ¢ãƒ‡ãƒ«åˆ¥ãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
    print(f"{'='*80}")

    # ä¸»è¦æŒ‡æ¨™ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    model_comparison = df.groupby('model').agg({
        'ssim': ['mean', 'std'],
        'psnr': ['mean', 'std'],
        'lpips': ['mean', 'std'],
        'total_score': ['mean', 'std'],
        'noise': ['mean', 'std'],
        'artifact_total': ['mean', 'std'],
        'sharpness': ['mean', 'std'],
        'edge_density': ['mean', 'std']
    }).round(4)

    # ç·åˆã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
    model_comparison = model_comparison.sort_values(('total_score', 'mean'), ascending=False)

    print(model_comparison.to_string())
    print(f"{'='*80}\n")

    # CSVä¿å­˜
    model_comparison.to_csv(output_dir / 'model_comparison.csv', encoding='utf-8-sig')

    # å¯è¦–åŒ–ï¼šãƒ¢ãƒ‡ãƒ«åˆ¥ç·åˆã‚¹ã‚³ã‚¢
    plt.figure(figsize=(12, 6))
    model_scores = df.groupby('model')['total_score'].mean().sort_values(ascending=False)

    plt.bar(range(len(model_scores)), model_scores.values)
    plt.xticks(range(len(model_scores)), model_scores.index, rotation=45, ha='right')
    plt.ylabel('ç·åˆã‚¹ã‚³ã‚¢ï¼ˆå¹³å‡ï¼‰')
    plt.title('ãƒ¢ãƒ‡ãƒ«åˆ¥ç·åˆã‚¹ã‚³ã‚¢æ¯”è¼ƒ')
    plt.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'model_scores.png', dpi=150)
    plt.close()

    print(f"ğŸ“Š ã‚°ãƒ©ãƒ•ä¿å­˜: {output_dir}/model_scores.png")


def analyze_correlations(df, output_dir):
    """
    17é …ç›®é–“ã®ç›¸é–¢åˆ†æ
    """

    print(f"\nğŸ”— ç›¸é–¢åˆ†æ:")
    print(f"{'='*80}")

    # æ•°å€¤åˆ—ã®ã¿æŠ½å‡º
    numeric_cols = ['ssim', 'psnr', 'lpips', 'ms_ssim', 'sharpness', 'contrast',
                    'entropy', 'noise', 'edge_density', 'artifact_total', 'delta_e',
                    'high_freq_ratio', 'texture_complexity', 'local_quality_mean',
                    'histogram_corr', 'lab_L_mean', 'total_score']

    corr_matrix = df[numeric_cols].corr()

    # ç›¸é–¢è¡Œåˆ—ã‚’ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—è¡¨ç¤º
    plt.figure(figsize=(14, 12))
    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,
                square=True, linewidths=0.5, cbar_kws={"shrink": 0.8})
    plt.title('17é …ç›®ã®ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(output_dir / 'correlation_matrix.png', dpi=150)
    plt.close()

    print(f"ğŸ“Š ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ä¿å­˜: {output_dir}/correlation_matrix.png")

    # é«˜ç›¸é–¢ãƒšã‚¢ã‚’è¡¨ç¤º
    print(f"\nğŸ”¥ é«˜ç›¸é–¢ãƒšã‚¢ï¼ˆ|r| > 0.7ï¼‰:")
    high_corr = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            corr_value = corr_matrix.iloc[i, j]
            if abs(corr_value) > 0.7:
                high_corr.append({
                    'metric1': corr_matrix.columns[i],
                    'metric2': corr_matrix.columns[j],
                    'correlation': corr_value
                })

    if high_corr:
        high_corr_df = pd.DataFrame(high_corr).sort_values('correlation', ascending=False)
        print(high_corr_df.to_string(index=False))
    else:
        print("   (ãªã—)")

    print(f"{'='*80}\n")


def suggest_thresholds(df, output_dir):
    """
    æ ¹æ‹ ã®ã‚ã‚‹é–¾å€¤ã‚’ææ¡ˆ
    """

    print(f"\nğŸ’¡ æ¨å¥¨é–¾å€¤ã®ææ¡ˆ:")
    print(f"{'='*80}")

    thresholds = {}

    # å„æŒ‡æ¨™ã®çµ±è¨ˆå€¤ã‹ã‚‰é–¾å€¤ã‚’æ±ºå®š
    # 17é …ç›®ã™ã¹ã¦ã®é–¾å€¤ã‚’ææ¡ˆ
    metrics_config = {
        'ssim': {'direction': 'high', 'percentile': 25, 'name': 'SSIMï¼ˆæ§‹é€ é¡ä¼¼æ€§ï¼‰'},
        'ms_ssim': {'direction': 'high', 'percentile': 25, 'name': 'MS-SSIMï¼ˆãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«SSIMï¼‰'},
        'psnr': {'direction': 'high', 'percentile': 25, 'name': 'PSNRï¼ˆä¿¡å·å¯¾é›‘éŸ³æ¯”ï¼‰'},
        'lpips': {'direction': 'low', 'percentile': 75, 'name': 'LPIPSï¼ˆçŸ¥è¦šçš„é¡ä¼¼åº¦ï¼‰'},
        'sharpness': {'direction': 'high', 'percentile': 25, 'name': 'ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹'},
        'contrast': {'direction': 'high', 'percentile': 25, 'name': 'ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ'},
        'entropy': {'direction': 'high', 'percentile': 25, 'name': 'ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆæƒ…å ±é‡ï¼‰'},
        'noise': {'direction': 'low', 'percentile': 75, 'name': 'ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«'},
        'edge_density': {'direction': 'high', 'percentile': 25, 'name': 'ã‚¨ãƒƒã‚¸å¯†åº¦'},
        'artifact_total': {'direction': 'low', 'percentile': 75, 'name': 'ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ'},
        'delta_e': {'direction': 'low', 'percentile': 75, 'name': 'è‰²å·®ï¼ˆÎ”Eï¼‰'},
        'high_freq_ratio': {'direction': 'high', 'percentile': 25, 'name': 'é«˜å‘¨æ³¢æˆåˆ†æ¯”ç‡'},
        'texture_complexity': {'direction': 'high', 'percentile': 25, 'name': 'ãƒ†ã‚¯ã‚¹ãƒãƒ£è¤‡é›‘åº¦'},
        'local_quality_mean': {'direction': 'high', 'percentile': 25, 'name': 'å±€æ‰€å“è³ªå¹³å‡'},
        'histogram_corr': {'direction': 'high', 'percentile': 25, 'name': 'ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ç›¸é–¢'},
        'lab_L_mean': {'direction': 'neutral', 'percentile': 50, 'name': 'LABæ˜åº¦ï¼ˆå‚è€ƒå€¤ï¼‰'},
        'total_score': {'direction': 'high', 'percentile': 25, 'name': 'ç·åˆã‚¹ã‚³ã‚¢'},
    }

    for metric, config in metrics_config.items():
        data = df[metric].dropna()

        if config['direction'] == 'neutral':
            # ä¸­ç«‹çš„ãªæŒ‡æ¨™ï¼ˆæ˜åº¦ãªã©ï¼‰ï¼šä¸­å¤®å€¤ã‚’å‚è€ƒå€¤ã¨ã—ã¦è¡¨ç¤º
            threshold = np.percentile(data, config['percentile'])
            condition = f"å‚è€ƒå€¤: {threshold:.4f}"
        elif config['direction'] == 'high':
            # é«˜ã„æ–¹ãŒè‰¯ã„æŒ‡æ¨™ï¼š25ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ä»¥ä¸Šã‚’æ¨å¥¨
            threshold = np.percentile(data, config['percentile'])
            condition = f">= {threshold:.4f}"
        else:
            # ä½ã„æ–¹ãŒè‰¯ã„æŒ‡æ¨™ï¼š75ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ä»¥ä¸‹ã‚’æ¨å¥¨
            threshold = np.percentile(data, config['percentile'])
            condition = f"<= {threshold:.4f}"

        thresholds[metric] = {
            'name': config['name'],
            'threshold': threshold,
            'condition': condition,
            'mean': data.mean(),
            'std': data.std(),
            'min': data.min(),
            'max': data.max()
        }

        print(f"{config['name']:30s}: {condition:20s} (å¹³å‡: {data.mean():.4f}, æ¨™æº–åå·®: {data.std():.4f})")

    print(f"{'='*80}")
    print(f"ğŸ’¡ è§£é‡ˆ:")
    print(f"   - ã“ã‚Œã‚‰ã®é–¾å€¤ã¯ã€å…¨ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆåˆ†å¸ƒã«åŸºã¥ã„ã¦ã„ã¾ã™")
    print(f"   - 25ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ« = ä¸Šä½75%ã®å“è³ªã‚’ã€Œåˆæ ¼ã€ã¨ã™ã‚‹åŸºæº–")
    print(f"   - 75ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ« = ä¸‹ä½75%ã®å“è³ªã‚’ã€Œåˆæ ¼ã€ã¨ã™ã‚‹åŸºæº–")
    print(f"{'='*80}\n")

    # JSONä¿å­˜
    import json
    with open(output_dir / 'recommended_thresholds.json', 'w', encoding='utf-8') as f:
        json.dump(thresholds, f, indent=2, ensure_ascii=False)

    print(f"ğŸ’¾ é–¾å€¤ä¿å­˜: {output_dir}/recommended_thresholds.json\n")


def suggest_hallucination_logic(df, output_dir):
    """
    ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ã®ææ¡ˆ
    """

    print(f"\nğŸ” ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ã®ææ¡ˆ:")
    print(f"{'='*80}")

    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: SSIMé«˜ã„ã®ã«PSNRä½ã„ï¼ˆæ§‹é€ ã¯ä¼¼ã¦ã‚‹ãŒãƒ”ã‚¯ã‚»ãƒ«å€¤ãŒé•ã†ï¼‰
    ssim_high = df['ssim'].quantile(0.75)
    psnr_low = df['psnr'].quantile(0.25)

    pattern1 = df[(df['ssim'] >= ssim_high) & (df['psnr'] <= psnr_low)]
    pattern1_rate = len(pattern1) / len(df) * 100

    print(f"ã€ãƒ‘ã‚¿ãƒ¼ãƒ³1ã€‘SSIMé«˜ & PSNRä½ (æ§‹é€ é¡ä¼¼ã ãŒãƒ”ã‚¯ã‚»ãƒ«å€¤ç›¸é•)")
    print(f"   æ¡ä»¶: SSIM >= {ssim_high:.4f} AND PSNR <= {psnr_low:.2f}")
    print(f"   è©²å½“ç‡: {pattern1_rate:.1f}% ({len(pattern1)}/{len(df)}ä»¶)")
    print(f"   ãƒªã‚¹ã‚¯: ä¸­ï½é«˜ï¼ˆAIãŒæ§‹é€ ã‚’æ¨¡å€£ã—ãŸå¯èƒ½æ€§ï¼‰")

    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹é«˜ã„ãŒãƒã‚¤ã‚ºã‚‚é«˜ã„ï¼ˆéå‰°å‡¦ç†ï¼‰
    sharp_high = df['sharpness'].quantile(0.75)
    noise_high = df['noise'].quantile(0.75)

    pattern2 = df[(df['sharpness'] >= sharp_high) & (df['noise'] >= noise_high)]
    pattern2_rate = len(pattern2) / len(df) * 100

    print(f"\nã€ãƒ‘ã‚¿ãƒ¼ãƒ³2ã€‘ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹é«˜ & ãƒã‚¤ã‚ºé«˜ (éå‰°å‡¦ç†)")
    print(f"   æ¡ä»¶: ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹ >= {sharp_high:.2f} AND ãƒã‚¤ã‚º >= {noise_high:.2f}")
    print(f"   è©²å½“ç‡: {pattern2_rate:.1f}% ({len(pattern2)}/{len(df)}ä»¶)")
    print(f"   ãƒªã‚¹ã‚¯: ä¸­ï¼ˆéåº¦ãªã‚·ãƒ£ãƒ¼ãƒ—åŒ–ã«ã‚ˆã‚‹ãƒã‚¤ã‚ºå¢—å¹…ï¼‰")

    # ãƒ‘ã‚¿ãƒ¼ãƒ³3: ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆé«˜ï¼ˆGANç‰¹æœ‰ï¼‰
    artifact_high = df['artifact_total'].quantile(0.90)

    pattern3 = df[df['artifact_total'] >= artifact_high]
    pattern3_rate = len(pattern3) / len(df) * 100

    print(f"\nã€ãƒ‘ã‚¿ãƒ¼ãƒ³3ã€‘ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆé«˜ (GANç‰¹æœ‰ã®æ­ªã¿)")
    print(f"   æ¡ä»¶: ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ >= {artifact_high:.2f}")
    print(f"   è©²å½“ç‡: {pattern3_rate:.1f}% ({len(pattern3)}/{len(df)}ä»¶)")
    print(f"   ãƒªã‚¹ã‚¯: é«˜ï¼ˆãƒªãƒ³ã‚®ãƒ³ã‚°ãƒ»ãƒ–ãƒ­ãƒƒã‚¯ãƒã‚¤ã‚ºã«ã‚ˆã‚‹è¨ºæ–­é˜»å®³ï¼‰")

    # ãƒ‘ã‚¿ãƒ¼ãƒ³4: å±€æ‰€å“è³ªã®ã°ã‚‰ã¤ãå¤§
    local_std_high = df['local_quality_std'].quantile(0.75)

    pattern4 = df[df['local_quality_std'] >= local_std_high]
    pattern4_rate = len(pattern4) / len(df) * 100

    print(f"\nã€ãƒ‘ã‚¿ãƒ¼ãƒ³4ã€‘å±€æ‰€å“è³ªã®ã°ã‚‰ã¤ãå¤§ (ä¸å‡ä¸€ãªå‡¦ç†)")
    print(f"   æ¡ä»¶: å±€æ‰€SSIMæ¨™æº–åå·® >= {local_std_high:.4f}")
    print(f"   è©²å½“ç‡: {pattern4_rate:.1f}% ({len(pattern4)}/{len(df)}ä»¶)")
    print(f"   ãƒªã‚¹ã‚¯: ä¸­ï½é«˜ï¼ˆé ˜åŸŸã«ã‚ˆã£ã¦å“è³ªãŒç•°ãªã‚‹ = ä¸€éƒ¨ã«ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼‰")

    print(f"{'='*80}\n")

    # ç·åˆãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
    print(f"ğŸ“Š ç·åˆãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ã®ææ¡ˆ:")
    print(f"{'='*80}")

    df['hallucination_risk_score'] = 0

    # ãƒ‘ã‚¿ãƒ¼ãƒ³1è©²å½“: +25ç‚¹
    df.loc[(df['ssim'] >= ssim_high) & (df['psnr'] <= psnr_low), 'hallucination_risk_score'] += 25

    # ãƒ‘ã‚¿ãƒ¼ãƒ³2è©²å½“: +20ç‚¹
    df.loc[(df['sharpness'] >= sharp_high) & (df['noise'] >= noise_high), 'hallucination_risk_score'] += 20

    # ãƒ‘ã‚¿ãƒ¼ãƒ³3è©²å½“: +30ç‚¹
    df.loc[df['artifact_total'] >= artifact_high, 'hallucination_risk_score'] += 30

    # ãƒ‘ã‚¿ãƒ¼ãƒ³4è©²å½“: +25ç‚¹
    df.loc[df['local_quality_std'] >= local_std_high, 'hallucination_risk_score'] += 25

    # ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«åˆ†é¡
    df['risk_level'] = pd.cut(df['hallucination_risk_score'],
                               bins=[0, 10, 30, 50, 100],
                               labels=['MINIMAL', 'LOW', 'MEDIUM', 'HIGH'])

    # ãƒªã‚¹ã‚¯åˆ†å¸ƒ
    risk_dist = df['risk_level'].value_counts().sort_index()
    print(f"\nãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒªã‚¹ã‚¯åˆ†å¸ƒ:")
    for level, count in risk_dist.items():
        pct = count / len(df) * 100
        print(f"   {level:10s}: {count:4d}ä»¶ ({pct:5.1f}%)")

    # ãƒªã‚¹ã‚¯ä»˜ãCSVä¿å­˜
    output_csv = output_dir / 'results_with_risk_score.csv'
    df.to_csv(output_csv, index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ä»˜ãçµæœä¿å­˜: {output_csv}")

    print(f"{'='*80}\n")


def generate_research_plots(df, output_dir):
    """
    ç ”ç©¶ç”¨ãƒ—ãƒ­ãƒƒãƒˆç”»åƒã‚’ç”Ÿæˆï¼ˆè«–æ–‡ãƒ»ç™ºè¡¨ç”¨ï¼‰
    """

    print(f"\nğŸ“Š ç ”ç©¶ç”¨ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆä¸­:")
    print(f"{'='*80}")

    # 1. ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹ vs PSNR æ•£å¸ƒå›³ï¼ˆAIãƒ¢ãƒ‡ãƒ«ã®æˆ¦ç•¥ã‚’ç¤ºã™ï¼‰
    plt.figure(figsize=(12, 8))

    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['psnr'], model_data['sharpness'],
                   label=model, alpha=0.6, s=50)

    plt.xlabel('PSNRï¼ˆå¿ å®Ÿåº¦ï¼‰[dB]', fontsize=14, fontweight='bold')
    plt.ylabel('ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹ï¼ˆé®®æ˜åº¦ï¼‰', fontsize=14, fontweight='bold')
    plt.title('AIãƒ¢ãƒ‡ãƒ«ã®æˆ¦ç•¥ãƒãƒƒãƒ—ï¼šå¿ å®Ÿåº¦ vs é®®æ˜åº¦', fontsize=16, fontweight='bold')
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)

    # æˆ¦ç•¥é ˜åŸŸã®æ³¨é‡ˆ
    plt.axhline(y=df['sharpness'].median(), color='red', linestyle='--', alpha=0.3, label='ä¸­å¤®å€¤')
    plt.axvline(x=df['psnr'].median(), color='red', linestyle='--', alpha=0.3)

    # é ˜åŸŸãƒ©ãƒ™ãƒ«
    max_psnr = df['psnr'].max()
    max_sharp = df['sharpness'].max()
    plt.text(max_psnr * 0.95, max_sharp * 0.95, 'ç†æƒ³é ˜åŸŸ\nï¼ˆé«˜å¿ å®Ÿãƒ»é«˜é®®æ˜ï¼‰',
             fontsize=10, ha='right', va='top', bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))
    plt.text(df['psnr'].min() * 1.05, max_sharp * 0.95, 'éå‰°å‡¦ç†é ˜åŸŸ\nï¼ˆä½å¿ å®Ÿãƒ»é«˜é®®æ˜ï¼‰\nãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ç–‘ã„',
             fontsize=10, ha='left', va='top', bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.5))

    plt.tight_layout()
    plot1_path = output_dir / 'strategy_map_sharpness_vs_psnr.png'
    plt.savefig(plot1_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹ vs PSNR æ•£å¸ƒå›³: {plot1_path}")


    # 2. LPIPS ç®±ã²ã’å›³ï¼ˆå®‰å®šæ€§ã‚’ç¤ºã™ï¼‰
    plt.figure(figsize=(10, 6))

    lpips_data = [df[df['model'] == model]['lpips'].values for model in df['model'].unique()]
    models = df['model'].unique()

    bp = plt.boxplot(lpips_data, labels=models, patch_artist=True,
                     showmeans=True, meanline=True)

    # ã‚«ãƒ©ãƒ¼ãƒªãƒ³ã‚°
    colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow', 'lightpink']
    for patch, color in zip(bp['boxes'], colors):
        patch.set_facecolor(color)

    plt.ylabel('LPIPSï¼ˆçŸ¥è¦šçš„é¡ä¼¼åº¦ï¼‰', fontsize=14, fontweight='bold')
    plt.xlabel('AIãƒ¢ãƒ‡ãƒ«', fontsize=14, fontweight='bold')
    plt.title('ãƒ¢ãƒ‡ãƒ«åˆ¥ LPIPS åˆ†å¸ƒï¼ˆå®‰å®šæ€§è©•ä¾¡ï¼‰\nç®±ãŒå°ã•ã„ = å®‰å®š', fontsize=16, fontweight='bold')
    plt.grid(axis='y', alpha=0.3)
    plt.xticks(rotation=45, ha='right')

    plt.tight_layout()
    plot2_path = output_dir / 'stability_lpips_boxplot.png'
    plt.savefig(plot2_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… LPIPS ç®±ã²ã’å›³: {plot2_path}")


    # 3. SSIM vs PSNR æ•£å¸ƒå›³ï¼ˆç›¸é–¢ç¢ºèªãƒ»ç•°å¸¸æ¤œå‡ºï¼‰
    plt.figure(figsize=(12, 8))

    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['ssim'], model_data['psnr'],
                   label=model, alpha=0.6, s=50)

    plt.xlabel('SSIMï¼ˆæ§‹é€ é¡ä¼¼æ€§ï¼‰', fontsize=14, fontweight='bold')
    plt.ylabel('PSNRï¼ˆä¿¡å·å¯¾é›‘éŸ³æ¯”ï¼‰[dB]', fontsize=14, fontweight='bold')
    plt.title('SSIM vs PSNR ç›¸é–¢åˆ†æ\nç›¸é–¢ã‹ã‚‰å¤–ã‚Œã‚‹ç‚¹ = ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³å€™è£œ', fontsize=16, fontweight='bold')
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)

    # è¿‘ä¼¼ç›´ç·š
    from scipy import stats
    slope, intercept, r_value, p_value, std_err = stats.linregress(df['ssim'], df['psnr'])
    x_line = np.array([df['ssim'].min(), df['ssim'].max()])
    y_line = slope * x_line + intercept
    plt.plot(x_line, y_line, 'r--', label=f'å›å¸°ç›´ç·š (RÂ²={r_value**2:.3f})', linewidth=2)
    plt.legend(fontsize=12)

    plt.tight_layout()
    plot3_path = output_dir / 'correlation_ssim_vs_psnr.png'
    plt.savefig(plot3_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… SSIM vs PSNR æ•£å¸ƒå›³: {plot3_path}")


    # 4. ãƒã‚¤ã‚º vs ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ æ•£å¸ƒå›³
    plt.figure(figsize=(12, 8))

    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['noise'], model_data['artifact_total'],
                   label=model, alpha=0.6, s=50)

    plt.xlabel('ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«', fontsize=14, fontweight='bold')
    plt.ylabel('ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆï¼ˆãƒ–ãƒ­ãƒƒã‚¯+ãƒªãƒ³ã‚®ãƒ³ã‚°ï¼‰', fontsize=14, fontweight='bold')
    plt.title('ãƒã‚¤ã‚º vs ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ\nå·¦ä¸‹ãŒç†æƒ³ï¼ˆä¸¡æ–¹å°‘ãªã„ï¼‰', fontsize=16, fontweight='bold')
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)

    # ç†æƒ³é ˜åŸŸã®è¡¨ç¤º
    low_noise = df['noise'].quantile(0.25)
    low_artifact = df['artifact_total'].quantile(0.25)
    plt.axvline(x=low_noise, color='green', linestyle='--', alpha=0.3)
    plt.axhline(y=low_artifact, color='green', linestyle='--', alpha=0.3)
    plt.fill_between([0, low_noise], 0, low_artifact, alpha=0.1, color='green', label='ç†æƒ³é ˜åŸŸ')
    plt.legend(fontsize=12)

    plt.tight_layout()
    plot4_path = output_dir / 'quality_noise_vs_artifact.png'
    plt.savefig(plot4_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ãƒã‚¤ã‚º vs ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ: {plot4_path}")


    # 5. ãƒ¢ãƒ‡ãƒ«åˆ¥ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆï¼ˆä¸»è¦6æŒ‡æ¨™ï¼‰
    fig = plt.figure(figsize=(14, 10))

    categories = ['SSIM', 'PSNR', 'ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹', 'ã‚¨ãƒƒã‚¸å¯†åº¦', 'ãƒã‚¤ã‚º\nï¼ˆåè»¢ï¼‰', 'ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ\nï¼ˆåè»¢ï¼‰']
    num_vars = len(categories)

    # æ­£è¦åŒ–ï¼ˆ0-1ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
    df_norm = df.copy()
    df_norm['ssim_norm'] = df['ssim']
    df_norm['psnr_norm'] = df['psnr'] / df['psnr'].max()
    df_norm['sharpness_norm'] = df['sharpness'] / df['sharpness'].max()
    df_norm['edge_norm'] = df['edge_density'] / df['edge_density'].max()
    df_norm['noise_norm'] = 1 - (df['noise'] / df['noise'].max())  # åè»¢ï¼ˆå°‘ãªã„æ–¹ãŒè‰¯ã„ï¼‰
    df_norm['artifact_norm'] = 1 - (df['artifact_total'] / df['artifact_total'].max())  # åè»¢

    angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()
    angles += angles[:1]

    ax = fig.add_subplot(111, polar=True)

    for model in df['model'].unique():
        model_data = df_norm[df_norm['model'] == model]
        values = [
            model_data['ssim_norm'].mean(),
            model_data['psnr_norm'].mean(),
            model_data['sharpness_norm'].mean(),
            model_data['edge_norm'].mean(),
            model_data['noise_norm'].mean(),
            model_data['artifact_norm'].mean()
        ]
        values += values[:1]

        ax.plot(angles, values, 'o-', linewidth=2, label=model)
        ax.fill(angles, values, alpha=0.1)

    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(categories, fontsize=12)
    ax.set_ylim(0, 1)
    ax.set_title('ãƒ¢ãƒ‡ãƒ«åˆ¥æ€§èƒ½ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆï¼‰\nå¤–å´ã»ã©é«˜æ€§èƒ½', fontsize=16, fontweight='bold', pad=20)
    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=12)
    ax.grid(True)

    plt.tight_layout()
    plot5_path = output_dir / 'radar_chart_model_comparison.png'
    plt.savefig(plot5_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ãƒ¢ãƒ‡ãƒ«åˆ¥ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ: {plot5_path}")


    # 6. 17é …ç›®ã®ãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆï¼ˆåˆ†å¸ƒã®å¯è¦–åŒ–ï¼‰
    fig, axes = plt.subplots(3, 6, figsize=(24, 12))
    fig.suptitle('17é …ç›®ã®åˆ†å¸ƒï¼ˆãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆï¼‰', fontsize=20, fontweight='bold')

    metrics_for_violin = [
        'ssim', 'ms_ssim', 'psnr', 'lpips', 'sharpness', 'contrast',
        'entropy', 'noise', 'edge_density', 'artifact_total', 'delta_e',
        'high_freq_ratio', 'texture_complexity', 'local_quality_mean',
        'histogram_corr', 'lab_L_mean', 'total_score'
    ]

    for i, metric in enumerate(metrics_for_violin):
        ax = axes[i // 6, i % 6]

        violin_data = [df[df['model'] == model][metric].values for model in df['model'].unique()]
        parts = ax.violinplot(violin_data, showmeans=True, showmedians=True)

        # ã‚«ãƒ©ãƒ¼ãƒªãƒ³ã‚°
        for pc in parts['bodies']:
            pc.set_facecolor('lightblue')
            pc.set_alpha(0.7)

        ax.set_xticks(range(1, len(df['model'].unique()) + 1))
        ax.set_xticklabels(df['model'].unique(), rotation=45, ha='right', fontsize=8)
        ax.set_title(metric, fontsize=10, fontweight='bold')
        ax.grid(axis='y', alpha=0.3)

    # ä½™ã£ãŸè»¸ã‚’éè¡¨ç¤º
    for i in range(len(metrics_for_violin), 18):
        axes[i // 6, i % 6].axis('off')

    plt.tight_layout()
    plot6_path = output_dir / 'violin_plots_all_metrics.png'
    plt.savefig(plot6_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… 17é …ç›®ãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆ: {plot6_path}")

    # ===== ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºç³»ãƒ—ãƒ­ãƒƒãƒˆ =====

    # 7. SSIMé«˜ Ã— PSNRä½ ã®ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ç–‘ã„å¯è¦–åŒ–
    plt.figure(figsize=(12, 8))

    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['ssim'], model_data['psnr'],
                   label=model, alpha=0.6, s=50)

    # ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ç–‘ã„é ˜åŸŸã‚’èµ¤ã§å¼·èª¿
    ssim_high = df['ssim'].quantile(0.75)
    psnr_low = df['psnr'].quantile(0.25)
    hallucination_candidates = df[(df['ssim'] >= ssim_high) & (df['psnr'] <= psnr_low)]

    if len(hallucination_candidates) > 0:
        plt.scatter(hallucination_candidates['ssim'], hallucination_candidates['psnr'],
                   color='red', s=200, marker='x', linewidths=3,
                   label=f'ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ç–‘ã„ ({len(hallucination_candidates)}ä»¶)', zorder=10)

    plt.axhline(y=psnr_low, color='orange', linestyle='--', alpha=0.5, label=f'PSNRé–¾å€¤ ({psnr_low:.1f})')
    plt.axvline(x=ssim_high, color='orange', linestyle='--', alpha=0.5, label=f'SSIMé–¾å€¤ ({ssim_high:.3f})')

    plt.xlabel('SSIMï¼ˆæ§‹é€ é¡ä¼¼æ€§ï¼‰', fontsize=14, fontweight='bold')
    plt.ylabel('PSNR [dB]', fontsize=14, fontweight='bold')
    plt.title('ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºï¼šSSIMé«˜ & PSNRä½\nå³ä¸‹é ˜åŸŸ = æ§‹é€ ã‚’æ¨¡å€£ã—ãŸãŒå¿ å®Ÿæ€§ãŒä½ã„',
              fontsize=16, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'hallucination_ssim_high_psnr_low.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºâ‘ ï¼ˆSSIMÃ—PSNRï¼‰")


    # 8. ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹ Ã— ãƒã‚¤ã‚ºï¼ˆéå‰°å‡¦ç†æ¤œå‡ºï¼‰
    plt.figure(figsize=(12, 8))

    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['sharpness'], model_data['noise'],
                   label=model, alpha=0.6, s=50)

    sharp_high = df['sharpness'].quantile(0.75)
    noise_high = df['noise'].quantile(0.75)
    over_processed = df[(df['sharpness'] >= sharp_high) & (df['noise'] >= noise_high)]

    if len(over_processed) > 0:
        plt.scatter(over_processed['sharpness'], over_processed['noise'],
                   color='red', s=200, marker='x', linewidths=3,
                   label=f'éå‰°å‡¦ç†ç–‘ã„ ({len(over_processed)}ä»¶)', zorder=10)

    plt.axhline(y=noise_high, color='orange', linestyle='--', alpha=0.5)
    plt.axvline(x=sharp_high, color='orange', linestyle='--', alpha=0.5)

    plt.xlabel('ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹', fontsize=14, fontweight='bold')
    plt.ylabel('ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«', fontsize=14, fontweight='bold')
    plt.title('éå‰°å‡¦ç†æ¤œå‡ºï¼šé«˜ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹ & é«˜ãƒã‚¤ã‚º\nå³ä¸Šé ˜åŸŸ = ã‚·ãƒ£ãƒ¼ãƒ—åŒ–ã§ãƒã‚¤ã‚ºå¢—å¹…',
              fontsize=16, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'hallucination_sharpness_vs_noise.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºâ‘¡ï¼ˆã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹Ã—ãƒã‚¤ã‚ºï¼‰")


    # 9. ã‚¨ãƒƒã‚¸å¯†åº¦ Ã— å±€æ‰€å“è³ªæ¨™æº–åå·®
    plt.figure(figsize=(12, 8))

    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['edge_density'], model_data['local_quality_std'],
                   label=model, alpha=0.6, s=50)

    edge_high = df['edge_density'].quantile(0.75)
    local_std_high = df['local_quality_std'].quantile(0.75)
    unnatural_edges = df[(df['edge_density'] >= edge_high) & (df['local_quality_std'] >= local_std_high)]

    if len(unnatural_edges) > 0:
        plt.scatter(unnatural_edges['edge_density'], unnatural_edges['local_quality_std'],
                   color='red', s=200, marker='x', linewidths=3,
                   label=f'ä¸è‡ªç„¶ãªã‚¨ãƒƒã‚¸ç–‘ã„ ({len(unnatural_edges)}ä»¶)', zorder=10)

    plt.xlabel('ã‚¨ãƒƒã‚¸å¯†åº¦', fontsize=14, fontweight='bold')
    plt.ylabel('å±€æ‰€å“è³ªæ¨™æº–åå·®', fontsize=14, fontweight='bold')
    plt.title('ä¸è‡ªç„¶ãªã‚¨ãƒƒã‚¸æ¤œå‡ºï¼šã‚¨ãƒƒã‚¸å¢—åŠ  & å±€æ‰€å“è³ªã°ã‚‰ã¤ãå¤§\nå³ä¸Šé ˜åŸŸ = ã‚¨ãƒƒã‚¸è¿½åŠ ãŒä¸å‡ä¸€',
              fontsize=16, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'hallucination_edge_vs_local_std.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºâ‘¢ï¼ˆã‚¨ãƒƒã‚¸Ã—å±€æ‰€å“è³ªï¼‰")


    # 10. é«˜å‘¨æ³¢æˆåˆ† Ã— ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
    plt.figure(figsize=(12, 8))

    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['high_freq_ratio'], model_data['entropy'],
                   label=model, alpha=0.6, s=50)

    # å›å¸°ç›´ç·š
    from scipy import stats
    slope, intercept, r_value, p_value, std_err = stats.linregress(df['high_freq_ratio'], df['entropy'])
    x_line = np.array([df['high_freq_ratio'].min(), df['high_freq_ratio'].max()])
    y_line = slope * x_line + intercept
    plt.plot(x_line, y_line, 'r--', label=f'å›å¸°ç›´ç·š (RÂ²={r_value**2:.3f})', linewidth=2)

    plt.xlabel('é«˜å‘¨æ³¢æˆåˆ†æ¯”ç‡', fontsize=14, fontweight='bold')
    plt.ylabel('ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆæƒ…å ±é‡ï¼‰', fontsize=14, fontweight='bold')
    plt.title('äººå·¥ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºï¼šé«˜å‘¨æ³¢ vs ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼\nç›¸é–¢ã‹ã‚‰å¤–ã‚Œã‚‹ = åå¾©ãƒ‘ã‚¿ãƒ¼ãƒ³ç–‘ã„',
              fontsize=16, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'hallucination_highfreq_vs_entropy.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºâ‘£ï¼ˆé«˜å‘¨æ³¢Ã—ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼‰")


    # ===== å“è³ªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ç³»ãƒ—ãƒ­ãƒƒãƒˆ =====

    # 11. SSIM Ã— ãƒã‚¤ã‚º
    plt.figure(figsize=(12, 8))
    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['ssim'], model_data['noise'],
                   label=model, alpha=0.6, s=50)
    plt.xlabel('SSIMï¼ˆæ§‹é€ é¡ä¼¼æ€§ï¼‰', fontsize=14, fontweight='bold')
    plt.ylabel('ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«', fontsize=14, fontweight='bold')
    plt.title('å“è³ªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼šæ§‹é€ é¡ä¼¼æ€§ vs ãƒã‚¤ã‚º', fontsize=16, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'tradeoff_ssim_vs_noise.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•â‘ ï¼ˆSSIMÃ—ãƒã‚¤ã‚ºï¼‰")


    # 12. PSNR Ã— ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ
    plt.figure(figsize=(12, 8))
    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['psnr'], model_data['contrast'],
                   label=model, alpha=0.6, s=50)
    plt.xlabel('PSNR [dB]', fontsize=14, fontweight='bold')
    plt.ylabel('ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ', fontsize=14, fontweight='bold')
    plt.title('å“è³ªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼šå¿ å®Ÿåº¦ vs ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå¼·èª¿', fontsize=16, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'tradeoff_psnr_vs_contrast.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•â‘¡ï¼ˆPSNRÃ—ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆï¼‰")


    # 13. ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹ Ã— ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆ
    plt.figure(figsize=(12, 8))
    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['sharpness'], model_data['artifact_total'],
                   label=model, alpha=0.6, s=50)
    plt.xlabel('ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹', fontsize=14, fontweight='bold')
    plt.ylabel('ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆï¼ˆãƒ–ãƒ­ãƒƒã‚¯+ãƒªãƒ³ã‚®ãƒ³ã‚°ï¼‰', fontsize=14, fontweight='bold')
    plt.title('å“è³ªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼šé®®æ˜åŒ– vs æ­ªã¿', fontsize=16, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'tradeoff_sharpness_vs_artifact.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•â‘¢ï¼ˆã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹Ã—ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆï¼‰")


    # 14. LPIPS Ã— MS-SSIM
    plt.figure(figsize=(12, 8))
    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['lpips'], model_data['ms_ssim'],
                   label=model, alpha=0.6, s=50)
    plt.xlabel('LPIPSï¼ˆçŸ¥è¦šçš„é¡ä¼¼åº¦ï¼‰', fontsize=14, fontweight='bold')
    plt.ylabel('MS-SSIM', fontsize=14, fontweight='bold')
    plt.title('çŸ¥è¦š vs æ§‹é€ ï¼šLPIPS vs MS-SSIM\nè² ã®ç›¸é–¢ãŒæœŸå¾…ã•ã‚Œã‚‹', fontsize=16, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'tradeoff_lpips_vs_msssim.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•â‘£ï¼ˆLPIPSÃ—MS-SSIMï¼‰")


    # 15. ãƒ†ã‚¯ã‚¹ãƒãƒ£ Ã— é«˜å‘¨æ³¢æˆåˆ†
    plt.figure(figsize=(12, 8))
    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['texture_complexity'], model_data['high_freq_ratio'],
                   label=model, alpha=0.6, s=50)
    plt.xlabel('ãƒ†ã‚¯ã‚¹ãƒãƒ£è¤‡é›‘åº¦', fontsize=14, fontweight='bold')
    plt.ylabel('é«˜å‘¨æ³¢æˆåˆ†æ¯”ç‡', fontsize=14, fontweight='bold')
    plt.title('ãƒ†ã‚¯ã‚¹ãƒãƒ£ vs å‘¨æ³¢æ•°æˆåˆ†ã®ä¸€è²«æ€§', fontsize=16, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'tradeoff_texture_vs_highfreq.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•â‘¤ï¼ˆãƒ†ã‚¯ã‚¹ãƒãƒ£Ã—é«˜å‘¨æ³¢ï¼‰")


    # ===== åŒ»ç™‚ç”»åƒç‰¹åŒ–ç³»ãƒ—ãƒ­ãƒƒãƒˆ =====

    # 16. ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ Ã— ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ç›¸é–¢
    plt.figure(figsize=(12, 8))
    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['contrast'], model_data['histogram_corr'],
                   label=model, alpha=0.6, s=50)
    plt.xlabel('ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ', fontsize=14, fontweight='bold')
    plt.ylabel('ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ç›¸é–¢', fontsize=14, fontweight='bold')
    plt.title('åŒ»ç™‚ç”»åƒå“è³ªï¼šã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå¼·èª¿ãŒæ¿ƒåº¦åˆ†å¸ƒã‚’å´©ã—ã¦ã„ãªã„ã‹', fontsize=16, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'medical_contrast_vs_histogram.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… åŒ»ç™‚ç‰¹åŒ–â‘ ï¼ˆã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆÃ—ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼‰")


    # 17. ã‚¨ãƒƒã‚¸å¯†åº¦ Ã— å±€æ‰€å“è³ªå¹³å‡
    plt.figure(figsize=(12, 8))
    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['edge_density'], model_data['local_quality_mean'],
                   label=model, alpha=0.6, s=50)
    plt.xlabel('ã‚¨ãƒƒã‚¸å¯†åº¦', fontsize=14, fontweight='bold')
    plt.ylabel('å±€æ‰€å“è³ªå¹³å‡', fontsize=14, fontweight='bold')
    plt.title('åŒ»ç™‚ç”»åƒå“è³ªï¼šã‚¨ãƒƒã‚¸ä¿æŒã¨å±€æ‰€å“è³ªã®é–¢ä¿‚', fontsize=16, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'medical_edge_vs_local_quality.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… åŒ»ç™‚ç‰¹åŒ–â‘¡ï¼ˆã‚¨ãƒƒã‚¸Ã—å±€æ‰€å“è³ªï¼‰")


    # 18. ãƒã‚¤ã‚º Ã— å±€æ‰€å“è³ªæ¨™æº–åå·®
    plt.figure(figsize=(12, 8))
    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['noise'], model_data['local_quality_std'],
                   label=model, alpha=0.6, s=50)
    plt.xlabel('ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«', fontsize=14, fontweight='bold')
    plt.ylabel('å±€æ‰€å“è³ªæ¨™æº–åå·®', fontsize=14, fontweight='bold')
    plt.title('åŒ»ç™‚ç”»åƒå“è³ªï¼šãƒã‚¤ã‚ºã®å±€æ‰€çš„ååœ¨æ€§', fontsize=16, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'medical_noise_vs_local_std.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… åŒ»ç™‚ç‰¹åŒ–â‘¢ï¼ˆãƒã‚¤ã‚ºÃ—å±€æ‰€å“è³ªSDï¼‰")


    # 19. è‰²å·®Î”E Ã— LABæ˜åº¦
    plt.figure(figsize=(12, 8))
    for model in df['model'].unique():
        model_data = df[df['model'] == model]
        plt.scatter(model_data['delta_e'], model_data['lab_L_mean'],
                   label=model, alpha=0.6, s=50)
    plt.xlabel('è‰²å·® Î”E', fontsize=14, fontweight='bold')
    plt.ylabel('LABæ˜åº¦', fontsize=14, fontweight='bold')
    plt.title('åŒ»ç™‚ç”»åƒå“è³ªï¼šè‰²å¤‰åŒ–ã¨æ˜åº¦ã®é–¢ä¿‚ï¼ˆç—…ç†ç”»åƒã§é‡è¦ï¼‰', fontsize=16, fontweight='bold')
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'medical_deltae_vs_lab.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… åŒ»ç™‚ç‰¹åŒ–â‘£ï¼ˆè‰²å·®Ã—LABæ˜åº¦ï¼‰")


    # ===== åˆ†å¸ƒãƒ»PCAç³»ãƒ—ãƒ­ãƒƒãƒˆ =====

    # 20. ç·åˆã‚¹ã‚³ã‚¢ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼ˆãƒ¢ãƒ‡ãƒ«åˆ¥ï¼‰
    plt.figure(figsize=(12, 6))
    for model in df['model'].unique():
        model_data = df[df['model'] == model]['total_score']
        plt.hist(model_data, bins=20, alpha=0.5, label=model, edgecolor='black')
    plt.xlabel('ç·åˆã‚¹ã‚³ã‚¢', fontsize=14, fontweight='bold')
    plt.ylabel('é »åº¦', fontsize=14, fontweight='bold')
    plt.title('ç·åˆã‚¹ã‚³ã‚¢åˆ†å¸ƒï¼ˆãƒ¢ãƒ‡ãƒ«åˆ¥ï¼‰', fontsize=16, fontweight='bold')
    plt.legend(fontsize=12)
    plt.grid(axis='y', alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'distribution_total_score_histogram.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… åˆ†å¸ƒâ‘ ï¼ˆç·åˆã‚¹ã‚³ã‚¢ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼‰")


    # 21. ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ãƒ—ãƒ­ãƒƒãƒˆ
    from sklearn.decomposition import PCA
    from sklearn.preprocessing import StandardScaler

    # 17é …ç›®ã‚’æ¨™æº–åŒ–
    metrics_for_pca = ['ssim', 'ms_ssim', 'psnr', 'lpips', 'sharpness', 'contrast',
                       'entropy', 'noise', 'edge_density', 'artifact_total', 'delta_e',
                       'high_freq_ratio', 'texture_complexity', 'local_quality_mean',
                       'histogram_corr', 'lab_L_mean', 'total_score']

    X = df[metrics_for_pca].fillna(0)
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # PCAå®Ÿè¡Œ
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X_scaled)

    plt.figure(figsize=(12, 8))
    for model in df['model'].unique():
        mask = df['model'] == model
        plt.scatter(X_pca[mask, 0], X_pca[mask, 1],
                   label=model, alpha=0.6, s=50)

    plt.xlabel(f'ç¬¬1ä¸»æˆåˆ† ({pca.explained_variance_ratio_[0]*100:.1f}%)', fontsize=14, fontweight='bold')
    plt.ylabel(f'ç¬¬2ä¸»æˆåˆ† ({pca.explained_variance_ratio_[1]*100:.1f}%)', fontsize=14, fontweight='bold')
    plt.title(f'ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ï¼š17é …ç›®ã‚’2æ¬¡å…ƒã«åœ§ç¸®\nç´¯ç©å¯„ä¸ç‡: {sum(pca.explained_variance_ratio_)*100:.1f}%',
              fontsize=16, fontweight='bold')
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'pca_2d_projection.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… åˆ†å¸ƒâ‘¡ï¼ˆPCA 2æ¬¡å…ƒãƒ—ãƒ­ãƒƒãƒˆï¼‰")


    # 22. ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ—ãƒ­ãƒƒãƒˆï¼ˆä¸»è¦æŒ‡æ¨™ï¼‰
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    fig.suptitle('ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ãƒãƒ³ãƒ‰ï¼ˆ25%-75%ï¼‰ãƒ—ãƒ­ãƒƒãƒˆ', fontsize=18, fontweight='bold')

    key_metrics = ['ssim', 'psnr', 'sharpness', 'noise', 'edge_density', 'total_score']
    metric_names = ['SSIM', 'PSNR [dB]', 'ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹', 'ãƒã‚¤ã‚º', 'ã‚¨ãƒƒã‚¸å¯†åº¦', 'ç·åˆã‚¹ã‚³ã‚¢']

    for idx, (metric, name) in enumerate(zip(key_metrics, metric_names)):
        ax = axes[idx // 3, idx % 3]

        models = df['model'].unique()
        positions = range(len(models))

        for i, model in enumerate(models):
            model_data = df[df['model'] == model][metric]
            q25 = model_data.quantile(0.25)
            q50 = model_data.quantile(0.50)
            q75 = model_data.quantile(0.75)

            ax.plot([i, i], [q25, q75], 'b-', linewidth=8, alpha=0.3)
            ax.plot(i, q50, 'ro', markersize=10)

        ax.set_xticks(positions)
        ax.set_xticklabels(models, rotation=45, ha='right', fontsize=9)
        ax.set_ylabel(name, fontsize=11, fontweight='bold')
        ax.grid(axis='y', alpha=0.3)
        ax.set_title(f'{name}ã®åˆ†å¸ƒ', fontsize=12)

    plt.tight_layout()
    plt.savefig(output_dir / 'percentile_bands.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… åˆ†å¸ƒâ‘¢ï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ãƒãƒ³ãƒ‰ï¼‰")


    # 23. å¯„ä¸ç‡ã‚°ãƒ©ãƒ•ï¼ˆPCAï¼‰
    pca_full = PCA()
    pca_full.fit(X_scaled)

    plt.figure(figsize=(12, 6))
    cumsum = np.cumsum(pca_full.explained_variance_ratio_)
    plt.plot(range(1, len(cumsum)+1), cumsum, 'bo-', linewidth=2, markersize=8)
    plt.axhline(y=0.95, color='r', linestyle='--', label='95%ãƒ©ã‚¤ãƒ³')
    plt.xlabel('ä¸»æˆåˆ†æ•°', fontsize=14, fontweight='bold')
    plt.ylabel('ç´¯ç©å¯„ä¸ç‡', fontsize=14, fontweight='bold')
    plt.title('PCAç´¯ç©å¯„ä¸ç‡ï¼šä½•æ¬¡å…ƒã§95%èª¬æ˜ã§ãã‚‹ã‹', fontsize=16, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.legend(fontsize=12)
    plt.tight_layout()
    plt.savefig(output_dir / 'pca_cumulative_variance.png', dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… åˆ†å¸ƒâ‘£ï¼ˆPCAå¯„ä¸ç‡ï¼‰")

    print(f"{'='*80}")
    print(f"âœ… å…¨23ç¨®é¡ã®ç ”ç©¶ç”¨ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆå®Œäº†")
    print(f"   è«–æ–‡ãƒ»ç™ºè¡¨è³‡æ–™ã«ãã®ã¾ã¾ä½¿ç”¨ã§ãã¾ã™ï¼ˆ300dpié«˜è§£åƒåº¦ï¼‰\n")


if __name__ == '__main__':
    if len(sys.argv) < 2:
        print(f"\nä½¿ã„æ–¹:")
        print(f"  python analyze_results.py results/batch_analysis.csv\n")
        sys.exit(1)

    csv_file = sys.argv[1]

    if not Path(csv_file).exists():
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_file}")
        sys.exit(1)

    analyze_batch_results(csv_file)
