import tkinter as tk
from tkinter import filedialog, messagebox
import os

class DataExtractionMixin:
    """ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºæ©Ÿèƒ½ã‚’æä¾›ã™ã‚‹Mixinã‚¯ãƒ©ã‚¹"""

    def extract_hallucination_suspects(self):
        """ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ç–‘ã„ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""
        csv_path = self.stats_csv_path.get()

        if not csv_path:
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", "CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„")
            return

        if not os.path.exists(csv_path):
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“:\n{csv_path}")
            return

        try:
            import pandas as pd
            from pathlib import Path

            # CSVã‚’èª­ã¿è¾¼ã¿
            df = pd.read_csv(csv_path, encoding='utf-8-sig')

            # ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ï¼ˆ17é …ç›®ã™ã¹ã¦ã‚’æ´»ç”¨ï¼‰

            # å„ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã®æ¤œå‡ºã‚«ã‚¦ãƒ³ãƒˆç”¨ï¼ˆå¤šæ•°æ±ºãƒ­ã‚¸ãƒƒã‚¯ï¼‰
            detection_count = pd.Series(0, index=df.index)
            detected_patterns = {idx: [] for idx in df.index}

            # ========== çµ„ã¿åˆã‚ã›ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆçŸ›ç›¾ãƒ»è¤‡åˆç•°å¸¸ï¼‰ ==========

            # === ãƒ‘ã‚¿ãƒ¼ãƒ³1: SSIMé«˜ Ã— PSNRä½ï¼ˆ2æ–¹å¼çµ±åˆï¼‰ ===
            # æ–¹æ³•A: å›ºå®šé–¾å€¤
            hallucination_1a_fixed = df[(df['ssim'] > 0.97) & (df['psnr'] < 25)]
            # æ–¹æ³•B: å‹•çš„é–¾å€¤
            ssim_high = df['ssim'].quantile(0.75)
            psnr_low = df['psnr'].quantile(0.25)
            hallucination_1b_quantile = df[(df['ssim'] >= ssim_high) & (df['psnr'] <= psnr_low)]
            # çµ±åˆ
            hallucination_1 = pd.concat([hallucination_1a_fixed, hallucination_1b_quantile]).drop_duplicates()
            detection_count[hallucination_1.index] += 1
            for idx in hallucination_1.index:
                detected_patterns[idx].append('P1:SSIMé«˜Ã—PSNRä½')

            # === ãƒ‘ã‚¿ãƒ¼ãƒ³2: ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹é«˜ Ã— ãƒã‚¤ã‚ºé«˜ ===
            sharpness_75 = df['sharpness'].quantile(0.75)
            noise_75 = df['noise'].quantile(0.75)
            hallucination_2 = df[(df['sharpness'] > sharpness_75) & (df['noise'] > noise_75)]
            detection_count[hallucination_2.index] += 1
            for idx in hallucination_2.index:
                detected_patterns[idx].append('P2:ã‚·ãƒ£ãƒ¼ãƒ—é«˜Ã—ãƒã‚¤ã‚ºé«˜')

            # === ãƒ‘ã‚¿ãƒ¼ãƒ³3: ã‚¨ãƒƒã‚¸å¯†åº¦é«˜ Ã— å±€æ‰€å“è³ªä½ ===
            edge_90 = df['edge_density'].quantile(0.90)
            quality_25 = df['local_quality_mean'].quantile(0.25)
            hallucination_3 = df[(df['edge_density'] > edge_90) & (df['local_quality_mean'] < quality_25)]
            detection_count[hallucination_3.index] += 1
            for idx in hallucination_3.index:
                detected_patterns[idx].append('P3:ã‚¨ãƒƒã‚¸é«˜Ã—å“è³ªä½')

            # === ãƒ‘ã‚¿ãƒ¼ãƒ³4: Artifactsç•°å¸¸é«˜ï¼ˆGANç‰¹æœ‰ã®æ­ªã¿ï¼‰ ===
            artifact_90 = df['artifact_total'].quantile(0.90)
            hallucination_4 = df[df['artifact_total'] > artifact_90]
            detection_count[hallucination_4.index] += 1
            for idx in hallucination_4.index:
                detected_patterns[idx].append('P4:Artifactsé«˜')

            # === ãƒ‘ã‚¿ãƒ¼ãƒ³5: LPIPSé«˜ Ã— SSIMé«˜ï¼ˆçŸ¥è¦šã¨æ§‹é€ ã®çŸ›ç›¾ï¼‰ ===
            lpips_75 = df['lpips'].quantile(0.75)
            ssim_75 = df['ssim'].quantile(0.75)
            hallucination_5 = df[(df['lpips'] > lpips_75) & (df['ssim'] > ssim_75)]
            detection_count[hallucination_5.index] += 1
            for idx in hallucination_5.index:
                detected_patterns[idx].append('P5:LPIPSé«˜Ã—SSIMé«˜')

            # === ãƒ‘ã‚¿ãƒ¼ãƒ³6: å±€æ‰€å“è³ªã°ã‚‰ã¤ãå¤§ ===
            if 'local_quality_std' in df.columns:
                quality_std_75 = df['local_quality_std'].quantile(0.75)
                hallucination_6 = df[df['local_quality_std'] > quality_std_75]
                detection_count[hallucination_6.index] += 1
                for idx in hallucination_6.index:
                    detected_patterns[idx].append('P6:å“è³ªã°ã‚‰ã¤ãå¤§')
            else:
                hallucination_6 = pd.DataFrame()

            # === ãƒ‘ã‚¿ãƒ¼ãƒ³7: Entropyä½ Ã— High-Freqé«˜ï¼ˆåå¾©ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰ ===
            entropy_25 = df['entropy'].quantile(0.25)
            highfreq_75 = df['high_freq_ratio'].quantile(0.75)
            hallucination_7 = df[(df['entropy'] < entropy_25) & (df['high_freq_ratio'] > highfreq_75)]
            detection_count[hallucination_7.index] += 1
            for idx in hallucination_7.index:
                detected_patterns[idx].append('P7:Entropyä½Ã—é«˜å‘¨æ³¢é«˜')

            # === ãƒ‘ã‚¿ãƒ¼ãƒ³8: Contrastç•°å¸¸ Ã— Histogramç›¸é–¢ä½ ===
            contrast_90 = df['contrast'].quantile(0.90)
            histcorr_25 = df['histogram_corr'].quantile(0.25)
            hallucination_8 = df[(df['contrast'] > contrast_90) & (df['histogram_corr'] < histcorr_25)]
            detection_count[hallucination_8.index] += 1
            for idx in hallucination_8.index:
                detected_patterns[idx].append('P8:Contrastç•°å¸¸Ã—Histç›¸é–¢ä½')

            # === ãƒ‘ã‚¿ãƒ¼ãƒ³9: MS-SSIMä½ Ã— ç·åˆã‚¹ã‚³ã‚¢ä½ ===
            msssim_25 = df['ms_ssim'].quantile(0.25)
            total_25 = df['total_score'].quantile(0.25)
            hallucination_9 = df[(df['ms_ssim'] < msssim_25) & (df['total_score'] < total_25)]
            detection_count[hallucination_9.index] += 1
            for idx in hallucination_9.index:
                detected_patterns[idx].append('P9:MS-SSIMä½Ã—ç·åˆä½')

            # ========== å˜ç‹¬ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå„é …ç›®ã®ç•°å¸¸å€¤ï¼‰ ==========

            # é«˜ã„æ–¹ãŒè‰¯ã„æŒ‡æ¨™ï¼ˆç•°å¸¸ã«ä½ã„ï¼‰
            for col, name in [
                ('ssim', 'SSIMä½'), ('ms_ssim', 'MS-SSIMä½'), ('psnr', 'PSNRä½'),
                ('sharpness', 'Sharpnessä½'), ('contrast', 'Contrastä½'), ('entropy', 'Entropyä½'),
                ('edge_density', 'EdgeDensityä½'), ('high_freq_ratio', 'HighFreqä½'),
                ('texture_complexity', 'Textureä½'), ('local_quality_mean', 'LocalQualityä½'),
                ('histogram_corr', 'HistCorrä½'), ('total_score', 'TotalScoreä½')
            ]:
                threshold = df[col].quantile(0.10)  # ä¸‹ä½10%
                detected = df[df[col] < threshold]
                detection_count[detected.index] += 1
                for idx in detected.index:
                    detected_patterns[idx].append(f'å˜ç‹¬:{name}')

            # ä½ã„æ–¹ãŒè‰¯ã„æŒ‡æ¨™ï¼ˆç•°å¸¸ã«é«˜ã„ï¼‰
            for col, name in [
                ('lpips', 'LPIPSé«˜'), ('noise', 'Noiseé«˜'), ('artifact_total', 'Artifactsé«˜'),
                ('delta_e', 'DeltaEé«˜')
            ]:
                threshold = df[col].quantile(0.90)  # ä¸Šä½10%
                detected = df[df[col] > threshold]
                detection_count[detected.index] += 1
                for idx in detected.index:
                    detected_patterns[idx].append(f'å˜ç‹¬:{name}')

            # ========== ä¿¡é ¼åº¦åˆ†é¡ï¼ˆå¤šæ•°æ±ºï¼‰ ==========
            high_confidence = df[detection_count >= 5]  # 5ãƒ‘ã‚¿ãƒ¼ãƒ³ä»¥ä¸Š
            medium_confidence = df[(detection_count >= 3) & (detection_count < 5)]  # 3-4ãƒ‘ã‚¿ãƒ¼ãƒ³
            low_confidence = df[(detection_count >= 1) & (detection_count < 3)]  # 1-2ãƒ‘ã‚¿ãƒ¼ãƒ³

            # å…¨æ¤œå‡ºãƒ‡ãƒ¼ã‚¿çµ±åˆ
            hallucination_all = df[detection_count >= 1].copy()
            hallucination_all['detection_count'] = detection_count[hallucination_all.index]
            hallucination_all['detected_patterns'] = hallucination_all.index.map(
                lambda idx: ', '.join(detected_patterns[idx])
            )

            # ãƒ¢ãƒ‡ãƒ«åˆ¥é›†è¨ˆ
            model_counts = hallucination_all['model'].value_counts()

            # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥é›†è¨ˆ
            pattern_counts = {
                'P1:SSIMÃ—PSNR': len(hallucination_1),
                'P1a:å›ºå®šé–¾å€¤': len(hallucination_1a_fixed),
                'P1b:å‹•çš„é–¾å€¤': len(hallucination_1b_quantile),
                'P2:ã‚·ãƒ£ãƒ¼ãƒ—Ã—ãƒã‚¤ã‚º': len(hallucination_2),
                'P3:ã‚¨ãƒƒã‚¸Ã—å“è³ª': len(hallucination_3),
                'P4:Artifacts': len(hallucination_4),
                'P5:LPIPSÃ—SSIM': len(hallucination_5),
                'P6:å“è³ªã°ã‚‰ã¤ã': len(hallucination_6),
                'P7:EntropyÃ—é«˜å‘¨æ³¢': len(hallucination_7),
                'P8:ContrastÃ—Hist': len(hallucination_8),
                'P9:MS-SSIMÃ—ç·åˆ': len(hallucination_9),
            }

            # ä¿¡é ¼åº¦åˆ¥é›†è¨ˆ
            confidence_stats = {
                'é«˜ä¿¡é ¼åº¦(5+)': len(high_confidence),
                'ä¸­ä¿¡é ¼åº¦(3-4)': len(medium_confidence),
                'ä½ä¿¡é ¼åº¦(1-2)': len(low_confidence),
            }

            # è©³ç´°çµ±è¨ˆ
            summary_stats = hallucination_all.groupby('model').agg({
                'ssim': ['mean', 'std', 'min', 'max'],
                'psnr': ['mean', 'std', 'min', 'max'],
                'sharpness': ['mean', 'std'],
                'noise': ['mean', 'std'],
                'total_score': ['mean', 'std'],
                'detection_count': ['mean', 'max']
            }).round(3)

            # çµæœè¡¨ç¤º
            result_text = f"={'='*70}\n"
            result_text += f"ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ç–‘ã„ãƒ‡ãƒ¼ã‚¿åˆ†æçµæœï¼ˆ17é …ç›®å…¨æ´»ç”¨ï¼‰\n"
            result_text += f"={'='*70}\n\n"

            result_text += f"ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(df)}ä»¶\n"
            result_text += f"ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ç–‘ã„: {len(hallucination_all)}ä»¶ ({len(hallucination_all)/len(df)*100:.1f}%)\n\n"

            result_text += f"ã€ä¿¡é ¼åº¦åˆ¥æ¤œå‡ºæ•°ã€‘\n"
            for conf, count in confidence_stats.items():
                percentage = count / len(df) * 100 if len(df) > 0 else 0
                result_text += f"  {conf}: {count}ä»¶ ({percentage:.1f}%)\n"
            result_text += f"\n"

            result_text += f"ã€çµ„ã¿åˆã‚ã›ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥æ¤œå‡ºæ•°ã€‘\n"
            result_text += f"  P1 (SSIMé«˜Ã—PSNRä½): {pattern_counts['P1:SSIMÃ—PSNR']}ä»¶\n"
            result_text += f"    - å›ºå®šé–¾å€¤: {pattern_counts['P1a:å›ºå®šé–¾å€¤']}ä»¶\n"
            result_text += f"    - å‹•çš„é–¾å€¤: {pattern_counts['P1b:å‹•çš„é–¾å€¤']}ä»¶\n"
            result_text += f"  P2 (ã‚·ãƒ£ãƒ¼ãƒ—Ã—ãƒã‚¤ã‚º): {pattern_counts['P2:ã‚·ãƒ£ãƒ¼ãƒ—Ã—ãƒã‚¤ã‚º']}ä»¶\n"
            result_text += f"  P3 (ã‚¨ãƒƒã‚¸Ã—å“è³ª): {pattern_counts['P3:ã‚¨ãƒƒã‚¸Ã—å“è³ª']}ä»¶\n"
            result_text += f"  P4 (Artifactsé«˜): {pattern_counts['P4:Artifacts']}ä»¶\n"
            result_text += f"  P5 (LPIPSÃ—SSIM): {pattern_counts['P5:LPIPSÃ—SSIM']}ä»¶\n"
            result_text += f"  P6 (å“è³ªã°ã‚‰ã¤ã): {pattern_counts['P6:å“è³ªã°ã‚‰ã¤ã']}ä»¶\n"
            result_text += f"  P7 (EntropyÃ—é«˜å‘¨æ³¢): {pattern_counts['P7:EntropyÃ—é«˜å‘¨æ³¢']}ä»¶\n"
            result_text += f"  P8 (ContrastÃ—Hist): {pattern_counts['P8:ContrastÃ—Hist']}ä»¶\n"
            result_text += f"  P9 (MS-SSIMÃ—ç·åˆ): {pattern_counts['P9:MS-SSIMÃ—ç·åˆ']}ä»¶\n"
            result_text += f"  â€»å˜ç‹¬ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ17é …ç›®ï¼‰ã‚‚æ¤œå‡ºæ¸ˆã¿\n\n"

            result_text += f"ã€ãƒ¢ãƒ‡ãƒ«åˆ¥ã€‘\n"
            for model in sorted(model_counts.index):
                count = model_counts[model]
                percentage = count / len(df) * 100
                avg_detection = hallucination_all[hallucination_all['model'] == model]['detection_count'].mean()
                result_text += f"  {model}: {count}ä»¶ ({percentage:.1f}%) å¹³å‡æ¤œå‡ºæ•°: {avg_detection:.1f}\n"

            result_text += f"\n{'='*70}\n"

            # CSVä¿å­˜ï¼ˆç–‘ã„ãƒ‡ãƒ¼ã‚¿ï¼‰
            output_path = str(Path(csv_path).parent / f"hallucination_suspects_{Path(csv_path).stem}.csv")
            hallucination_all.to_csv(output_path, index=False, encoding='utf-8-sig')
            result_text += f"âœ… ç–‘ã„ãƒ‡ãƒ¼ã‚¿CSV: {output_path}\n"

            # ã‚µãƒãƒªãƒ¼CSVä¿å­˜ï¼ˆãƒ¢ãƒ‡ãƒ«åˆ¥çµ±è¨ˆï¼‰
            summary_path = str(Path(csv_path).parent / f"hallucination_summary_{Path(csv_path).stem}.csv")

            # ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ä½œæˆ
            summary_data = []
            for model in df['model'].unique():
                model_all = df[df['model'] == model]
                model_hal = hallucination_all[hallucination_all['model'] == model]
                model_high = high_confidence[high_confidence['model'] == model]
                model_medium = medium_confidence[medium_confidence['model'] == model]
                model_low = low_confidence[low_confidence['model'] == model]

                summary_data.append({
                    'model': model,
                    'total_count': len(model_all),
                    'hallucination_count': len(model_hal),
                    'hallucination_rate_%': len(model_hal) / len(model_all) * 100 if len(model_all) > 0 else 0,
                    'high_confidence': len(model_high),
                    'medium_confidence': len(model_medium),
                    'low_confidence': len(model_low),
                    'avg_detection_count': model_hal['detection_count'].mean() if len(model_hal) > 0 else 0,
                    'avg_ssim': model_hal['ssim'].mean() if len(model_hal) > 0 else 0,
                    'avg_psnr': model_hal['psnr'].mean() if len(model_hal) > 0 else 0,
                    'avg_sharpness': model_hal['sharpness'].mean() if len(model_hal) > 0 else 0,
                    'avg_noise': model_hal['noise'].mean() if len(model_hal) > 0 else 0,
                    'avg_total_score': model_hal['total_score'].mean() if len(model_hal) > 0 else 0
                })

            summary_df = pd.DataFrame(summary_data)
            summary_df.to_csv(summary_path, index=False, encoding='utf-8-sig')
            result_text += f"âœ… ã‚µãƒãƒªãƒ¼CSV: {summary_path}\n"

            # è©³ç´°çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ï¼ˆãƒ†ã‚­ã‚¹ãƒˆï¼‰
            report_path = str(Path(csv_path).parent / f"hallucination_report_{Path(csv_path).stem}.txt")
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(result_text)
                f.write(f"\n{'='*60}\n")
                f.write("ã€ãƒ¢ãƒ‡ãƒ«åˆ¥è©³ç´°çµ±è¨ˆã€‘\n")
                f.write(f"{'='*60}\n\n")
                f.write(summary_stats.to_string())
            result_text += f"âœ… è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}\n"

            # ã‚°ãƒ©ãƒ•ç”Ÿæˆ
            import matplotlib.pyplot as plt
            import matplotlib
            matplotlib.rcParams['font.family'] = ['Yu Gothic', 'MS Gothic', 'sans-serif']
            matplotlib.rcParams['axes.unicode_minus'] = False

            fig = plt.figure(figsize=(16, 12))
            gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

            # 1. ãƒ¢ãƒ‡ãƒ«åˆ¥ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ç™ºç”Ÿç‡ï¼ˆæ£’ã‚°ãƒ©ãƒ•ï¼‰
            ax1 = fig.add_subplot(gs[0, :2])
            models = []
            rates = []
            for model in sorted(df['model'].unique()):
                model_total = len(df[df['model'] == model])
                model_hal = len(hallucination_all[hallucination_all['model'] == model])
                models.append(model)
                rates.append(model_hal / model_total * 100 if model_total > 0 else 0)

            bars = ax1.bar(models, rates, color=['#4CAF50', '#FFC107', '#F44336'])
            ax1.set_ylabel('ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ç™ºç”Ÿç‡ (%)', fontsize=12)
            ax1.set_title('ãƒ¢ãƒ‡ãƒ«åˆ¥ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ç™ºç”Ÿç‡', fontsize=14, fontweight='bold')
            ax1.grid(axis='y', alpha=0.3)

            # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
            for bar, rate in zip(bars, rates):
                height = bar.get_height()
                ax1.text(bar.get_x() + bar.get_width()/2., height,
                        f'{rate:.1f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')

            # 2. ä¿¡é ¼åº¦åˆ¥åˆ†å¸ƒï¼ˆå††ã‚°ãƒ©ãƒ•ï¼‰
            ax2 = fig.add_subplot(gs[0, 2])
            conf_labels = ['é«˜\n(5+)', 'ä¸­\n(3-4)', 'ä½\n(1-2)']
            conf_counts = [len(high_confidence), len(medium_confidence), len(low_confidence)]
            conf_colors = ['#F44336', '#FFC107', '#4CAF50']

            # 0ä»¶ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å¤–
            filtered_labels = []
            filtered_counts = []
            filtered_colors = []
            for label, count, color in zip(conf_labels, conf_counts, conf_colors):
                if count > 0:
                    filtered_labels.append(label)
                    filtered_counts.append(count)
                    filtered_colors.append(color)

            if len(filtered_counts) > 0:
                ax2.pie(filtered_counts, labels=filtered_labels, autopct='%1.1f%%',
                       colors=filtered_colors, startangle=90)
            ax2.set_title('ä¿¡é ¼åº¦åˆ¥åˆ†å¸ƒ\né«˜=5+ãƒ‘ã‚¿ãƒ¼ãƒ³\nä¸­=3-4ãƒ‘ã‚¿ãƒ¼ãƒ³\nä½=1-2ãƒ‘ã‚¿ãƒ¼ãƒ³', fontsize=11, fontweight='bold')

            # 3. SSIM vs PSNRæ•£å¸ƒå›³ï¼ˆç–‘ã„ãƒ‡ãƒ¼ã‚¿ï¼‰
            ax3 = fig.add_subplot(gs[1, 0])
            for model in hallucination_all['model'].unique():
                model_data = hallucination_all[hallucination_all['model'] == model]
                ax3.scatter(model_data['ssim'], model_data['psnr'], label=model, alpha=0.6, s=80)
            ax3.set_xlabel('SSIM', fontsize=11)
            ax3.set_ylabel('PSNR (dB)', fontsize=11)
            ax3.set_title('SSIM vs PSNRï¼ˆç–‘ã„ãƒ‡ãƒ¼ã‚¿ï¼‰', fontsize=12, fontweight='bold')
            ax3.legend()
            ax3.grid(alpha=0.3)

            # 4. ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹ vs ãƒã‚¤ã‚ºæ•£å¸ƒå›³ï¼ˆç–‘ã„ãƒ‡ãƒ¼ã‚¿ï¼‰
            ax4 = fig.add_subplot(gs[1, 1])
            for model in hallucination_all['model'].unique():
                model_data = hallucination_all[hallucination_all['model'] == model]
                ax4.scatter(model_data['sharpness'], model_data['noise'], label=model, alpha=0.6, s=80)
            ax4.set_xlabel('ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹', fontsize=11)
            ax4.set_ylabel('ãƒã‚¤ã‚º', fontsize=11)
            ax4.set_title('ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹ vs ãƒã‚¤ã‚ºï¼ˆç–‘ã„ãƒ‡ãƒ¼ã‚¿ï¼‰', fontsize=12, fontweight='bold')
            ax4.legend()
            ax4.grid(alpha=0.3)

            # 5. ã‚¨ãƒƒã‚¸å¯†åº¦ vs å±€æ‰€å“è³ªæ•£å¸ƒå›³ï¼ˆç–‘ã„ãƒ‡ãƒ¼ã‚¿ï¼‰
            ax5 = fig.add_subplot(gs[1, 2])
            for model in hallucination_all['model'].unique():
                model_data = hallucination_all[hallucination_all['model'] == model]
                ax5.scatter(model_data['edge_density'], model_data['local_quality_mean'], label=model, alpha=0.6, s=80)
            ax5.set_xlabel('ã‚¨ãƒƒã‚¸å¯†åº¦', fontsize=11)
            ax5.set_ylabel('å±€æ‰€å“è³ª', fontsize=11)
            ax5.set_title('ã‚¨ãƒƒã‚¸å¯†åº¦ vs å±€æ‰€å“è³ªï¼ˆç–‘ã„ãƒ‡ãƒ¼ã‚¿ï¼‰', fontsize=12, fontweight='bold')
            ax5.legend()
            ax5.grid(alpha=0.3)

            # 6. ãƒ¢ãƒ‡ãƒ«åˆ¥å¹³å‡ã‚¹ã‚³ã‚¢æ¯”è¼ƒï¼ˆãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆï¼‰
            ax6 = fig.add_subplot(gs[2, :], projection='polar')

            categories = ['SSIM', 'PSNR/50', 'ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹\n(æ­£è¦åŒ–)', 'ãƒã‚¤ã‚º\n(åè»¢)', 'ç·åˆã‚¹ã‚³ã‚¢/100']
            angles = [n / float(len(categories)) * 2 * 3.14159 for n in range(len(categories))]
            angles += angles[:1]

            for model in sorted(hallucination_all['model'].unique()):
                model_data = hallucination_all[hallucination_all['model'] == model]
                if len(model_data) > 0:
                    values = [
                        model_data['ssim'].mean(),
                        model_data['psnr'].mean() / 50,
                        min(model_data['sharpness'].mean() / 300, 1.0),
                        1.0 - min(model_data['noise'].mean() / 0.1, 1.0),
                        model_data['total_score'].mean() / 100
                    ]
                    values += values[:1]
                    ax6.plot(angles, values, 'o-', linewidth=2, label=model)
                    ax6.fill(angles, values, alpha=0.15)

            ax6.set_xticks(angles[:-1])
            ax6.set_xticklabels(categories, fontsize=10)
            ax6.set_ylim(0, 1)
            ax6.set_title('ãƒ¢ãƒ‡ãƒ«åˆ¥å¹³å‡ã‚¹ã‚³ã‚¢æ¯”è¼ƒï¼ˆãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ç–‘ã„ãƒ‡ãƒ¼ã‚¿ï¼‰', fontsize=14, fontweight='bold', pad=20)
            ax6.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
            ax6.grid(True)

            plt.suptitle(f'ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ç–‘ã„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆ17é …ç›®å…¨æ´»ç”¨ï¼‰ (n={len(hallucination_all)})',
                        fontsize=16, fontweight='bold', y=0.98)

            # ä¿å­˜
            graph_path = str(Path(csv_path).parent / f"hallucination_analysis_{Path(csv_path).stem}.png")
            plt.savefig(graph_path, dpi=300, bbox_inches='tight')
            plt.close()

            result_text += f"âœ… åˆ†æã‚°ãƒ©ãƒ•: {graph_path}\n"
            result_text += f"{'='*60}\n"

            # çµæœè¡¨ç¤º
            self.batch_result_text.delete("1.0", tk.END)
            self.batch_result_text.insert("1.0", result_text)

            self.batch_status_label.configure(
                text=f"âœ… ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ç–‘ã„ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºå®Œäº†ï¼ˆ{len(hallucination_all)}ä»¶ï¼‰",
                text_color="#ff4444"
            )

            messagebox.showinfo(
                "æŠ½å‡ºå®Œäº†",
                f"ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ç–‘ã„ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚\n\n"
                f"ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(df)}ä»¶\n"
                f"ç–‘ã„ã‚ã‚Š: {len(hallucination_all)}ä»¶ ({len(hallucination_all)/len(df)*100:.1f}%)\n\n"
                f"ä¿å­˜å…ˆ:\n{output_path}\n\n"
                f"ã“ã®CSVã§å†åº¦çµ±è¨ˆåˆ†æã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚"
            )

        except Exception as e:
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æŠ½å‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:\n{str(e)}")

    def extract_clean_dataset(self):
        """æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¤œå‡º0ï¼‰ã‚’æŠ½å‡ºã—ã¦ã‚¯ãƒªãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ"""
        try:
            csv_path = filedialog.askopenfilename(
                title="ãƒãƒƒãƒåˆ†æCSVã‚’é¸æŠ",
                filetypes=[("CSV files", "*.csv")]
            )
            if not csv_path:
                return

            from pathlib import Path
            import pandas as pd
            import shutil
            from datetime import datetime

            self.batch_status_label.configure(
                text="â³ æ­£å¸¸ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºä¸­...",
                text_color="#ffaa00"
            )
            self.root.update()

            # CSVã‚’èª­ã¿è¾¼ã¿
            df = pd.read_csv(csv_path, encoding='utf-8-sig')

            # ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯å®Ÿè¡Œï¼ˆdetection_countè¨ˆç®—ï¼‰
            detection_count = pd.Series(0, index=df.index)
            detected_patterns = {idx: [] for idx in df.index}

            # === çµ„ã¿åˆã‚ã›ãƒ‘ã‚¿ãƒ¼ãƒ³ ===
            # P1: SSIMé«˜ Ã— PSNRä½
            hallucination_1a_fixed = df[(df['ssim'] > 0.97) & (df['psnr'] < 25)]
            ssim_high = df['ssim'].quantile(0.75)
            psnr_low = df['psnr'].quantile(0.25)
            hallucination_1b_quantile = df[(df['ssim'] >= ssim_high) & (df['psnr'] <= psnr_low)]
            hallucination_1 = pd.concat([hallucination_1a_fixed, hallucination_1b_quantile]).drop_duplicates()
            detection_count[hallucination_1.index] += 1
            for idx in hallucination_1.index:
                detected_patterns[idx].append('P1')

            # P2: ã‚·ãƒ£ãƒ¼ãƒ—ãƒã‚¹é«˜ Ã— ãƒã‚¤ã‚ºé«˜
            sharpness_75 = df['sharpness'].quantile(0.75)
            noise_75 = df['noise'].quantile(0.75)
            hallucination_2 = df[(df['sharpness'] > sharpness_75) & (df['noise'] > noise_75)]
            detection_count[hallucination_2.index] += 1
            for idx in hallucination_2.index:
                detected_patterns[idx].append('P2')

            # P3: ã‚¨ãƒƒã‚¸å¯†åº¦é«˜ Ã— å±€æ‰€å“è³ªä½
            edge_90 = df['edge_density'].quantile(0.90)
            quality_25 = df['local_quality_mean'].quantile(0.25)
            hallucination_3 = df[(df['edge_density'] > edge_90) & (df['local_quality_mean'] < quality_25)]
            detection_count[hallucination_3.index] += 1
            for idx in hallucination_3.index:
                detected_patterns[idx].append('P3')

            # P4: Artifactsç•°å¸¸é«˜
            artifact_90 = df['artifact_total'].quantile(0.90)
            hallucination_4 = df[df['artifact_total'] > artifact_90]
            detection_count[hallucination_4.index] += 1
            for idx in hallucination_4.index:
                detected_patterns[idx].append('P4')

            # P5: LPIPSé«˜ Ã— SSIMé«˜
            lpips_75 = df['lpips'].quantile(0.75)
            ssim_75 = df['ssim'].quantile(0.75)
            hallucination_5 = df[(df['lpips'] > lpips_75) & (df['ssim'] > ssim_75)]
            detection_count[hallucination_5.index] += 1
            for idx in hallucination_5.index:
                detected_patterns[idx].append('P5')

            # P6: å±€æ‰€å“è³ªã°ã‚‰ã¤ãå¤§
            if 'local_quality_std' in df.columns:
                quality_std_75 = df['local_quality_std'].quantile(0.75)
                hallucination_6 = df[df['local_quality_std'] > quality_std_75]
                detection_count[hallucination_6.index] += 1
                for idx in hallucination_6.index:
                    detected_patterns[idx].append('P6')

            # P7-P9çœç•¥ï¼ˆå¿…è¦ã«å¿œã˜ã¦è¿½åŠ ï¼‰

            # === å˜ç‹¬ãƒ‘ã‚¿ãƒ¼ãƒ³ ===
            for col, name in [
                ('ssim', 'SSIM'), ('ms_ssim', 'MS-SSIM'), ('psnr', 'PSNR'),
                ('sharpness', 'Sharpness'), ('contrast', 'Contrast'), ('entropy', 'Entropy'),
                ('edge_density', 'EdgeDensity'), ('high_freq_ratio', 'HighFreq'),
                ('texture_complexity', 'Texture'), ('local_quality_mean', 'LocalQuality'),
                ('histogram_corr', 'HistCorr'), ('total_score', 'TotalScore')
            ]:
                threshold = df[col].quantile(0.10)
                detected = df[df[col] < threshold]
                detection_count[detected.index] += 1

            for col, name in [
                ('lpips', 'LPIPS'), ('noise', 'Noise'), ('artifact_total', 'Artifacts'),
                ('delta_e', 'DeltaE')
            ]:
                threshold = df[col].quantile(0.90)
                detected = df[df[col] > threshold]
                detection_count[detected.index] += 1

            # æ­£å¸¸ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼ˆdetection_count == 0ï¼‰
            normal_df = df[detection_count == 0].copy()

            # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_dir = Path(csv_path).parent / f"clean_dataset_{timestamp}"
            output_dir.mkdir(exist_ok=True)

            # ãƒ¢ãƒ‡ãƒ«åˆ¥ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ
            original_dir = output_dir / "original"
            original_dir.mkdir(exist_ok=True)

            model_dirs = {}
            for model in df['model'].unique():
                model_dir = output_dir / f"{model}_clean"
                model_dir.mkdir(exist_ok=True)
                model_dirs[model] = model_dir

            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ”ãƒ¼
            copied_files = []
            metadata = []

            for image_id in normal_df['image_id'].unique():
                # å…ƒç”»åƒã‚’ã‚³ãƒ”ãƒ¼ï¼ˆ1å›ã®ã¿ï¼‰
                image_rows = normal_df[normal_df['image_id'] == image_id]
                if len(image_rows) > 0:
                    first_row = image_rows.iloc[0]
                    original_path = first_row['original_path']

                    if os.path.exists(original_path):
                        dest_orig = original_dir / Path(original_path).name
                        if not dest_orig.exists():
                            shutil.copy2(original_path, dest_orig)
                            copied_files.append(str(dest_orig))

                # ãƒ¢ãƒ‡ãƒ«åˆ¥è¶…è§£åƒç”»åƒã‚’ã‚³ãƒ”ãƒ¼
                model_status = {}
                for model in df['model'].unique():
                    model_row = image_rows[image_rows['model'] == model]
                    if len(model_row) > 0:
                        upscaled_path = model_row.iloc[0]['upscaled_path']
                        if os.path.exists(upscaled_path):
                            dest_upscaled = model_dirs[model] / Path(upscaled_path).name
                            shutil.copy2(upscaled_path, dest_upscaled)
                            copied_files.append(str(dest_upscaled))
                            model_status[model] = 'clean'
                        else:
                            model_status[model] = 'missing'
                    else:
                        model_status[model] = 'hallucination'

                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆ
                metadata_row = {
                    'image_id': image_id,
                    'original_path': str(dest_orig) if 'dest_orig' in locals() else '',
                }
                for model in sorted(df['model'].unique()):
                    metadata_row[f'{model}_status'] = model_status.get(model, 'none')
                    model_row = normal_df[(normal_df['image_id'] == image_id) & (normal_df['model'] == model)]
                    if len(model_row) > 0:
                        metadata_row[f'{model}_ssim'] = model_row.iloc[0]['ssim']
                        metadata_row[f'{model}_psnr'] = model_row.iloc[0]['psnr']
                        metadata_row[f'{model}_total_score'] = model_row.iloc[0]['total_score']

                metadata.append(metadata_row)

            # metadata.csvä¿å­˜
            metadata_df = pd.DataFrame(metadata)
            metadata_path = output_dir / "metadata.csv"
            metadata_df.to_csv(metadata_path, index=False, encoding='utf-8-sig')

            # READMEä½œæˆ
            readme_path = output_dir / "README.txt"
            with open(readme_path, 'w', encoding='utf-8') as f:
                f.write("=" * 70 + "\n")
                f.write("ã‚¯ãƒªãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆæ­£å¸¸ç”»åƒã®ã¿ï¼‰\n")
                f.write("=" * 70 + "\n\n")
                f.write(f"ä½œæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"å…ƒãƒ‡ãƒ¼ã‚¿: {csv_path}\n\n")
                f.write(f"ç·ç”»åƒæ•°: {len(normal_df['image_id'].unique())}æš\n")
                for model in sorted(df['model'].unique()):
                    count = len(normal_df[normal_df['model'] == model])
                    f.write(f"  {model}: {count}æš\n")
                f.write("\nã€ãƒ•ã‚©ãƒ«ãƒ€æ§‹æˆã€‘\n")
                f.write("  original/      : å…ƒç”»åƒ\n")
                for model in sorted(df['model'].unique()):
                    f.write(f"  {model}_clean/ : {model}ã§æ­£å¸¸ãªè¶…è§£åƒç”»åƒ\n")
                f.write("  metadata.csv   : è©³ç´°æƒ…å ±ï¼ˆAIå­¦ç¿’ç”¨ï¼‰\n\n")
                f.write("ã€ä½¿ã„æ–¹ã€‘\n")
                f.write("1. AIå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä½¿ç”¨\n")
                f.write("2. å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n")
                f.write("3. ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿\n\n")
                f.write("â€» ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºã§å•é¡Œãªã—ã¨åˆ¤å®šã•ã‚ŒãŸç”»åƒã®ã¿ã‚’å«ã¿ã¾ã™\n")

            # çµæœè¡¨ç¤º
            result_text = f"=" * 70 + "\n"
            result_text += "âœ… ã‚¯ãƒªãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Œäº†\n"
            result_text += "=" * 70 + "\n\n"
            result_text += f"ğŸ“ å‡ºåŠ›å…ˆ: {output_dir}\n\n"
            result_text += f"ğŸ“Š çµ±è¨ˆ:\n"
            result_text += f"  ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(df)}ä»¶\n"
            result_text += f"  æ­£å¸¸ãƒ‡ãƒ¼ã‚¿: {len(normal_df)}ä»¶ ({len(normal_df)/len(df)*100:.1f}%)\n"
            result_text += f"  æ­£å¸¸ç”»åƒæ•°: {len(normal_df['image_id'].unique())}æš\n\n"
            result_text += f"ã€ãƒ¢ãƒ‡ãƒ«åˆ¥æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã€‘\n"
            for model in sorted(df['model'].unique()):
                count = len(normal_df[normal_df['model'] == model])
                total = len(df[df['model'] == model])
                result_text += f"  {model}: {count}/{total}ä»¶ ({count/total*100:.1f}%)\n"
            result_text += f"\nğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«:\n"
            result_text += f"  metadata.csv : ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿\n"
            result_text += f"  README.txt   : èª¬æ˜æ›¸\n"
            result_text += f"  ã‚³ãƒ”ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(copied_files)}å€‹\n"

            self.batch_result_text.delete("1.0", tk.END)
            self.batch_result_text.insert("1.0", result_text)

            self.batch_status_label.configure(
                text=f"âœ… ã‚¯ãƒªãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Œäº†ï¼ˆ{len(normal_df['image_id'].unique())}æšï¼‰",
                text_color="#44ff44"
            )

            messagebox.showinfo(
                "ä½œæˆå®Œäº†",
                f"ã‚¯ãƒªãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆæ­£å¸¸ç”»åƒã®ã¿ï¼‰ã‚’ä½œæˆã—ã¾ã—ãŸã€‚\n\n"
                f"æ­£å¸¸ç”»åƒæ•°: {len(normal_df['image_id'].unique())}æš\n"
                f"å‡ºåŠ›å…ˆ: {output_dir}\n\n"
                f"AIå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä½¿ç”¨ã§ãã¾ã™ã€‚"
            )

        except Exception as e:
            import traceback
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"ã‚¯ãƒªãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:\n{str(e)}\n\n{traceback.format_exc()}")

